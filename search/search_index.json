{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RL Insights Warning This site is in its early days, most pages are still empty! see below for a list of pages with actual content. How is this site structured? The pages are grouped in 3 categories: Papers : for each publication, provide a quick summary, the list of key concepts, and links to high-quality notebooks and blog posts. Concepts : cross-reference RL \"building blocks\" across papers. Describes what they are about, list most influential papers along with their contribution. FAQ : curated answers to common questions. Existing pages Most pages are currently empty. Here are some pages with already some content: Papers CURL: Contrastive Unsupervised Representations for Reinforcement Learning New to RL? While this site is meant as a repository for research papers, here are some good starting points: What is reinforcement learning? A quick intro to what this is all about. Best RL educational resources A number of excellent articles and courses to get started on this topic. Foundational papers Most of the RL knowledge still only exists in papers - these provide a good starting point.","title":"Home"},{"location":"#rl-insights","text":"Warning This site is in its early days, most pages are still empty! see below for a list of pages with actual content.","title":"RL Insights"},{"location":"#how-is-this-site-structured","text":"The pages are grouped in 3 categories: Papers : for each publication, provide a quick summary, the list of key concepts, and links to high-quality notebooks and blog posts. Concepts : cross-reference RL \"building blocks\" across papers. Describes what they are about, list most influential papers along with their contribution. FAQ : curated answers to common questions.","title":"How is this site structured?"},{"location":"#existing-pages","text":"Most pages are currently empty. Here are some pages with already some content:","title":"Existing pages"},{"location":"#papers","text":"CURL: Contrastive Unsupervised Representations for Reinforcement Learning","title":"Papers"},{"location":"#new-to-rl","text":"While this site is meant as a repository for research papers, here are some good starting points: What is reinforcement learning? A quick intro to what this is all about. Best RL educational resources A number of excellent articles and courses to get started on this topic. Foundational papers Most of the RL knowledge still only exists in papers - these provide a good starting point.","title":"New to RL?"},{"location":"a3c/","text":"Asynchronous Methods for Deep Reinforcement Learning (A3C) Tldr Introduces A3C - Asynchronous Advantage Actor Critic , a parallel training method that uses multiple CPU cores to speed up actor-critic training on a single machine. A non-asynchronous version called A2C, geared towards GPU, is generally preferred today. Published February 2016 - Very Influential - arXiv Summary This paper investigates a new parallel training method which uses multiple CPU cores on the same machine, rather than relying on GPUs. Both value-based and policy-based methods are considered. The major result is the performance of this training scheme using an actor-critic method . This training scheme also shows promising results for value-based methods. However, these methods being off-policy, they can be significantly improved with the use of a replay buffer. The core idea is to use multiple asynchronous \"actor-learners\" running in different threads. Each actor-learner performs a number of learning steps, then perform an asynchronous update of the global parameters using accumulated gradients. Since this method is designed to train networks using a CPU, asynchronous updates made a lot of sense as it enabled Hogwild syle updates. However, it appeared later on that this was not optimal when using a GPU. For this reason, this method is commonly used today in a synchronous way in which the updates from all the actor-learners are batched to make better use of the GPU. This non-asynchronous version is simply called A2C (Advantage Actor Critic) 1 . In hindsight \u2705 Using multiple environments in parallel is a good way to decorrelate the agent's data \u274c Asynchronous updates are not a good match for GPU training \u274c Using exclusively on-policy data is very bad for sample efficiency Key Concepts Builds on DQN and Actor-Critic Introduces a new single-machine Parallel Training method Uses Entropy Regularization Uses an LSTM Recurrent Neural Network Uses multi-step return with the forward view Uses shared layers between policy and value functions Context At this point, DQN had proven that value-based methods could be used to reach good performance on the Atari domain . Multiple improvements had been proposed such as Dueling Networks, Double DQN etc, but the Rainbow paper which would combine them all wasn't out yet. On the policy gradient front, the generalized advantage estimation was out. Related Links Actor-Critic Methods: A3C and A2C from Daniel Seita OpenAI Baselines: ACKTR & A2C \u21a9","title":"Asynchronous Methods for Deep Reinforcement Learning (A3C)"},{"location":"a3c/#asynchronous-methods-for-deep-reinforcement-learning-a3c","text":"Tldr Introduces A3C - Asynchronous Advantage Actor Critic , a parallel training method that uses multiple CPU cores to speed up actor-critic training on a single machine. A non-asynchronous version called A2C, geared towards GPU, is generally preferred today. Published February 2016 - Very Influential - arXiv","title":"Asynchronous Methods for Deep Reinforcement Learning (A3C)"},{"location":"a3c/#summary","text":"This paper investigates a new parallel training method which uses multiple CPU cores on the same machine, rather than relying on GPUs. Both value-based and policy-based methods are considered. The major result is the performance of this training scheme using an actor-critic method . This training scheme also shows promising results for value-based methods. However, these methods being off-policy, they can be significantly improved with the use of a replay buffer. The core idea is to use multiple asynchronous \"actor-learners\" running in different threads. Each actor-learner performs a number of learning steps, then perform an asynchronous update of the global parameters using accumulated gradients. Since this method is designed to train networks using a CPU, asynchronous updates made a lot of sense as it enabled Hogwild syle updates. However, it appeared later on that this was not optimal when using a GPU. For this reason, this method is commonly used today in a synchronous way in which the updates from all the actor-learners are batched to make better use of the GPU. This non-asynchronous version is simply called A2C (Advantage Actor Critic) 1 .","title":"Summary"},{"location":"a3c/#in-hindsight","text":"\u2705 Using multiple environments in parallel is a good way to decorrelate the agent's data \u274c Asynchronous updates are not a good match for GPU training \u274c Using exclusively on-policy data is very bad for sample efficiency","title":"In hindsight"},{"location":"a3c/#key-concepts","text":"Builds on DQN and Actor-Critic Introduces a new single-machine Parallel Training method Uses Entropy Regularization Uses an LSTM Recurrent Neural Network Uses multi-step return with the forward view Uses shared layers between policy and value functions","title":"Key Concepts"},{"location":"a3c/#context","text":"At this point, DQN had proven that value-based methods could be used to reach good performance on the Atari domain . Multiple improvements had been proposed such as Dueling Networks, Double DQN etc, but the Rainbow paper which would combine them all wasn't out yet. On the policy gradient front, the generalized advantage estimation was out.","title":"Context"},{"location":"a3c/#related-links","text":"Actor-Critic Methods: A3C and A2C from Daniel Seita OpenAI Baselines: ACKTR & A2C \u21a9","title":"Related Links"},{"location":"acer/","text":"Sample Efficient Actor-Critic with Experience Replay (ACER) Warning Under construction! This page is still vastly incomplete. Tldr ACER - Actor-Critic with Experience Replay extends the parallel implementation of actor-critic methods described in A3C to the off-policy setting. Published November 2016 - Influential (251 citations) - arXiv Summary \"ACER may be understood as the off-policy counterpart of the A3C method .\" Key concepts Uses ?? for variance reduction (GAE?) Uses Generalized Advantage Estimation Uses the the off-policy Retrace algorithm Uses parallel-training as in A3C Introduces Truncated Importance Sampling Introduces stochastic dueling network architectures Introduces efficient trust region policy optimization Legacy Used in PPO (?)","title":"Sample Efficient Actor-Critic with Experience Replay (ACER)"},{"location":"acer/#sample-efficient-actor-critic-with-experience-replay-acer","text":"Warning Under construction! This page is still vastly incomplete. Tldr ACER - Actor-Critic with Experience Replay extends the parallel implementation of actor-critic methods described in A3C to the off-policy setting. Published November 2016 - Influential (251 citations) - arXiv","title":"Sample Efficient Actor-Critic with Experience Replay (ACER)"},{"location":"acer/#summary","text":"\"ACER may be understood as the off-policy counterpart of the A3C method .\"","title":"Summary"},{"location":"acer/#key-concepts","text":"Uses ?? for variance reduction (GAE?) Uses Generalized Advantage Estimation Uses the the off-policy Retrace algorithm Uses parallel-training as in A3C Introduces Truncated Importance Sampling Introduces stochastic dueling network architectures Introduces efficient trust region policy optimization","title":"Key concepts"},{"location":"acer/#legacy","text":"Used in PPO (?)","title":"Legacy"},{"location":"actor-critic/","text":"Actor-Critic Algorithms Tldr Introduces Actor-Critic Algorithms . Published February 2016 - Very Influential - Link Summary This paper introduces the the notion of actor-critic methods, which is the basis of a large part of the reinforcement learning methods today. This idea is to combine a value-function approximation, the critic, which helps reduce the variance of a policy-gradient approximation, the actor. This idea will further be extended with the notion of advantage . Note that this work only considers linear approximations, ie it doesn't use neural networks. In the actor-critic setting, learning the critic can be seen as a supervised learning problem. This means it can either be learned using TD-learning, or simply least square. It is important to understand that the goal of the critic is to approximate the returns that the actor will receive, and not the returns of the optimal policy. But the critic is regularly updated, which means that the critic is trying to learn a moving target! in practice, this is not a problem as the actor is updated more slowly than the critic Silver lecture on PG methods: https://www.youtube.com/watch?v=KHZVXao4qXs (actor-critic from https://youtu.be/KHZVXao4qXs?t=3178) Pretty much all the gradient-based approaches are now actor-critic methods, so this doesn't mean much anymore! https://github.com/openai/spinningup/issues/156#issuecomment-494596739 TODO cover Degris paper, the original resource for off-policy AC: https://arxiv.org/abs/1205.4839","title":"Actor-Critic Algorithms"},{"location":"actor-critic/#actor-critic-algorithms","text":"Tldr Introduces Actor-Critic Algorithms . Published February 2016 - Very Influential - Link","title":"Actor-Critic Algorithms"},{"location":"actor-critic/#summary","text":"This paper introduces the the notion of actor-critic methods, which is the basis of a large part of the reinforcement learning methods today. This idea is to combine a value-function approximation, the critic, which helps reduce the variance of a policy-gradient approximation, the actor. This idea will further be extended with the notion of advantage . Note that this work only considers linear approximations, ie it doesn't use neural networks. In the actor-critic setting, learning the critic can be seen as a supervised learning problem. This means it can either be learned using TD-learning, or simply least square. It is important to understand that the goal of the critic is to approximate the returns that the actor will receive, and not the returns of the optimal policy. But the critic is regularly updated, which means that the critic is trying to learn a moving target! in practice, this is not a problem as the actor is updated more slowly than the critic Silver lecture on PG methods: https://www.youtube.com/watch?v=KHZVXao4qXs (actor-critic from https://youtu.be/KHZVXao4qXs?t=3178) Pretty much all the gradient-based approaches are now actor-critic methods, so this doesn't mean much anymore! https://github.com/openai/spinningup/issues/156#issuecomment-494596739 TODO cover Degris paper, the original resource for off-policy AC: https://arxiv.org/abs/1205.4839","title":"Summary"},{"location":"atari/","text":"Atari Learning Environment Tldr Short summary Published November XXXX - Influential (XXX citations) - arXiv Summary Key concepts - - Legacy","title":"Atari Learning Environment"},{"location":"atari/#atari-learning-environment","text":"Tldr Short summary Published November XXXX - Influential (XXX citations) - arXiv","title":"Atari Learning Environment"},{"location":"atari/#summary","text":"","title":"Summary"},{"location":"atari/#key-concepts","text":"","title":"Key concepts"},{"location":"atari/#-","text":"-","title":"-"},{"location":"atari/#legacy","text":"","title":"Legacy"},{"location":"auxiliary-tasks/","text":"UNREAL paper Self-Supervised Learning for RL: Auxiliary tasks such as predicting the future conditioned on the past observation(s) and action(s) (Jaderberg et al., 2016; Shelhamer et al., 2016; van den Oord et al., 2018), and predicting the depth image for maze navigation (Mirowski et al., 2016) are a few representative examples of using auxiliary tasks to improve the sample-efficiency of model-free RL algorithms. The future prediction is either done in a pixel space (Jaderberg et al., 2016) or latent space (van den Oord et al., 2018). The sample-efficiency gains from reconstruction-based auxiliary losses have been benchmarked in Jaderberg et al. (2016); Higgins et al. (2017); Yarats et al. (2019). Contrastive learning across has been used to extract reward signals characterized as distance metrics in the latent space by -- CURL paper","title":"Auxiliary tasks"},{"location":"best-resources/","text":"Best RL resources","title":"Best RL resources"},{"location":"best-resources/#best-rl-resources","text":"","title":"Best RL resources"},{"location":"cem/","text":"Cross-Entropy Methods http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf I. Szita and A. L\u00a8orincz. \u201cLearning Tetris using the noisy cross-entropy method\u201d. In: Neural computation 18.12 (2006), pp. 2936\u20132941.","title":"Cross-Entropy Methods"},{"location":"cem/#cross-entropy-methods","text":"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf I. Szita and A. L\u00a8orincz. \u201cLearning Tetris using the noisy cross-entropy method\u201d. In: Neural computation 18.12 (2006), pp. 2936\u20132941.","title":"Cross-Entropy Methods"},{"location":"contrastive-learning/","text":"Contrastive Learning: Contrastive Learning is a framework to learn representations that obey similarity constraints in a dataset typically organized by similar and dissimilar pairs. This is often best understood as performing a dictionary lookup task wherein the positive and negatives represent a set of keys with respect to a query (or an anchor). A simple instantiation of contrastive learning is Instance Discrimination (Wu et al., 2018) wherein a query and key are positive pairs if they are data-augmentations of the same instance (example, image) and negative otherwise. A key challenge in contrastive learning is the choice of negatives which can decide the quality of the underlying representations learned. The loss functions used to contrast could be among several choices such as InfoNCE (van den Oord et al., 2018), Triplet (Wang & Gupta, 2015), Siamese (Chopra et al., 2005) and so forth. -- CURL paper","title":"Contrastive learning"},{"location":"curl/","text":"CURL: Contrastive Unsupervised Representations for Reinforcement Learning Tldr CURL improves the sample efficiency when learning from pixels by using contrastive learning, a self-supervised method, as an auxiliary task. The CURL contrastive objective works in a similar way as the SimCLR framework, using random cropping as an augmentation method. After 100k interactions, CURL outperforms all other methods on DM Control Suite, and shows strong results on Atari games. April 2020 - arXiv - Code What is this about? In reinforcement learning, solving a task from pixels is much harder than solving an equivalent task using \"physical\" features such as coordinates and angles. This makes sense: you can consider an image as a high-dimensional vector containing hundreds of features, which don't have any clear connection with the goal of the environment! As a result, you generally need a lot more interactions between an agent and its environment in order to learn a good policy from pixels. For example, the figure below shows the results of multiple recent RL methods on the DeepMind Control Suite, learning from physical features. Take a look at the x-axis: depending on the complexity of the task, the agents are trained from 1 to 10 million interactions: 1 On the other hand, let's have a look at results using Atari games as a benchmark. There is no notion of \"physical\" features with the Atari emulator: the only observations the agent can work with are RGB images. The agents now have to be trained for a staggering 200 million frames! 2 Some distributed approaches even consider numbers up to a billion. This low sample-efficiency is clearly a problem. Not only does it mean the experiment turn-around time is excessive, it also means that there can be little hope of bringing such methods to the real world. Can you imagine having to collect a billion real-world interactions? The paper we are considering takes a stab at this problem by bringing recent advances from vision and NLP to reinforcement learning. Contrastive learning takes advantage of data augmentation to learn more efficiently. CURL shows that it can be very useful in the context of RL to learn a good latent representation faster. What is contrastive learning? The core idea is to compare (contrast!) pairs of augmented samples. We consider two kinds of such pairs: Positive pairs consist of two different augmentations of the same sample Negative pairs contain augmentations of two different samples For each original sample, we create both positive and negative pairs. The contrastive representation is then learned by maximizing the agreement between positive pairs, and minimizing the agreement between negative pairs. Contrastive learning has seen dramatic progress in recent years for language and vision. See for example BERT, an application to masked language modeling 3 , or the SimCLR framework, used to learn visual representations 4 . The way contrastive learning is implemented in CURL is mostly influenced by the SimCLR framework 4 , Momentum Contrast (MoCo) 5 and Contrastive Predictive Coding (CPC) 6 . How CURL works With CURL, the same latent representation is used for both the RL algorithm and the contrastive learning, as illustrated below: 7 CURL uses random crops to augment the observations. Since most RL methods use frame-stacking, each observation is effectively a \"stack\" of sequential images. CURL preserves their temporal structure by applying the same augmentation to each frame in the stack. The illustration below gives an example of a positive pair: the same observation is augmented in two different ways. The representation will be changed in a way that maximizes their agreement. 7 Evaluation Using a contrastive objective as an auxiliary task appears to significantly improve the performance of the RL algorithm. CURL's performance is evaluated in two settings: with SAC on DeepMind Control Suite (continuous control) with data-efficient Rainbow DQN on Atari games (discrete control). In both cases, the performance is evaluated after 100k interactions, as the goal is to evaluate sample efficiency rather than asymptotic performance. Results are remarkable on DeepMind Control Suite : 7 (The last column, State SAC , uses physical states and is used as an \"oracle\" upper-bound.) Results are very good on Atari games. This is again after 100k interactions: 7 How can this difference in performance be explained? One idea explored in the appendix is that in some environments, there is simply not enough information to fully recover the state when looking only at the pixel data: the problem then becomes partially observable and therefore much harder. Key concepts \"Building blocks\" involved in this work: Focuses on sample efficiency Uses contrastive learning Uses auxiliary tasks Benchmarked on DeepMind Control Suite Benchmarked on Atari games Limitations \u274c No comparison with MuZero , which is SotA on multiple Atari games 8 Authors Aravind Srinivas Twitter / Scholar / Academic Michael Laskin Twitter / Scholar / Academic Pieter Abbeel Twitter / Scholar / Academic Links Official code repository , a PyTorch implementation for SAC on DeepMind Control Suite Official project page , which provides a short summary of the paper Twitter summary from first author Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor \u21a9 Rainbow: Combining Improvements in Deep Reinforcement Learning \u21a9 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding \u21a9 A Simple Framework for Contrastive Learning of Visual Representations (SimCLR) \u21a9 \u21a9 Momentum Contrast for Unsupervised Visual Representation Learning (MoCo) \u21a9 Data-Efficient Image Recognition with Contrastive Predictive Coding (CPC) \u21a9 excerpts from original paper \u21a9 \u21a9 \u21a9 \u21a9 https://twitter.com/gwern/status/1248087160391163906 \u21a9","title":"CURL: Contrastive Unsupervised Representations for Reinforcement Learning"},{"location":"curl/#curl-contrastive-unsupervised-representations-for-reinforcement-learning","text":"Tldr CURL improves the sample efficiency when learning from pixels by using contrastive learning, a self-supervised method, as an auxiliary task. The CURL contrastive objective works in a similar way as the SimCLR framework, using random cropping as an augmentation method. After 100k interactions, CURL outperforms all other methods on DM Control Suite, and shows strong results on Atari games. April 2020 - arXiv - Code","title":"CURL: Contrastive Unsupervised Representations for Reinforcement Learning"},{"location":"curl/#what-is-this-about","text":"In reinforcement learning, solving a task from pixels is much harder than solving an equivalent task using \"physical\" features such as coordinates and angles. This makes sense: you can consider an image as a high-dimensional vector containing hundreds of features, which don't have any clear connection with the goal of the environment! As a result, you generally need a lot more interactions between an agent and its environment in order to learn a good policy from pixels. For example, the figure below shows the results of multiple recent RL methods on the DeepMind Control Suite, learning from physical features. Take a look at the x-axis: depending on the complexity of the task, the agents are trained from 1 to 10 million interactions: 1 On the other hand, let's have a look at results using Atari games as a benchmark. There is no notion of \"physical\" features with the Atari emulator: the only observations the agent can work with are RGB images. The agents now have to be trained for a staggering 200 million frames! 2 Some distributed approaches even consider numbers up to a billion. This low sample-efficiency is clearly a problem. Not only does it mean the experiment turn-around time is excessive, it also means that there can be little hope of bringing such methods to the real world. Can you imagine having to collect a billion real-world interactions? The paper we are considering takes a stab at this problem by bringing recent advances from vision and NLP to reinforcement learning. Contrastive learning takes advantage of data augmentation to learn more efficiently. CURL shows that it can be very useful in the context of RL to learn a good latent representation faster.","title":"What is this about?"},{"location":"curl/#what-is-contrastive-learning","text":"The core idea is to compare (contrast!) pairs of augmented samples. We consider two kinds of such pairs: Positive pairs consist of two different augmentations of the same sample Negative pairs contain augmentations of two different samples For each original sample, we create both positive and negative pairs. The contrastive representation is then learned by maximizing the agreement between positive pairs, and minimizing the agreement between negative pairs. Contrastive learning has seen dramatic progress in recent years for language and vision. See for example BERT, an application to masked language modeling 3 , or the SimCLR framework, used to learn visual representations 4 . The way contrastive learning is implemented in CURL is mostly influenced by the SimCLR framework 4 , Momentum Contrast (MoCo) 5 and Contrastive Predictive Coding (CPC) 6 .","title":"What is contrastive learning?"},{"location":"curl/#how-curl-works","text":"With CURL, the same latent representation is used for both the RL algorithm and the contrastive learning, as illustrated below: 7 CURL uses random crops to augment the observations. Since most RL methods use frame-stacking, each observation is effectively a \"stack\" of sequential images. CURL preserves their temporal structure by applying the same augmentation to each frame in the stack. The illustration below gives an example of a positive pair: the same observation is augmented in two different ways. The representation will be changed in a way that maximizes their agreement. 7","title":"How CURL works"},{"location":"curl/#evaluation","text":"Using a contrastive objective as an auxiliary task appears to significantly improve the performance of the RL algorithm. CURL's performance is evaluated in two settings: with SAC on DeepMind Control Suite (continuous control) with data-efficient Rainbow DQN on Atari games (discrete control). In both cases, the performance is evaluated after 100k interactions, as the goal is to evaluate sample efficiency rather than asymptotic performance. Results are remarkable on DeepMind Control Suite : 7 (The last column, State SAC , uses physical states and is used as an \"oracle\" upper-bound.) Results are very good on Atari games. This is again after 100k interactions: 7 How can this difference in performance be explained? One idea explored in the appendix is that in some environments, there is simply not enough information to fully recover the state when looking only at the pixel data: the problem then becomes partially observable and therefore much harder.","title":"Evaluation"},{"location":"curl/#key-concepts","text":"\"Building blocks\" involved in this work: Focuses on sample efficiency Uses contrastive learning Uses auxiliary tasks Benchmarked on DeepMind Control Suite Benchmarked on Atari games","title":"Key concepts"},{"location":"curl/#limitations","text":"\u274c No comparison with MuZero , which is SotA on multiple Atari games 8","title":"Limitations"},{"location":"curl/#authors","text":"Aravind Srinivas Twitter / Scholar / Academic Michael Laskin Twitter / Scholar / Academic Pieter Abbeel Twitter / Scholar / Academic","title":"Authors"},{"location":"curl/#links","text":"Official code repository , a PyTorch implementation for SAC on DeepMind Control Suite Official project page , which provides a short summary of the paper Twitter summary from first author Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor \u21a9 Rainbow: Combining Improvements in Deep Reinforcement Learning \u21a9 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding \u21a9 A Simple Framework for Contrastive Learning of Visual Representations (SimCLR) \u21a9 \u21a9 Momentum Contrast for Unsupervised Visual Representation Learning (MoCo) \u21a9 Data-Efficient Image Recognition with Contrastive Predictive Coding (CPC) \u21a9 excerpts from original paper \u21a9 \u21a9 \u21a9 \u21a9 https://twitter.com/gwern/status/1248087160391163906 \u21a9","title":"Links"},{"location":"david-silver-ucl/","text":"David Silver at UCL https://www.davidsilver.uk/teaching/ Lecture 7: Policy Gradient Methods https://www.youtube.com/watch?v=KHZVXao4qXs https://www.davidsilver.uk/wp-content/uploads/2020/03/pg.pdf Why use policy-based methods? \"The TD error is an unbiaised estimate of the advantage function\"","title":"David Silver at UCL"},{"location":"david-silver-ucl/#david-silver-at-ucl","text":"https://www.davidsilver.uk/teaching/","title":"David Silver at UCL"},{"location":"david-silver-ucl/#lecture-7-policy-gradient-methods","text":"https://www.youtube.com/watch?v=KHZVXao4qXs https://www.davidsilver.uk/wp-content/uploads/2020/03/pg.pdf Why use policy-based methods? \"The TD error is an unbiaised estimate of the advantage function\"","title":"Lecture 7: Policy Gradient Methods"},{"location":"distributed-training/","text":"","title":"Distributed training"},{"location":"dm-control-suite/","text":"TODO - https://arxiv.org/abs/1801.00690 - https://github.com/deepmind/dm_control DeepMind Control: Recently, there have been a number of papers that have benchmarked for sample efficiency on challenging visual continuous control tasks belonging to the DMControl suite (Tassa et al., 2018) where the agent operates purely from pixels. The reason for operating in these environments is multi fold: (i) they present a reasonably challenging and diverse set of tasks; (ii) sample-efficiency of pure model-free RL algorithms operating from pixels on these benchmarks is poor; (iii) multiple recent efforts to improve the sample efficiency of both model-free and model-based methods on these benchmarks thereby giving us sufficient baselines to compare against; (iv) performance on the DM control suite is relevant to robot learning in real world benchmarks. -- CURL paper","title":"Dm control suite"},{"location":"experience-replay/","text":"Experience replay (Lin, 1992) has gained popularity in deep Q-learning (Mnih et al., 2015; Schaul et al., 2016; Wang et al., 2016; Narasimhan et al., 2015), where it is often motivated as a technique for reducing sample correlation. Replay is actually a valuable tool for improving sample efficiency -- ACER","title":"Experience replay"},{"location":"foundational-papers/","text":"Foundational Papers","title":"Foundational Papers"},{"location":"foundational-papers/#foundational-papers","text":"","title":"Foundational Papers"},{"location":"gae/","text":"High-Dimensional Continuous Control Using Generalized Advantage Estimation June 2015 - https://arxiv.org/abs/1506.02438 Abstract Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.","title":"High-Dimensional Continuous Control Using Generalized Advantage Estimation"},{"location":"gae/#high-dimensional-continuous-control-using-generalized-advantage-estimation","text":"June 2015 - https://arxiv.org/abs/1506.02438 Abstract Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.","title":"High-Dimensional Continuous Control Using Generalized Advantage Estimation"},{"location":"how-to-benchmark/","text":"","title":"How to benchmark"},{"location":"how-to-evaluate/","text":"A table of results and learning curves for all 49 games is provided in Appendix B. We consider the following two scoring metrics: (1) average reward per episode over entire training period (which favors fast learning), and (2) average reward per episode over last 100 episodes of training (which favors final performance). Table 2 shows the number of games \u201cwon\u201d by each algorithm, where we compute the victor by averaging the scoring metric across three trials. -- PPO paper","title":"How to evaluate"},{"location":"methods-comparison/","text":"Which RL Method Should I Use? Policy-based vs value-based \"Advantages of Policy-Based RL\" https://youtu.be/KHZVXao4qXs?t=625 - better convergence properties - effective in high-dimensional or continuous action spaces (no need to compute the max, a problem when continuous) - can learn stochastic properties - useful eg rock-paper-scicorss: less predictable - aliased states: may lead to situations where it is stuck! problems (at least naive methods): - convergence to local optimum - high variance ^silver-lecture-7","title":"Which RL Method Should I Use?"},{"location":"methods-comparison/#which-rl-method-should-i-use","text":"","title":"Which RL Method Should I Use?"},{"location":"methods-comparison/#policy-based-vs-value-based","text":"\"Advantages of Policy-Based RL\" https://youtu.be/KHZVXao4qXs?t=625 - better convergence properties - effective in high-dimensional or continuous action spaces (no need to compute the max, a problem when continuous) - can learn stochastic properties - useful eg rock-paper-scicorss: less predictable - aliased states: may lead to situations where it is stuck! problems (at least naive methods): - convergence to local optimum - high variance ^silver-lecture-7","title":"Policy-based vs value-based"},{"location":"model-free/","text":"AKA \"direct learning technique; it does not require a model to be given or learned\" (Baird, http://leemon.com/papers/1993b.pdf)","title":"Model free"},{"location":"muzero/","text":"","title":"Muzero"},{"location":"policy-based-methods/","text":"Policy-Based Methods gradient free methods: easy to scale, but don't work so well with too many parameters policy gradient methods Different ways to do policy optimization: https://youtu.be/KHZVXao4qXs?t=1532 - gradient free, eg evolution methods - gradient based, eg using gradient descent, see policy gradient methods","title":"Policy-Based Methods"},{"location":"policy-based-methods/#policy-based-methods","text":"gradient free methods: easy to scale, but don't work so well with too many parameters policy gradient methods Different ways to do policy optimization: https://youtu.be/KHZVXao4qXs?t=1532 - gradient free, eg evolution methods - gradient based, eg using gradient descent, see policy gradient methods","title":"Policy-Based Methods"},{"location":"policy-gradient-methods/","text":"Policy Gradient Methods Part of policy-based methods The existing variants applicable to both continuous and discrete domains, such as the on-policy asynchronous advantage actor critic (A3C) of Mnih et al. (2016), are sample inefficient. -- ACER Silver lecture on PG methods: https://www.youtube.com/watch?v=KHZVXao4qXs Links The best explanation of policy gradient is probably the lecture from David Silver at UCL This post highlights how policy gradient can be seen as a way to do supervised learning without a true label: https://amoudgl.github.io/blog/policy-gradient/","title":"Policy Gradient Methods"},{"location":"policy-gradient-methods/#policy-gradient-methods","text":"Part of policy-based methods The existing variants applicable to both continuous and discrete domains, such as the on-policy asynchronous advantage actor critic (A3C) of Mnih et al. (2016), are sample inefficient. -- ACER Silver lecture on PG methods: https://www.youtube.com/watch?v=KHZVXao4qXs","title":"Policy Gradient Methods"},{"location":"policy-gradient-methods/#links","text":"The best explanation of policy gradient is probably the lecture from David Silver at UCL This post highlights how policy gradient can be seen as a way to do supervised learning without a true label: https://amoudgl.github.io/blog/policy-gradient/","title":"Links"},{"location":"ppo/","text":"Proximal Policy Optimization Algorithms (PPO) Tldr Published July 2017 - highly influential (1673 citations) - arXiv Used all the time by OpenAI: DotA, robotic manipulation...","title":"Proximal Policy Optimization Algorithms (PPO)"},{"location":"ppo/#proximal-policy-optimization-algorithms-ppo","text":"Tldr Published July 2017 - highly influential (1673 citations) - arXiv Used all the time by OpenAI: DotA, robotic manipulation...","title":"Proximal Policy Optimization Algorithms (PPO)"},{"location":"rainbow/","text":"Rainbow: Combining Improvements in Deep Reinforcement Learning Published October 2017 - arXiv Main concepts Context At this point, Links Rainbow DQN (Hessel et al., 2017) is best summarized as multiple improvements on top of the original Nature DQN (Mnih et al., 2015) applied together. Specifically, Deep Q Network (DQN) (Mnih et al., 2015) combines the off-policy algorithm Q-Learning with a convolutional neural network as the function approximator to map raw pixels to action value functions. Since then, multiple improvements have been proposed such as Double Q Learning (Van Hasselt et al., 2016), Dueling Network Architectures (Wang et al., 2015), Prioritized Experience Replay (Schaul et al., 2015), and Noisy Networks (Fortunato et al., 2017). Additionally, distributional reinforcement learning (Bellemare et al., 2017) proposed the technique of predicting a distribution over possible value function bins through the C51 Algorithm. Rainbow DQN combines all of the above techniques into a single off-policy algorithm for state-of-the-art sample efficiency on Atari benchmarks. Additionally, Rainbow al -- CURL paper","title":"Rainbow: Combining Improvements in Deep Reinforcement Learning"},{"location":"rainbow/#rainbow-combining-improvements-in-deep-reinforcement-learning","text":"Published October 2017 - arXiv","title":"Rainbow: Combining Improvements in Deep Reinforcement Learning"},{"location":"rainbow/#main-concepts","text":"","title":"Main concepts"},{"location":"rainbow/#context","text":"At this point,","title":"Context"},{"location":"rainbow/#links","text":"Rainbow DQN (Hessel et al., 2017) is best summarized as multiple improvements on top of the original Nature DQN (Mnih et al., 2015) applied together. Specifically, Deep Q Network (DQN) (Mnih et al., 2015) combines the off-policy algorithm Q-Learning with a convolutional neural network as the function approximator to map raw pixels to action value functions. Since then, multiple improvements have been proposed such as Double Q Learning (Van Hasselt et al., 2016), Dueling Network Architectures (Wang et al., 2015), Prioritized Experience Replay (Schaul et al., 2015), and Noisy Networks (Fortunato et al., 2017). Additionally, distributional reinforcement learning (Bellemare et al., 2017) proposed the technique of predicting a distribution over possible value function bins through the C51 Algorithm. Rainbow DQN combines all of the above techniques into a single off-policy algorithm for state-of-the-art sample efficiency on Atari benchmarks. Additionally, Rainbow al -- CURL paper","title":"Links"},{"location":"rl-approaches/","text":"gradient free value-based policy-based model based vs model free","title":"Rl approaches"},{"location":"sac/","text":"SAC Tldr Short summary Published November XXXX - Influential (XXX citations) - arXiv Summary Key concepts - - Legacy","title":"SAC"},{"location":"sac/#sac","text":"Tldr Short summary Published November XXXX - Influential (XXX citations) - arXiv","title":"SAC"},{"location":"sac/#summary","text":"","title":"Summary"},{"location":"sac/#key-concepts","text":"","title":"Key concepts"},{"location":"sac/#-","text":"-","title":"-"},{"location":"sac/#legacy","text":"","title":"Legacy"},{"location":"sample-efficiency/","text":"TODO - Improving Sample Efficiency in Model-Free Reinforcement Learning from Images https://arxiv.org/abs/1910.01741 This need for sample efficiency is even more compelling when agents are deployed in the real world -- ACER paper A number of approaches have been proposed in the literature to address the sample inefficiency of deep RL algorithms. Broadly, they can be classified into two streams of research, though not mutually exclusive: (i) Auxiliary tasks on the agent\u2019s sensory observations; (ii) World models that predict the future. While the former class of methods use auxiliary self-supervision tasks to accelerate the learning progress of model-free RL methods (Jaderberg et al., 2016; Mirowski et al., 2016), the latter class of methods build explicit predictive models of the world and use those models to plan through or collect fictitious rollouts for model-free methods to learn from (Sutton, 1990; Ha & Schmidhuber, 2018; Kaiser et al., 2019; Schrittwieser et al., 2019). -- CURL paper The DMControl suite has been used widely by Yarats et al. (2019), Hafner et al. (2018), Hafner et al. (2019) and Lee et al. (2019) for benchmarking sample-efficiency for image based continuous control methods. As for Atari, Kaiser et al. (2019) propose to use the 100k interaction steps benchmark for sample-efficiency which has been adopted in Kielak (2020); van Hasselt et al. (2019). The Rainbow DQN (Hessel et al., 2017) was originally proposed for maximum sample-efficiency on the Atari benchmark and in recent times has been adapted to a version known as DataEfficient Rainbow (van Hasselt et al., 2019) with competitive performance to SimPLe without learning world models. -- CURL paper We measure the data-efficiency and performance of our method and baselines at 100k interaction steps on both DMControl and Atari, which we will henceforth refer to as DMControl100k and Atari100k for clarity. Benchmarking at 100k steps makes for a fixed experimental setup that is easy to reproduce and has been common practice when investigating data-efficiency on Atari Kaiser et al. (2019); Kielak (2020). A broader motivation is that while RL al- CURL: Contrastive Unsupervised Representations for Reinforcement Learning gorithms can achieve super-human performance on many Atari games, they are still far from the data-efficiency of a human learner. Training for 100k steps is within the order of magnitude that we would expect for humans to a learn similar tasks. In our experiments, 100k steps corresponds to 300-400k frames (due to using a frame-skip of 3 or 4), which equates to roughly a 2-4 hours of human game play. -- CURL paper","title":"Sample efficiency"},{"location":"tips-and-tricks/","text":"RL Tips and Tricks Find Twitter thread! Study each topic in depth instead of covering as much as you can Do read old papers in details. You will be surprised how many old insights keep coming back, often presented as new.","title":"RL Tips and Tricks"},{"location":"tips-and-tricks/#rl-tips-and-tricks","text":"Find Twitter thread! Study each topic in depth instead of covering as much as you can Do read old papers in details. You will be surprised how many old insights keep coming back, often presented as new.","title":"RL Tips and Tricks"},{"location":"value-based-methods/","text":"Value-Based Methods However, we need to do better than deep Q-learning, because it has two important limitations. First, the deterministic nature of the optimal policy limits its use in adversarial domains. Second, finding the greedy action with respect to the Q function is costly for large action spaces. -- ACER https://arxiv.org/pdf/1611.01224.pdf Such methods are indirect in the sense that they do not try to optimize directly over a policy space. A method of this type may succeed in constructing a \"good\" approximation of the value function, yet lack reliable guarantees in terms of near-optimality of the resulting policy. -- Actor Critic https://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf","title":"Value-Based Methods"},{"location":"value-based-methods/#value-based-methods","text":"However, we need to do better than deep Q-learning, because it has two important limitations. First, the deterministic nature of the optimal policy limits its use in adversarial domains. Second, finding the greedy action with respect to the Q function is costly for large action spaces. -- ACER https://arxiv.org/pdf/1611.01224.pdf Such methods are indirect in the sense that they do not try to optimize directly over a policy space. A method of this type may succeed in constructing a \"good\" approximation of the value function, yet lack reliable guarantees in terms of near-optimality of the resulting policy. -- Actor Critic https://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf","title":"Value-Based Methods"},{"location":"what-is-rl/","text":"What is Reinforcement Learning? Reinforcement learning is a sub-field of machine learning. In reinforcement learning, our goal is to learn a behavior . Let's consider an example. How would you design a self-driving car? Assume you want to design a vehicle to drive you from Paris to Berlin. Many recent improvement in AI will come handy - image recognition could be used to locate other cars and pedestrians around you, text recognition will let you read the signs on the road, etc. However, the car ultimately needs to take decisions to bring you from point A to point B. This is where reinforcement learning takes the stage. In reinforcement learning, we train an agent to decide which action should be performed given the current situation. In the case of our self-driving car, this agent could take low-level decisions such as the angle of the steering weel or the pressure on the gas pedal. It could also take higher-level decisions, such as the general way to take to reach the destination as fast as possible. This kind of problems can't be handled by traditional supervised and unsupervised learning approaches. The diagram below shows how the sub-fields of machine learning interact. How to learn a behavior The core question of reinforcement learning is as follow: Given the current state, what is the optimal action in order to maximize the expected reward? There are many approaches to tackle this question. Evolution strategies The simplest, most intuitive approach may be the evolution strategy family of algorithms. The idea is Value-based methods More details about value-based methods... Reinforcement learning lingo In general, the agent can not access an exhaustive description of its environment. Instead, it needs to work with an observation of that state. A short history of reinforcement learning The history of reinforcement learning is actually nothing but short. The field as it exists today combines elements from different directions.","title":"What is Reinforcement Learning?"},{"location":"what-is-rl/#what-is-reinforcement-learning","text":"Reinforcement learning is a sub-field of machine learning. In reinforcement learning, our goal is to learn a behavior . Let's consider an example. How would you design a self-driving car? Assume you want to design a vehicle to drive you from Paris to Berlin. Many recent improvement in AI will come handy - image recognition could be used to locate other cars and pedestrians around you, text recognition will let you read the signs on the road, etc. However, the car ultimately needs to take decisions to bring you from point A to point B. This is where reinforcement learning takes the stage. In reinforcement learning, we train an agent to decide which action should be performed given the current situation. In the case of our self-driving car, this agent could take low-level decisions such as the angle of the steering weel or the pressure on the gas pedal. It could also take higher-level decisions, such as the general way to take to reach the destination as fast as possible. This kind of problems can't be handled by traditional supervised and unsupervised learning approaches. The diagram below shows how the sub-fields of machine learning interact.","title":"What is Reinforcement Learning?"},{"location":"what-is-rl/#how-to-learn-a-behavior","text":"The core question of reinforcement learning is as follow: Given the current state, what is the optimal action in order to maximize the expected reward? There are many approaches to tackle this question.","title":"How to learn a behavior"},{"location":"what-is-rl/#evolution-strategies","text":"The simplest, most intuitive approach may be the evolution strategy family of algorithms. The idea is","title":"Evolution strategies"},{"location":"what-is-rl/#value-based-methods","text":"More details about value-based methods...","title":"Value-based methods"},{"location":"what-is-rl/#reinforcement-learning-lingo","text":"In general, the agent can not access an exhaustive description of its environment. Instead, it needs to work with an observation of that state.","title":"Reinforcement learning lingo"},{"location":"what-is-rl/#a-short-history-of-reinforcement-learning","text":"The history of reinforcement learning is actually nothing but short. The field as it exists today combines elements from different directions.","title":"A short history of reinforcement learning"},{"location":"why-dnq-not-converging/","text":"Why is my DQN implementation not converging? \"Implementing the DQN\"","title":"Why is my DQN implementation not converging?"},{"location":"why-dnq-not-converging/#why-is-my-dqn-implementation-not-converging","text":"\"Implementing the DQN\"","title":"Why is my DQN implementation not converging?"}]}