{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RL Insights \u00b6 Warning This site is in its early days, most pages are still empty! Pages with actual content so far: CURL: Contrastive Unsupervised Representations for Reinforcement Learning Asynchronous Methods for Deep Reinforcement Learning (A3C) Site structure \u00b6 Papers : provides summaries, key concepts, and links to blog posts and notebooks. Concepts : cross-references RL \"building blocks\" across papers. FAQ : curated answers to common questions. New to RL? \u00b6 While this site is meant as a repository for research papers, here are some good starting points: What is reinforcement learning? A quick intro to what this is all about. Best RL educational resources A number of excellent articles and courses to get started on this topic. Foundational papers Most of the RL knowledge still only exists in papers - these provide a good starting point.","title":"Home"},{"location":"#rl-insights","text":"Warning This site is in its early days, most pages are still empty! Pages with actual content so far: CURL: Contrastive Unsupervised Representations for Reinforcement Learning Asynchronous Methods for Deep Reinforcement Learning (A3C)","title":"RL Insights"},{"location":"#site-structure","text":"Papers : provides summaries, key concepts, and links to blog posts and notebooks. Concepts : cross-references RL \"building blocks\" across papers. FAQ : curated answers to common questions.","title":"Site structure"},{"location":"#new-to-rl","text":"While this site is meant as a repository for research papers, here are some good starting points: What is reinforcement learning? A quick intro to what this is all about. Best RL educational resources A number of excellent articles and courses to get started on this topic. Foundational papers Most of the RL knowledge still only exists in papers - these provide a good starting point.","title":"New to RL?"},{"location":"a3c/","text":"Asynchronous Methods for Deep Reinforcement Learning \u00b6 Warning Under construction! This page is still incomplete. Tldr Introduces a parallel training method that uses multiple CPU cores to speed up training on a single machine. The main result is A3C, a parallel actor-critic method using an LSTM layer, n-step returns and entropy regularization. Also investigates parallel training with DQN, but with less convincing results. A non-asynchronous version called A2C, geared towards GPUs, is generally preferred nowadays. 1 Published February 2016 - arXiv Learning faster \u00b6 Let's start with some context. When this paper comes out in February 2016, DQN had already shown the promise of \"deep\" reinforcement learning, with extensions such as Double DQN and Prioritized Experience Replay already published - but the Rainbow agent that would combine them all was still a year away. On the policy gradient front, TPRO was out, but not yet PPO . ~ The concerns of learning in a stable and efficient way were even more at the forefront than right now. The replay buffer is an efficient solution against observation non-stationarity. But: - uses memory (yes) - can't be used for off-po methods! (YES) To address these concerns, instead of using a replay buffer, this method executes multiple agents in parallel: - no need There are multiple ways to consider efficiency of RL training: - sample efficiency - time efficiency This method, as presented, has very low sample efficiency (each observation is used only once, or a few times due to n-step learning) but can be scaled up easily. Note that for off-policy method, it could very well be combined with a replay buffer! A stated goal of this approach is to rely only on CPU. This allows Howgwild-style updates. However, a different method called A2C, which does leverage GPUs, is preferred today. The asynchronous RL framework Summary \u00b6 This paper investigates a new parallel training method which uses multiple CPU cores on the same machine, rather than relying on GPUs. Both value-based and policy-based methods are considered. The major result is the performance of this training scheme using an actor-critic method . This training scheme also shows promising results for value-based methods. However, these methods being off-policy, they can be significantly improved with the use of a replay buffer. The core idea is to use multiple asynchronous \"actor-learners\" running in different threads. Each actor-learner performs a number of learning steps, then perform an asynchronous update of the global parameters using accumulated gradients. Since this method is designed to train networks using a CPU, asynchronous updates made a lot of sense as it enabled Hogwild syle updates. However, it appeared later on that this was not optimal when using a GPU. For this reason, this method is commonly used today in a synchronous way in which the updates from all the actor-learners are batched to make better use of the GPU. This non-asynchronous version is simply called A2C (Advantage Actor Critic) [^openai-acktr-a2c]. In hindsight \u00b6 \u2705 Using multiple environments in parallel is a good way to decorrelate the agent's data \u274c Asynchronous updates are not a good match for GPU training \u274c Using exclusively on-policy data is very bad for sample efficiency Key concepts \u00b6 \"Building blocks\" involved in this work: Builds on Gorila distributed training system Builds on DQN and Actor-Critic methods Introduces a single-machine parallel training method Uses entropy regularization Uses an LSTM Recurrent Neural Network layer Uses multi-step return with the forward view Uses shared layers between policy and value functions Benchmarked on Atari games Links \u00b6 Actor-Critic Methods: A3C and A2C from Daniel Seita Authors \u00b6 Volodymyr Mnih Twitter / Scholar / Academic TODO! Adri\u00e0 Puigdom\u00e8nech Badia Twitter / Scholar / Academic Mehdi Mirza Twitter / Scholar / Academic Alex Graves Twitter / Scholar / Academic Tim Harley Twitter / Scholar / Academic Timothy P. Lillicrap Twitter / Scholar / Academic David Silver Twitter / Scholar / Academic Koray Kavukcuoglu Twitter / Scholar / Academic OpenAI Baselines: ACKTR & A2C \u21a9","title":"Asynchronous Methods for Deep Reinforcement Learning"},{"location":"a3c/#asynchronous-methods-for-deep-reinforcement-learning","text":"Warning Under construction! This page is still incomplete. Tldr Introduces a parallel training method that uses multiple CPU cores to speed up training on a single machine. The main result is A3C, a parallel actor-critic method using an LSTM layer, n-step returns and entropy regularization. Also investigates parallel training with DQN, but with less convincing results. A non-asynchronous version called A2C, geared towards GPUs, is generally preferred nowadays. 1 Published February 2016 - arXiv","title":"Asynchronous Methods for Deep Reinforcement Learning"},{"location":"a3c/#learning-faster","text":"Let's start with some context. When this paper comes out in February 2016, DQN had already shown the promise of \"deep\" reinforcement learning, with extensions such as Double DQN and Prioritized Experience Replay already published - but the Rainbow agent that would combine them all was still a year away. On the policy gradient front, TPRO was out, but not yet PPO . ~ The concerns of learning in a stable and efficient way were even more at the forefront than right now. The replay buffer is an efficient solution against observation non-stationarity. But: - uses memory (yes) - can't be used for off-po methods! (YES) To address these concerns, instead of using a replay buffer, this method executes multiple agents in parallel: - no need There are multiple ways to consider efficiency of RL training: - sample efficiency - time efficiency This method, as presented, has very low sample efficiency (each observation is used only once, or a few times due to n-step learning) but can be scaled up easily. Note that for off-policy method, it could very well be combined with a replay buffer! A stated goal of this approach is to rely only on CPU. This allows Howgwild-style updates. However, a different method called A2C, which does leverage GPUs, is preferred today. The asynchronous RL framework","title":"Learning faster"},{"location":"a3c/#summary","text":"This paper investigates a new parallel training method which uses multiple CPU cores on the same machine, rather than relying on GPUs. Both value-based and policy-based methods are considered. The major result is the performance of this training scheme using an actor-critic method . This training scheme also shows promising results for value-based methods. However, these methods being off-policy, they can be significantly improved with the use of a replay buffer. The core idea is to use multiple asynchronous \"actor-learners\" running in different threads. Each actor-learner performs a number of learning steps, then perform an asynchronous update of the global parameters using accumulated gradients. Since this method is designed to train networks using a CPU, asynchronous updates made a lot of sense as it enabled Hogwild syle updates. However, it appeared later on that this was not optimal when using a GPU. For this reason, this method is commonly used today in a synchronous way in which the updates from all the actor-learners are batched to make better use of the GPU. This non-asynchronous version is simply called A2C (Advantage Actor Critic) [^openai-acktr-a2c].","title":"Summary"},{"location":"a3c/#in-hindsight","text":"\u2705 Using multiple environments in parallel is a good way to decorrelate the agent's data \u274c Asynchronous updates are not a good match for GPU training \u274c Using exclusively on-policy data is very bad for sample efficiency","title":"In hindsight"},{"location":"a3c/#key-concepts","text":"\"Building blocks\" involved in this work: Builds on Gorila distributed training system Builds on DQN and Actor-Critic methods Introduces a single-machine parallel training method Uses entropy regularization Uses an LSTM Recurrent Neural Network layer Uses multi-step return with the forward view Uses shared layers between policy and value functions Benchmarked on Atari games","title":"Key concepts"},{"location":"a3c/#links","text":"Actor-Critic Methods: A3C and A2C from Daniel Seita","title":"Links"},{"location":"a3c/#authors","text":"Volodymyr Mnih Twitter / Scholar / Academic TODO! Adri\u00e0 Puigdom\u00e8nech Badia Twitter / Scholar / Academic Mehdi Mirza Twitter / Scholar / Academic Alex Graves Twitter / Scholar / Academic Tim Harley Twitter / Scholar / Academic Timothy P. Lillicrap Twitter / Scholar / Academic David Silver Twitter / Scholar / Academic Koray Kavukcuoglu Twitter / Scholar / Academic OpenAI Baselines: ACKTR & A2C \u21a9","title":"Authors"},{"location":"acer/","text":"Sample Efficient Actor-Critic with Experience Replay (ACER) \u00b6 Warning Under construction! This page is still vastly incomplete. Tldr ACER - Actor-Critic with Experience Replay extends the parallel implementation of actor-critic methods described in A3C to the off-policy setting. Published November 2016 - Influential (251 citations) - arXiv Summary \u00b6 \"ACER may be understood as the off-policy counterpart of the A3C method .\" Key concepts \u00b6 Uses ?? for variance reduction (GAE?) Uses Generalized Advantage Estimation Uses the the off-policy Retrace algorithm Uses parallel-training as in A3C Introduces Truncated Importance Sampling Introduces stochastic dueling network architectures Introduces efficient trust region policy optimization Legacy \u00b6 Used in PPO (?)","title":"Sample Efficient Actor-Critic with Experience Replay (ACER)"},{"location":"acer/#sample-efficient-actor-critic-with-experience-replay-acer","text":"Warning Under construction! This page is still vastly incomplete. Tldr ACER - Actor-Critic with Experience Replay extends the parallel implementation of actor-critic methods described in A3C to the off-policy setting. Published November 2016 - Influential (251 citations) - arXiv","title":"Sample Efficient Actor-Critic with Experience Replay (ACER)"},{"location":"acer/#summary","text":"\"ACER may be understood as the off-policy counterpart of the A3C method .\"","title":"Summary"},{"location":"acer/#key-concepts","text":"Uses ?? for variance reduction (GAE?) Uses Generalized Advantage Estimation Uses the the off-policy Retrace algorithm Uses parallel-training as in A3C Introduces Truncated Importance Sampling Introduces stochastic dueling network architectures Introduces efficient trust region policy optimization","title":"Key concepts"},{"location":"acer/#legacy","text":"Used in PPO (?)","title":"Legacy"},{"location":"actor-critic/","text":"Actor-Critic Algorithms \u00b6 Tldr Introduces Actor-Critic Algorithms . Published February 2016 - Very Influential - Link Summary \u00b6 This paper introduces the the notion of actor-critic methods, which is the basis of a large part of the reinforcement learning methods today. This idea is to combine a value-function approximation, the critic, which helps reduce the variance of a policy-gradient approximation, the actor. This idea will further be extended with the notion of advantage . Note that this work only considers linear approximations, ie it doesn't use neural networks. In the actor-critic setting, learning the critic can be seen as a supervised learning problem. This means it can either be learned using TD-learning, or simply least square. It is important to understand that the goal of the critic is to approximate the returns that the actor will receive, and not the returns of the optimal policy. But the critic is regularly updated, which means that the critic is trying to learn a moving target! in practice, this is not a problem as the actor is updated more slowly than the critic Silver lecture on PG methods: https://www.youtube.com/watch?v=KHZVXao4qXs (actor-critic from https://youtu.be/KHZVXao4qXs?t=3178) Pretty much all the gradient-based approaches are now actor-critic methods, so this doesn't mean much anymore! https://github.com/openai/spinningup/issues/156#issuecomment-494596739 TODO cover Degris paper, the original resource for off-policy AC: https://arxiv.org/abs/1205.4839","title":"Actor-Critic Algorithms"},{"location":"actor-critic/#actor-critic-algorithms","text":"Tldr Introduces Actor-Critic Algorithms . Published February 2016 - Very Influential - Link","title":"Actor-Critic Algorithms"},{"location":"actor-critic/#summary","text":"This paper introduces the the notion of actor-critic methods, which is the basis of a large part of the reinforcement learning methods today. This idea is to combine a value-function approximation, the critic, which helps reduce the variance of a policy-gradient approximation, the actor. This idea will further be extended with the notion of advantage . Note that this work only considers linear approximations, ie it doesn't use neural networks. In the actor-critic setting, learning the critic can be seen as a supervised learning problem. This means it can either be learned using TD-learning, or simply least square. It is important to understand that the goal of the critic is to approximate the returns that the actor will receive, and not the returns of the optimal policy. But the critic is regularly updated, which means that the critic is trying to learn a moving target! in practice, this is not a problem as the actor is updated more slowly than the critic Silver lecture on PG methods: https://www.youtube.com/watch?v=KHZVXao4qXs (actor-critic from https://youtu.be/KHZVXao4qXs?t=3178) Pretty much all the gradient-based approaches are now actor-critic methods, so this doesn't mean much anymore! https://github.com/openai/spinningup/issues/156#issuecomment-494596739 TODO cover Degris paper, the original resource for off-policy AC: https://arxiv.org/abs/1205.4839","title":"Summary"},{"location":"atari/","text":"Atari Learning Environment \u00b6 Tldr Short summary Published November XXXX - Influential (XXX citations) - arXiv Summary \u00b6 Key concepts \u00b6 - \u00b6 - Legacy \u00b6","title":"Atari Learning Environment"},{"location":"atari/#atari-learning-environment","text":"Tldr Short summary Published November XXXX - Influential (XXX citations) - arXiv","title":"Atari Learning Environment"},{"location":"atari/#summary","text":"","title":"Summary"},{"location":"atari/#key-concepts","text":"","title":"Key concepts"},{"location":"atari/#-","text":"-","title":"-"},{"location":"atari/#legacy","text":"","title":"Legacy"},{"location":"auxiliary-tasks/","text":"UNREAL paper Self-Supervised Learning for RL: Auxiliary tasks such as predicting the future conditioned on the past observation(s) and action(s) (Jaderberg et al., 2016; Shelhamer et al., 2016; van den Oord et al., 2018), and predicting the depth image for maze navigation (Mirowski et al., 2016) are a few representative examples of using auxiliary tasks to improve the sample-efficiency of model-free RL algorithms. The future prediction is either done in a pixel space (Jaderberg et al., 2016) or latent space (van den Oord et al., 2018). The sample-efficiency gains from reconstruction-based auxiliary losses have been benchmarked in Jaderberg et al. (2016); Higgins et al. (2017); Yarats et al. (2019). Contrastive learning across has been used to extract reward signals characterized as distance metrics in the latent space by -- CURL paper","title":"Auxiliary tasks"},{"location":"best-resources/","text":"Best RL resources \u00b6","title":"Best RL resources"},{"location":"best-resources/#best-rl-resources","text":"","title":"Best RL resources"},{"location":"cem/","text":"Cross-Entropy Methods \u00b6 http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf I. Szita and A. L\u00a8orincz. \u201cLearning Tetris using the noisy cross-entropy method\u201d. In: Neural computation 18.12 (2006), pp. 2936\u20132941.","title":"Cross-Entropy Methods"},{"location":"cem/#cross-entropy-methods","text":"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf I. Szita and A. L\u00a8orincz. \u201cLearning Tetris using the noisy cross-entropy method\u201d. In: Neural computation 18.12 (2006), pp. 2936\u20132941.","title":"Cross-Entropy Methods"},{"location":"contrastive-learning/","text":"Contrastive Learning: Contrastive Learning is a framework to learn representations that obey similarity constraints in a dataset typically organized by similar and dissimilar pairs. This is often best understood as performing a dictionary lookup task wherein the positive and negatives represent a set of keys with respect to a query (or an anchor). A simple instantiation of contrastive learning is Instance Discrimination (Wu et al., 2018) wherein a query and key are positive pairs if they are data-augmentations of the same instance (example, image) and negative otherwise. A key challenge in contrastive learning is the choice of negatives which can decide the quality of the underlying representations learned. The loss functions used to contrast could be among several choices such as InfoNCE (van den Oord et al., 2018), Triplet (Wang & Gupta, 2015), Siamese (Chopra et al., 2005) and so forth. -- CURL paper","title":"Contrastive learning"},{"location":"curl/","text":"CURL: Contrastive Unsupervised Representations for Reinforcement Learning \u00b6 Tldr CURL improves the sample efficiency when learning from pixels by using contrastive learning, a self-supervised method, as an auxiliary task. The CURL contrastive objective works in a similar way as the SimCLR framework, using simple random cropping as an augmentation method. After 100k interactions, CURL outperforms all other methods on DM Control Suite, and shows strong results on Atari games. April 2020 - arXiv - Code Learning from pixels \u00b6 In reinforcement learning, solving a task from pixels is much harder than solving an equivalent task using \"physical\" features such as coordinates and angles. This makes sense: you can consider an image as a high-dimensional vector containing hundreds of features, which don't have any clear connection with the goal of the environment! As a result, you generally need a lot more interactions between an agent and its environment in order to learn a good policy from pixels. For example, the figure below shows the results of multiple recent RL methods on the DeepMind Control Suite, learning from physical features. Take a look at the x-axis: depending on the complexity of the task, the agents are trained from 1 to 10 million interactions: 1 On the other hand, let's have a look at results using Atari games as a benchmark. There is no notion of \"physical\" features with the Atari emulator: the only observations the agent can work with are RGB images. The agents now have to be trained for a staggering 200 million frames! 2 Some distributed approaches even consider numbers up to a billion. This low sample-efficiency is clearly a problem. Not only does it mean the experiment turn-around time is excessive, it also means that there can be little hope of bringing such methods to the real world. Can you imagine having to collect a billion real-world interactions? The paper we are considering takes a stab at this problem by bringing recent advances from vision and NLP to reinforcement learning. Contrastive learning takes advantage of data augmentation to learn more efficiently. CURL shows that it can be very useful in the context of RL to learn a good latent representation faster. What is contrastive learning? \u00b6 The core idea is to compare (contrast!) pairs of augmented samples. We consider two kinds of such pairs: Positive pairs consist of two different augmentations of the same sample Negative pairs contain augmentations of two different samples For each original sample, we create both positive and negative pairs. The contrastive representation is then learned by maximizing the agreement between positive pairs, and minimizing the agreement between negative pairs. Contrastive learning has seen dramatic progress in recent years for language and vision. See for example BERT, an application to masked language modeling 3 , or the SimCLR framework, used to learn visual representations 4 . The way contrastive learning is implemented in CURL is mostly influenced by the SimCLR framework 4 , Momentum Contrast (MoCo) 5 and Contrastive Predictive Coding (CPC) 6 . How CURL works \u00b6 With CURL, the same latent representation is used for both the RL algorithm and the contrastive learning, as illustrated below: 7 CURL uses random crops to augment the observations. Since most RL methods use frame-stacking, each observation is effectively a \"stack\" of sequential images. CURL preserves their temporal structure by applying the same augmentation to each frame in the stack. The illustration below gives an example of a positive pair: the same observation is augmented in two different ways. The representation will be changed in a way that maximizes their agreement. 7 Evaluation \u00b6 Using a contrastive objective as an auxiliary task appears to significantly improve the performance of the RL algorithm. CURL's performance is evaluated in two settings: with SAC on DeepMind Control Suite (continuous control) with data-efficient Rainbow DQN on Atari games (discrete control). In both cases, the performance is evaluated after 100k interactions, as the goal is to evaluate sample efficiency rather than asymptotic performance. Results are remarkable on DeepMind Control Suite : 7 (The last column, State SAC , uses physical states and is used as an \"oracle\" upper-bound.) Results are very good on Atari games. This is again after 100k interactions: 7 How can this difference in performance be explained? One idea explored in the appendix is that in some environments, there is simply not enough information to fully recover the state when looking only at the pixel data: the problem then becomes partially observable and therefore much harder. Key concepts \u00b6 \"Building blocks\" involved in this work: Focuses on sample efficiency Uses contrastive learning Uses auxiliary tasks Benchmarked on DeepMind Control Suite Benchmarked on Atari games Limitations \u00b6 \u274c No comparison with MuZero , which is SotA on multiple Atari games 8 Links \u00b6 Official code repository , a PyTorch implementation for SAC on DeepMind Control Suite Official project page , which provides a short summary of the paper Twitter summary from first author Authors \u00b6 Aravind Srinivas Twitter / Scholar / Academic Michael Laskin Twitter / Scholar / Academic Pieter Abbeel Twitter / Scholar / Academic Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor \u21a9 Rainbow: Combining Improvements in Deep Reinforcement Learning \u21a9 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding \u21a9 A Simple Framework for Contrastive Learning of Visual Representations (SimCLR) \u21a9 \u21a9 Momentum Contrast for Unsupervised Visual Representation Learning (MoCo) \u21a9 Data-Efficient Image Recognition with Contrastive Predictive Coding (CPC) \u21a9 excerpts from original paper \u21a9 \u21a9 \u21a9 \u21a9 https://twitter.com/gwern/status/1248087160391163906 \u21a9","title":"CURL: Contrastive Unsupervised Representations for Reinforcement Learning"},{"location":"curl/#curl-contrastive-unsupervised-representations-for-reinforcement-learning","text":"Tldr CURL improves the sample efficiency when learning from pixels by using contrastive learning, a self-supervised method, as an auxiliary task. The CURL contrastive objective works in a similar way as the SimCLR framework, using simple random cropping as an augmentation method. After 100k interactions, CURL outperforms all other methods on DM Control Suite, and shows strong results on Atari games. April 2020 - arXiv - Code","title":"CURL: Contrastive Unsupervised Representations for Reinforcement Learning"},{"location":"curl/#learning-from-pixels","text":"In reinforcement learning, solving a task from pixels is much harder than solving an equivalent task using \"physical\" features such as coordinates and angles. This makes sense: you can consider an image as a high-dimensional vector containing hundreds of features, which don't have any clear connection with the goal of the environment! As a result, you generally need a lot more interactions between an agent and its environment in order to learn a good policy from pixels. For example, the figure below shows the results of multiple recent RL methods on the DeepMind Control Suite, learning from physical features. Take a look at the x-axis: depending on the complexity of the task, the agents are trained from 1 to 10 million interactions: 1 On the other hand, let's have a look at results using Atari games as a benchmark. There is no notion of \"physical\" features with the Atari emulator: the only observations the agent can work with are RGB images. The agents now have to be trained for a staggering 200 million frames! 2 Some distributed approaches even consider numbers up to a billion. This low sample-efficiency is clearly a problem. Not only does it mean the experiment turn-around time is excessive, it also means that there can be little hope of bringing such methods to the real world. Can you imagine having to collect a billion real-world interactions? The paper we are considering takes a stab at this problem by bringing recent advances from vision and NLP to reinforcement learning. Contrastive learning takes advantage of data augmentation to learn more efficiently. CURL shows that it can be very useful in the context of RL to learn a good latent representation faster.","title":"Learning from pixels"},{"location":"curl/#what-is-contrastive-learning","text":"The core idea is to compare (contrast!) pairs of augmented samples. We consider two kinds of such pairs: Positive pairs consist of two different augmentations of the same sample Negative pairs contain augmentations of two different samples For each original sample, we create both positive and negative pairs. The contrastive representation is then learned by maximizing the agreement between positive pairs, and minimizing the agreement between negative pairs. Contrastive learning has seen dramatic progress in recent years for language and vision. See for example BERT, an application to masked language modeling 3 , or the SimCLR framework, used to learn visual representations 4 . The way contrastive learning is implemented in CURL is mostly influenced by the SimCLR framework 4 , Momentum Contrast (MoCo) 5 and Contrastive Predictive Coding (CPC) 6 .","title":"What is contrastive learning?"},{"location":"curl/#how-curl-works","text":"With CURL, the same latent representation is used for both the RL algorithm and the contrastive learning, as illustrated below: 7 CURL uses random crops to augment the observations. Since most RL methods use frame-stacking, each observation is effectively a \"stack\" of sequential images. CURL preserves their temporal structure by applying the same augmentation to each frame in the stack. The illustration below gives an example of a positive pair: the same observation is augmented in two different ways. The representation will be changed in a way that maximizes their agreement. 7","title":"How CURL works"},{"location":"curl/#evaluation","text":"Using a contrastive objective as an auxiliary task appears to significantly improve the performance of the RL algorithm. CURL's performance is evaluated in two settings: with SAC on DeepMind Control Suite (continuous control) with data-efficient Rainbow DQN on Atari games (discrete control). In both cases, the performance is evaluated after 100k interactions, as the goal is to evaluate sample efficiency rather than asymptotic performance. Results are remarkable on DeepMind Control Suite : 7 (The last column, State SAC , uses physical states and is used as an \"oracle\" upper-bound.) Results are very good on Atari games. This is again after 100k interactions: 7 How can this difference in performance be explained? One idea explored in the appendix is that in some environments, there is simply not enough information to fully recover the state when looking only at the pixel data: the problem then becomes partially observable and therefore much harder.","title":"Evaluation"},{"location":"curl/#key-concepts","text":"\"Building blocks\" involved in this work: Focuses on sample efficiency Uses contrastive learning Uses auxiliary tasks Benchmarked on DeepMind Control Suite Benchmarked on Atari games","title":"Key concepts"},{"location":"curl/#limitations","text":"\u274c No comparison with MuZero , which is SotA on multiple Atari games 8","title":"Limitations"},{"location":"curl/#links","text":"Official code repository , a PyTorch implementation for SAC on DeepMind Control Suite Official project page , which provides a short summary of the paper Twitter summary from first author","title":"Links"},{"location":"curl/#authors","text":"Aravind Srinivas Twitter / Scholar / Academic Michael Laskin Twitter / Scholar / Academic Pieter Abbeel Twitter / Scholar / Academic Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor \u21a9 Rainbow: Combining Improvements in Deep Reinforcement Learning \u21a9 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding \u21a9 A Simple Framework for Contrastive Learning of Visual Representations (SimCLR) \u21a9 \u21a9 Momentum Contrast for Unsupervised Visual Representation Learning (MoCo) \u21a9 Data-Efficient Image Recognition with Contrastive Predictive Coding (CPC) \u21a9 excerpts from original paper \u21a9 \u21a9 \u21a9 \u21a9 https://twitter.com/gwern/status/1248087160391163906 \u21a9","title":"Authors"},{"location":"david-silver-ucl/","text":"David Silver at UCL \u00b6 https://www.davidsilver.uk/teaching/ Lecture 7: Policy Gradient Methods \u00b6 https://www.youtube.com/watch?v=KHZVXao4qXs https://www.davidsilver.uk/wp-content/uploads/2020/03/pg.pdf Why use policy-based methods? \"The TD error is an unbiaised estimate of the advantage function\"","title":"David Silver at UCL"},{"location":"david-silver-ucl/#david-silver-at-ucl","text":"https://www.davidsilver.uk/teaching/","title":"David Silver at UCL"},{"location":"david-silver-ucl/#lecture-7-policy-gradient-methods","text":"https://www.youtube.com/watch?v=KHZVXao4qXs https://www.davidsilver.uk/wp-content/uploads/2020/03/pg.pdf Why use policy-based methods? \"The TD error is an unbiaised estimate of the advantage function\"","title":"Lecture 7: Policy Gradient Methods"},{"location":"distributed-training/","text":"","title":"Distributed training"},{"location":"dm-control-suite/","text":"TODO - https://arxiv.org/abs/1801.00690 - https://github.com/deepmind/dm_control DeepMind Control: Recently, there have been a number of papers that have benchmarked for sample efficiency on challenging visual continuous control tasks belonging to the DMControl suite (Tassa et al., 2018) where the agent operates purely from pixels. The reason for operating in these environments is multi fold: (i) they present a reasonably challenging and diverse set of tasks; (ii) sample-efficiency of pure model-free RL algorithms operating from pixels on these benchmarks is poor; (iii) multiple recent efforts to improve the sample efficiency of both model-free and model-based methods on these benchmarks thereby giving us sufficient baselines to compare against; (iv) performance on the DM control suite is relevant to robot learning in real world benchmarks. -- CURL paper","title":"Dm control suite"},{"location":"experience-replay/","text":"Experience replay (Lin, 1992) has gained popularity in deep Q-learning (Mnih et al., 2015; Schaul et al., 2016; Wang et al., 2016; Narasimhan et al., 2015), where it is often motivated as a technique for reducing sample correlation. Replay is actually a valuable tool for improving sample efficiency -- ACER","title":"Experience replay"},{"location":"foundational-papers/","text":"Foundational Papers \u00b6","title":"Foundational Papers"},{"location":"foundational-papers/#foundational-papers","text":"","title":"Foundational Papers"},{"location":"gae/","text":"High-Dimensional Continuous Control Using Generalized Advantage Estimation \u00b6 June 2015 - https://arxiv.org/abs/1506.02438 Abstract Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.","title":"High-Dimensional Continuous Control Using Generalized Advantage Estimation"},{"location":"gae/#high-dimensional-continuous-control-using-generalized-advantage-estimation","text":"June 2015 - https://arxiv.org/abs/1506.02438 Abstract Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.","title":"High-Dimensional Continuous Control Using Generalized Advantage Estimation"},{"location":"how-to-benchmark/","text":"","title":"How to benchmark"},{"location":"how-to-evaluate/","text":"A table of results and learning curves for all 49 games is provided in Appendix B. We consider the following two scoring metrics: (1) average reward per episode over entire training period (which favors fast learning), and (2) average reward per episode over last 100 episodes of training (which favors final performance). Table 2 shows the number of games \u201cwon\u201d by each algorithm, where we compute the victor by averaging the scoring metric across three trials. -- PPO paper","title":"How to evaluate"},{"location":"methods-comparison/","text":"Which RL Method Should I Use? \u00b6 Policy-based vs value-based \u00b6 \"Advantages of Policy-Based RL\" https://youtu.be/KHZVXao4qXs?t=625 - better convergence properties - effective in high-dimensional or continuous action spaces (no need to compute the max, a problem when continuous) - can learn stochastic properties - useful eg rock-paper-scicorss: less predictable - aliased states: may lead to situations where it is stuck! problems (at least naive methods): - convergence to local optimum - high variance ^silver-lecture-7","title":"Which RL Method Should I Use?"},{"location":"methods-comparison/#which-rl-method-should-i-use","text":"","title":"Which RL Method Should I Use?"},{"location":"methods-comparison/#policy-based-vs-value-based","text":"\"Advantages of Policy-Based RL\" https://youtu.be/KHZVXao4qXs?t=625 - better convergence properties - effective in high-dimensional or continuous action spaces (no need to compute the max, a problem when continuous) - can learn stochastic properties - useful eg rock-paper-scicorss: less predictable - aliased states: may lead to situations where it is stuck! problems (at least naive methods): - convergence to local optimum - high variance ^silver-lecture-7","title":"Policy-based vs value-based"},{"location":"model-free/","text":"AKA \"direct learning technique; it does not require a model to be given or learned\" (Baird, http://leemon.com/papers/1993b.pdf)","title":"Model free"},{"location":"muzero/","text":"","title":"Muzero"},{"location":"policy-based-methods/","text":"Policy-Based Methods \u00b6 gradient free methods: easy to scale, but don't work so well with too many parameters policy gradient methods Different ways to do policy optimization: https://youtu.be/KHZVXao4qXs?t=1532 - gradient free, eg evolution methods - gradient based, eg using gradient descent, see policy gradient methods","title":"Policy-Based Methods"},{"location":"policy-based-methods/#policy-based-methods","text":"gradient free methods: easy to scale, but don't work so well with too many parameters policy gradient methods Different ways to do policy optimization: https://youtu.be/KHZVXao4qXs?t=1532 - gradient free, eg evolution methods - gradient based, eg using gradient descent, see policy gradient methods","title":"Policy-Based Methods"},{"location":"policy-gradient-methods/","text":"Policy Gradient Methods \u00b6 Part of policy-based methods The existing variants applicable to both continuous and discrete domains, such as the on-policy asynchronous advantage actor critic (A3C) of Mnih et al. (2016), are sample inefficient. -- ACER Silver lecture on PG methods: https://www.youtube.com/watch?v=KHZVXao4qXs Links \u00b6 The best explanation of policy gradient is probably the lecture from David Silver at UCL This post highlights how policy gradient can be seen as a way to do supervised learning without a true label: https://amoudgl.github.io/blog/policy-gradient/","title":"Policy Gradient Methods"},{"location":"policy-gradient-methods/#policy-gradient-methods","text":"Part of policy-based methods The existing variants applicable to both continuous and discrete domains, such as the on-policy asynchronous advantage actor critic (A3C) of Mnih et al. (2016), are sample inefficient. -- ACER Silver lecture on PG methods: https://www.youtube.com/watch?v=KHZVXao4qXs","title":"Policy Gradient Methods"},{"location":"policy-gradient-methods/#links","text":"The best explanation of policy gradient is probably the lecture from David Silver at UCL This post highlights how policy gradient can be seen as a way to do supervised learning without a true label: https://amoudgl.github.io/blog/policy-gradient/","title":"Links"},{"location":"ppo/","text":"Proximal Policy Optimization Algorithms (PPO) \u00b6 Tldr Published July 2017 - highly influential (1673 citations) - arXiv Used all the time by OpenAI: DotA, robotic manipulation...","title":"Proximal Policy Optimization Algorithms (PPO)"},{"location":"ppo/#proximal-policy-optimization-algorithms-ppo","text":"Tldr Published July 2017 - highly influential (1673 citations) - arXiv Used all the time by OpenAI: DotA, robotic manipulation...","title":"Proximal Policy Optimization Algorithms (PPO)"},{"location":"rainbow/","text":"Rainbow: Combining Improvements in Deep Reinforcement Learning \u00b6 Published October 2017 - arXiv Main concepts \u00b6 Context \u00b6 At this point, Links \u00b6 Rainbow DQN (Hessel et al., 2017) is best summarized as multiple improvements on top of the original Nature DQN (Mnih et al., 2015) applied together. Specifically, Deep Q Network (DQN) (Mnih et al., 2015) combines the off-policy algorithm Q-Learning with a convolutional neural network as the function approximator to map raw pixels to action value functions. Since then, multiple improvements have been proposed such as Double Q Learning (Van Hasselt et al., 2016), Dueling Network Architectures (Wang et al., 2015), Prioritized Experience Replay (Schaul et al., 2015), and Noisy Networks (Fortunato et al., 2017). Additionally, distributional reinforcement learning (Bellemare et al., 2017) proposed the technique of predicting a distribution over possible value function bins through the C51 Algorithm. Rainbow DQN combines all of the above techniques into a single off-policy algorithm for state-of-the-art sample efficiency on Atari benchmarks. Additionally, Rainbow al -- CURL paper","title":"Rainbow: Combining Improvements in Deep Reinforcement Learning"},{"location":"rainbow/#rainbow-combining-improvements-in-deep-reinforcement-learning","text":"Published October 2017 - arXiv","title":"Rainbow: Combining Improvements in Deep Reinforcement Learning"},{"location":"rainbow/#main-concepts","text":"","title":"Main concepts"},{"location":"rainbow/#context","text":"At this point,","title":"Context"},{"location":"rainbow/#links","text":"Rainbow DQN (Hessel et al., 2017) is best summarized as multiple improvements on top of the original Nature DQN (Mnih et al., 2015) applied together. Specifically, Deep Q Network (DQN) (Mnih et al., 2015) combines the off-policy algorithm Q-Learning with a convolutional neural network as the function approximator to map raw pixels to action value functions. Since then, multiple improvements have been proposed such as Double Q Learning (Van Hasselt et al., 2016), Dueling Network Architectures (Wang et al., 2015), Prioritized Experience Replay (Schaul et al., 2015), and Noisy Networks (Fortunato et al., 2017). Additionally, distributional reinforcement learning (Bellemare et al., 2017) proposed the technique of predicting a distribution over possible value function bins through the C51 Algorithm. Rainbow DQN combines all of the above techniques into a single off-policy algorithm for state-of-the-art sample efficiency on Atari benchmarks. Additionally, Rainbow al -- CURL paper","title":"Links"},{"location":"rl-approaches/","text":"gradient free value-based policy-based model based vs model free","title":"Rl approaches"},{"location":"sac/","text":"SAC \u00b6 Tldr Short summary Published November XXXX - Influential (XXX citations) - arXiv Summary \u00b6 Key concepts \u00b6 - \u00b6 - Legacy \u00b6","title":"SAC"},{"location":"sac/#sac","text":"Tldr Short summary Published November XXXX - Influential (XXX citations) - arXiv","title":"SAC"},{"location":"sac/#summary","text":"","title":"Summary"},{"location":"sac/#key-concepts","text":"","title":"Key concepts"},{"location":"sac/#-","text":"-","title":"-"},{"location":"sac/#legacy","text":"","title":"Legacy"},{"location":"sample-efficiency/","text":"TODO - Improving Sample Efficiency in Model-Free Reinforcement Learning from Images https://arxiv.org/abs/1910.01741 This need for sample efficiency is even more compelling when agents are deployed in the real world -- ACER paper A number of approaches have been proposed in the literature to address the sample inefficiency of deep RL algorithms. Broadly, they can be classified into two streams of research, though not mutually exclusive: (i) Auxiliary tasks on the agent\u2019s sensory observations; (ii) World models that predict the future. While the former class of methods use auxiliary self-supervision tasks to accelerate the learning progress of model-free RL methods (Jaderberg et al., 2016; Mirowski et al., 2016), the latter class of methods build explicit predictive models of the world and use those models to plan through or collect fictitious rollouts for model-free methods to learn from (Sutton, 1990; Ha & Schmidhuber, 2018; Kaiser et al., 2019; Schrittwieser et al., 2019). -- CURL paper The DMControl suite has been used widely by Yarats et al. (2019), Hafner et al. (2018), Hafner et al. (2019) and Lee et al. (2019) for benchmarking sample-efficiency for image based continuous control methods. As for Atari, Kaiser et al. (2019) propose to use the 100k interaction steps benchmark for sample-efficiency which has been adopted in Kielak (2020); van Hasselt et al. (2019). The Rainbow DQN (Hessel et al., 2017) was originally proposed for maximum sample-efficiency on the Atari benchmark and in recent times has been adapted to a version known as DataEfficient Rainbow (van Hasselt et al., 2019) with competitive performance to SimPLe without learning world models. -- CURL paper We measure the data-efficiency and performance of our method and baselines at 100k interaction steps on both DMControl and Atari, which we will henceforth refer to as DMControl100k and Atari100k for clarity. Benchmarking at 100k steps makes for a fixed experimental setup that is easy to reproduce and has been common practice when investigating data-efficiency on Atari Kaiser et al. (2019); Kielak (2020). A broader motivation is that while RL al- CURL: Contrastive Unsupervised Representations for Reinforcement Learning gorithms can achieve super-human performance on many Atari games, they are still far from the data-efficiency of a human learner. Training for 100k steps is within the order of magnitude that we would expect for humans to a learn similar tasks. In our experiments, 100k steps corresponds to 300-400k frames (due to using a frame-skip of 3 or 4), which equates to roughly a 2-4 hours of human game play. -- CURL paper","title":"Sample efficiency"},{"location":"tips-and-tricks/","text":"RL Tips and Tricks \u00b6 Find Twitter thread! Study each topic in depth instead of covering as much as you can Do read old papers in details. You will be surprised how many old insights keep coming back, often presented as new.","title":"RL Tips and Tricks"},{"location":"tips-and-tricks/#rl-tips-and-tricks","text":"Find Twitter thread! Study each topic in depth instead of covering as much as you can Do read old papers in details. You will be surprised how many old insights keep coming back, often presented as new.","title":"RL Tips and Tricks"},{"location":"value-based-methods/","text":"Value-Based Methods \u00b6 However, we need to do better than deep Q-learning, because it has two important limitations. First, the deterministic nature of the optimal policy limits its use in adversarial domains. Second, finding the greedy action with respect to the Q function is costly for large action spaces. -- ACER https://arxiv.org/pdf/1611.01224.pdf Such methods are indirect in the sense that they do not try to optimize directly over a policy space. A method of this type may succeed in constructing a \"good\" approximation of the value function, yet lack reliable guarantees in terms of near-optimality of the resulting policy. -- Actor Critic https://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf","title":"Value-Based Methods"},{"location":"value-based-methods/#value-based-methods","text":"However, we need to do better than deep Q-learning, because it has two important limitations. First, the deterministic nature of the optimal policy limits its use in adversarial domains. Second, finding the greedy action with respect to the Q function is costly for large action spaces. -- ACER https://arxiv.org/pdf/1611.01224.pdf Such methods are indirect in the sense that they do not try to optimize directly over a policy space. A method of this type may succeed in constructing a \"good\" approximation of the value function, yet lack reliable guarantees in terms of near-optimality of the resulting policy. -- Actor Critic https://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf","title":"Value-Based Methods"},{"location":"what-is-rl/","text":"What is Reinforcement Learning? \u00b6 Reinforcement learning is a sub-field of machine learning. In reinforcement learning, our goal is to learn a behavior . Let's consider an example. How would you design a self-driving car? Assume you want to design a vehicle to drive you from Paris to Berlin. Many recent improvement in AI will come handy - image recognition could be used to locate other cars and pedestrians around you, text recognition will let you read the signs on the road, etc. However, the car ultimately needs to take decisions to bring you from point A to point B. This is where reinforcement learning takes the stage. In reinforcement learning, we train an agent to decide which action should be performed given the current situation. In the case of our self-driving car, this agent could take low-level decisions such as the angle of the steering weel or the pressure on the gas pedal. It could also take higher-level decisions, such as the general way to take to reach the destination as fast as possible. This kind of problems can't be handled by traditional supervised and unsupervised learning approaches. The diagram below shows how the sub-fields of machine learning interact. How to learn a behavior \u00b6 The core question of reinforcement learning is as follow: Given the current state, what is the optimal action in order to maximize the expected reward? There are many approaches to tackle this question. Evolution strategies \u00b6 The simplest, most intuitive approach may be the evolution strategy family of algorithms. The idea is Value-based methods \u00b6 More details about value-based methods... Reinforcement learning lingo \u00b6 In general, the agent can not access an exhaustive description of its environment. Instead, it needs to work with an observation of that state. A short history of reinforcement learning \u00b6 The history of reinforcement learning is actually nothing but short. The field as it exists today combines elements from different directions.","title":"What is Reinforcement Learning?"},{"location":"what-is-rl/#what-is-reinforcement-learning","text":"Reinforcement learning is a sub-field of machine learning. In reinforcement learning, our goal is to learn a behavior . Let's consider an example. How would you design a self-driving car? Assume you want to design a vehicle to drive you from Paris to Berlin. Many recent improvement in AI will come handy - image recognition could be used to locate other cars and pedestrians around you, text recognition will let you read the signs on the road, etc. However, the car ultimately needs to take decisions to bring you from point A to point B. This is where reinforcement learning takes the stage. In reinforcement learning, we train an agent to decide which action should be performed given the current situation. In the case of our self-driving car, this agent could take low-level decisions such as the angle of the steering weel or the pressure on the gas pedal. It could also take higher-level decisions, such as the general way to take to reach the destination as fast as possible. This kind of problems can't be handled by traditional supervised and unsupervised learning approaches. The diagram below shows how the sub-fields of machine learning interact.","title":"What is Reinforcement Learning?"},{"location":"what-is-rl/#how-to-learn-a-behavior","text":"The core question of reinforcement learning is as follow: Given the current state, what is the optimal action in order to maximize the expected reward? There are many approaches to tackle this question.","title":"How to learn a behavior"},{"location":"what-is-rl/#evolution-strategies","text":"The simplest, most intuitive approach may be the evolution strategy family of algorithms. The idea is","title":"Evolution strategies"},{"location":"what-is-rl/#value-based-methods","text":"More details about value-based methods...","title":"Value-based methods"},{"location":"what-is-rl/#reinforcement-learning-lingo","text":"In general, the agent can not access an exhaustive description of its environment. Instead, it needs to work with an observation of that state.","title":"Reinforcement learning lingo"},{"location":"what-is-rl/#a-short-history-of-reinforcement-learning","text":"The history of reinforcement learning is actually nothing but short. The field as it exists today combines elements from different directions.","title":"A short history of reinforcement learning"},{"location":"why-dnq-not-converging/","text":"Why is my DQN implementation not converging? \u00b6 \"Implementing the DQN\"","title":"Why is my DQN implementation not converging?"},{"location":"why-dnq-not-converging/#why-is-my-dqn-implementation-not-converging","text":"\"Implementing the DQN\"","title":"Why is my DQN implementation not converging?"}]}