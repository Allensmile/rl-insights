{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RL Insights \u00b6 Warning This site is in its early days, most pages are still empty! Pages with actual content so far: CURL: Contrastive Unsupervised Representations for Reinforcement Learning Asynchronous Methods for Deep Reinforcement Learning (A3C) Running Long Tasks in Notebooks Site structure \u00b6 Papers : provides summaries, key concepts, and links to blog posts and notebooks. Concepts : cross-references RL \"building blocks\" across papers. Notebooks : runnable code samples to illustrate concepts. Blog : opinions and research ideas. FAQ : curated answers to common questions. New to RL? \u00b6 While this site is meant as a repository for research papers, here are some good starting points: What is reinforcement learning? A quick intro to what this is all about. Best RL educational resources A number of excellent articles and courses to get started on this topic. Foundational papers Most of the RL knowledge still only exists in papers - these provide a good starting point.","title":"Home"},{"location":"#rl-insights","text":"Warning This site is in its early days, most pages are still empty! Pages with actual content so far: CURL: Contrastive Unsupervised Representations for Reinforcement Learning Asynchronous Methods for Deep Reinforcement Learning (A3C) Running Long Tasks in Notebooks","title":"RL Insights"},{"location":"#site-structure","text":"Papers : provides summaries, key concepts, and links to blog posts and notebooks. Concepts : cross-references RL \"building blocks\" across papers. Notebooks : runnable code samples to illustrate concepts. Blog : opinions and research ideas. FAQ : curated answers to common questions.","title":"Site structure"},{"location":"#new-to-rl","text":"While this site is meant as a repository for research papers, here are some good starting points: What is reinforcement learning? A quick intro to what this is all about. Best RL educational resources A number of excellent articles and courses to get started on this topic. Foundational papers Most of the RL knowledge still only exists in papers - these provide a good starting point.","title":"New to RL?"},{"location":"a3c/","text":"Asynchronous Methods for Deep Reinforcement Learning \u00b6 Tldr Introduces an RL framework that uses multiple CPU cores to speed up training on a single machine. The main result is A3C, a parallel actor-critic method that uses shared layers between actor and critic, n-step returns and entropy regularization. Parallel training for value-based methods is also investigated, but results are less convincing. A synchronous version called A2C, geared towards GPUs, is generally preferred nowadays. 1 February 2016 - arXiv Scaling up RL \u00b6 This paper introduces a multi-threaded RL training framework. Its main result is the A3C algorithm , which at the time of publication beat the state of the art on the Atari domain while training for only half the time. The core idea is to scale up training by using the multiple CPU cores available on modern workstations. Scaling up the training process brings two advantages: It enables shorter training times, as it allows us to throw more compute at the problem It helps stabilize training, by decorrelating the experiences observed by the agent Decorreleting experiences \u00b6 A common problem in reinforcement learning is the correlation and non-stationarity of successive experiences. When using off-policy methods such as DQN , it is possible to use a replay buffer to store experiences then sample them randomly. This is more complicated when using on-policy algorithms such as policy gradient methods , as they can't reuse past experiences. The proposed framework addresses this problem by executing multiple agents in parallel: Instead of experience replay, we asynchronously execute multiple agents in parallel, on multiple instances of the environment. This parallelism decorrelates the agents\u2019 data into a more stationary process, since at any given timestep the parallel agents will be experiencing a variety of different states. For off-policy methods, this is a memory vs compute trade-off: Replay buffers are memory-intensive, but use little computing power Parallel agents are CPU-intensive, but are not as memory-hungry as replay buffers Note that it would be possible to combine a replay buffer - either central or separate per worker - with parallel agents. Going in this direction leads us to architectures such as Gorila or Ape-X. For on-policy methods, there is no downside to using parallel agents, as they can't learn from previous experiences! (Following this publication other methods came up to address this limitation such as ACER , IMPALA and others.) Implementation details \u00b6 A stated goal of this framework is to leverage multi-cores CPUs without relying on GPUs: Our parallel reinforcement learning paradigm also offers practical benefits. Whereas previous approaches to deep reinforcement learning rely heavily on specialized hardware such as GPUs or massively distributed architectures, our experiments run on a single machine with a standard multi-core CPU. Since the gradients are calculated on a CPU, there's no need to batch large amount of data to optimize performance. As a result, the updates are performed asynchronously: each agent performs a number of training steps then performs an asynchronous updates with the global parameters. Here's the algorithm as pseudocode: 2 Performing all the computation on the CPU of a single machine means that there is no communication overhead, and that updates can be done Hogwild-style without any locking. Evaluation \u00b6 The paper first provides a comparison of multiple RL methods used within this framework: SARSA, deep Q-learning, n-step deep Q-learning, and advantage actor-critic. The evaluation is performed on four different platforms: The traditional Atari learning environment, using 5 games The Torcs driving simulator, using 4 game modes The Mujoco domain, using 14 tasks A custom \"labyrinth\" environment (subsequently released as DeepMind Lab ) Parallel methods learn faster in almost all cases. The results are most impressive when combining the parallel framework with advantage actor-critic. See for example the last line of these benchmarks on 3 Atari games: 2 The resulting method is A3C , the \"Asynchronous Advantage Actor Critic\" - this paper's claim to fame! The paper then provide in-depth evaluation of A3C on the Atari domain, using the complete test suite of 57 games. Looking at mean performances, A3C reaches state of the art status, training twice faster than its competition: 2 It should be noted that the question of evaluation of RL agents , and more specifically evaluation when using Atari games, has been hotly debated since this paper came out. The use of the mean performance is no longer regarded as reliable as some games can easily skew the final ranking. Extra improvements \u00b6 Beyond its parallel nature, A3C includes a number of improvements compared to vanilla advantage actor critic : The layers are shared between the actor and the critic Both actor and critic use n-step learning The policy uses entropy regularization Adding an LSTM layer helps in some environments The RMSProp optimizer uses shared statistics between agents From A3C to A2C \u00b6 Since this method is designed to train networks using a CPU, asynchronous updates made a lot of sense as it enabled Hogwild syle updates. However, it appeared later on that this was not optimal when using a GPU. For this reason, this method is commonly used today in a synchronous way in which the updates from all the actor-learners are batched to make better use of the GPU. This non-asynchronous version is simply called A2C (Advantage Actor Critic) 1 . However, a different method called A2C, which does leverage GPUs, is preferred today. In hindsight \u00b6 Executing multiple agents in parallel is a good way to decorrelate observations Asynchronous updates are not a good match for GPU training: see A2C Using exclusively on-policy data is bad for sample efficiency: see ACER , IMPALA Key concepts \u00b6 \"Building blocks\" involved in this work: Builds on Actor-Critic methods Introduces a multi-threaded parallel training method Uses entropy regularization Uses an LSTM Recurrent Neural Network layer Uses multi-step return Uses shared layers between policy and value functions Benchmarked on Atari games Links \u00b6 Actor-Critic Methods: A3C and A2C from Daniel Seita, a detailed walkthrough Authors \u00b6 Volodymyr Mnih Twitter / Scholar / Academic Adri\u00e0 Puigdom\u00e8nech Badia Scholar Mehdi Mirza Twitter / Scholar / Personal Alex Graves Scholar / Academic Tim Harley Twitter / Scholar Timothy P. Lillicrap Twitter / Scholar / Personal David Silver Scholar / Personal Koray Kavukcuoglu Twitter / Scholar / Personal OpenAI Baselines: ACKTR & A2C \u21a9 \u21a9 excerpts from original paper \u21a9 \u21a9 \u21a9","title":"Asynchronous Methods for Deep Reinforcement Learning"},{"location":"a3c/#asynchronous-methods-for-deep-reinforcement-learning","text":"Tldr Introduces an RL framework that uses multiple CPU cores to speed up training on a single machine. The main result is A3C, a parallel actor-critic method that uses shared layers between actor and critic, n-step returns and entropy regularization. Parallel training for value-based methods is also investigated, but results are less convincing. A synchronous version called A2C, geared towards GPUs, is generally preferred nowadays. 1 February 2016 - arXiv","title":"Asynchronous Methods for Deep Reinforcement Learning"},{"location":"a3c/#scaling-up-rl","text":"This paper introduces a multi-threaded RL training framework. Its main result is the A3C algorithm , which at the time of publication beat the state of the art on the Atari domain while training for only half the time. The core idea is to scale up training by using the multiple CPU cores available on modern workstations. Scaling up the training process brings two advantages: It enables shorter training times, as it allows us to throw more compute at the problem It helps stabilize training, by decorrelating the experiences observed by the agent","title":"Scaling up RL"},{"location":"a3c/#decorreleting-experiences","text":"A common problem in reinforcement learning is the correlation and non-stationarity of successive experiences. When using off-policy methods such as DQN , it is possible to use a replay buffer to store experiences then sample them randomly. This is more complicated when using on-policy algorithms such as policy gradient methods , as they can't reuse past experiences. The proposed framework addresses this problem by executing multiple agents in parallel: Instead of experience replay, we asynchronously execute multiple agents in parallel, on multiple instances of the environment. This parallelism decorrelates the agents\u2019 data into a more stationary process, since at any given timestep the parallel agents will be experiencing a variety of different states. For off-policy methods, this is a memory vs compute trade-off: Replay buffers are memory-intensive, but use little computing power Parallel agents are CPU-intensive, but are not as memory-hungry as replay buffers Note that it would be possible to combine a replay buffer - either central or separate per worker - with parallel agents. Going in this direction leads us to architectures such as Gorila or Ape-X. For on-policy methods, there is no downside to using parallel agents, as they can't learn from previous experiences! (Following this publication other methods came up to address this limitation such as ACER , IMPALA and others.)","title":"Decorreleting experiences"},{"location":"a3c/#implementation-details","text":"A stated goal of this framework is to leverage multi-cores CPUs without relying on GPUs: Our parallel reinforcement learning paradigm also offers practical benefits. Whereas previous approaches to deep reinforcement learning rely heavily on specialized hardware such as GPUs or massively distributed architectures, our experiments run on a single machine with a standard multi-core CPU. Since the gradients are calculated on a CPU, there's no need to batch large amount of data to optimize performance. As a result, the updates are performed asynchronously: each agent performs a number of training steps then performs an asynchronous updates with the global parameters. Here's the algorithm as pseudocode: 2 Performing all the computation on the CPU of a single machine means that there is no communication overhead, and that updates can be done Hogwild-style without any locking.","title":"Implementation details"},{"location":"a3c/#evaluation","text":"The paper first provides a comparison of multiple RL methods used within this framework: SARSA, deep Q-learning, n-step deep Q-learning, and advantage actor-critic. The evaluation is performed on four different platforms: The traditional Atari learning environment, using 5 games The Torcs driving simulator, using 4 game modes The Mujoco domain, using 14 tasks A custom \"labyrinth\" environment (subsequently released as DeepMind Lab ) Parallel methods learn faster in almost all cases. The results are most impressive when combining the parallel framework with advantage actor-critic. See for example the last line of these benchmarks on 3 Atari games: 2 The resulting method is A3C , the \"Asynchronous Advantage Actor Critic\" - this paper's claim to fame! The paper then provide in-depth evaluation of A3C on the Atari domain, using the complete test suite of 57 games. Looking at mean performances, A3C reaches state of the art status, training twice faster than its competition: 2 It should be noted that the question of evaluation of RL agents , and more specifically evaluation when using Atari games, has been hotly debated since this paper came out. The use of the mean performance is no longer regarded as reliable as some games can easily skew the final ranking.","title":"Evaluation"},{"location":"a3c/#extra-improvements","text":"Beyond its parallel nature, A3C includes a number of improvements compared to vanilla advantage actor critic : The layers are shared between the actor and the critic Both actor and critic use n-step learning The policy uses entropy regularization Adding an LSTM layer helps in some environments The RMSProp optimizer uses shared statistics between agents","title":"Extra improvements"},{"location":"a3c/#from-a3c-to-a2c","text":"Since this method is designed to train networks using a CPU, asynchronous updates made a lot of sense as it enabled Hogwild syle updates. However, it appeared later on that this was not optimal when using a GPU. For this reason, this method is commonly used today in a synchronous way in which the updates from all the actor-learners are batched to make better use of the GPU. This non-asynchronous version is simply called A2C (Advantage Actor Critic) 1 . However, a different method called A2C, which does leverage GPUs, is preferred today.","title":"From A3C to A2C"},{"location":"a3c/#in-hindsight","text":"Executing multiple agents in parallel is a good way to decorrelate observations Asynchronous updates are not a good match for GPU training: see A2C Using exclusively on-policy data is bad for sample efficiency: see ACER , IMPALA","title":"In hindsight"},{"location":"a3c/#key-concepts","text":"\"Building blocks\" involved in this work: Builds on Actor-Critic methods Introduces a multi-threaded parallel training method Uses entropy regularization Uses an LSTM Recurrent Neural Network layer Uses multi-step return Uses shared layers between policy and value functions Benchmarked on Atari games","title":"Key concepts"},{"location":"a3c/#links","text":"Actor-Critic Methods: A3C and A2C from Daniel Seita, a detailed walkthrough","title":"Links"},{"location":"a3c/#authors","text":"Volodymyr Mnih Twitter / Scholar / Academic Adri\u00e0 Puigdom\u00e8nech Badia Scholar Mehdi Mirza Twitter / Scholar / Personal Alex Graves Scholar / Academic Tim Harley Twitter / Scholar Timothy P. Lillicrap Twitter / Scholar / Personal David Silver Scholar / Personal Koray Kavukcuoglu Twitter / Scholar / Personal OpenAI Baselines: ACKTR & A2C \u21a9 \u21a9 excerpts from original paper \u21a9 \u21a9 \u21a9","title":"Authors"},{"location":"acer/","text":"Sample Efficient Actor-Critic with Experience Replay (ACER) \u00b6 Warning Under construction! This page is still vastly incomplete. Tldr ACER - Actor-Critic with Experience Replay extends the parallel implementation of actor-critic methods described in A3C to the off-policy setting. Published November 2016 - Influential (251 citations) - arXiv Summary \u00b6 \"ACER may be understood as the off-policy counterpart of the A3C method .\" Key concepts \u00b6 Uses ?? for variance reduction (GAE?) Uses Generalized Advantage Estimation Uses the the off-policy Retrace algorithm Uses parallel-training as in A3C Introduces Truncated Importance Sampling Introduces stochastic dueling network architectures Introduces efficient trust region policy optimization Legacy \u00b6 Used in PPO (?)","title":"Sample Efficient Actor-Critic with Experience Replay (ACER)"},{"location":"acer/#sample-efficient-actor-critic-with-experience-replay-acer","text":"Warning Under construction! This page is still vastly incomplete. Tldr ACER - Actor-Critic with Experience Replay extends the parallel implementation of actor-critic methods described in A3C to the off-policy setting. Published November 2016 - Influential (251 citations) - arXiv","title":"Sample Efficient Actor-Critic with Experience Replay (ACER)"},{"location":"acer/#summary","text":"\"ACER may be understood as the off-policy counterpart of the A3C method .\"","title":"Summary"},{"location":"acer/#key-concepts","text":"Uses ?? for variance reduction (GAE?) Uses Generalized Advantage Estimation Uses the the off-policy Retrace algorithm Uses parallel-training as in A3C Introduces Truncated Importance Sampling Introduces stochastic dueling network architectures Introduces efficient trust region policy optimization","title":"Key concepts"},{"location":"acer/#legacy","text":"Used in PPO (?)","title":"Legacy"},{"location":"actor-critic/","text":"Actor-Critic Algorithms \u00b6 Tldr Introduces Actor-Critic Algorithms . Published February 2016 - Very Influential - Link Summary \u00b6 This paper introduces the the notion of actor-critic methods, which is the basis of a large part of the reinforcement learning methods today. This idea is to combine a value-function approximation, the critic, which helps reduce the variance of a policy-gradient approximation, the actor. This idea will further be extended with the notion of advantage . Note that this work only considers linear approximations, ie it doesn't use neural networks. In the actor-critic setting, learning the critic can be seen as a supervised learning problem. This means it can either be learned using TD-learning, or simply least square. It is important to understand that the goal of the critic is to approximate the returns that the actor will receive, and not the returns of the optimal policy. But the critic is regularly updated, which means that the critic is trying to learn a moving target! in practice, this is not a problem as the actor is updated more slowly than the critic Silver lecture on PG methods: https://www.youtube.com/watch?v=KHZVXao4qXs (actor-critic from https://youtu.be/KHZVXao4qXs?t=3178) Pretty much all the gradient-based approaches are now actor-critic methods, so this doesn't mean much anymore! https://github.com/openai/spinningup/issues/156#issuecomment-494596739 TODO cover Degris paper, the original resource for off-policy AC: https://arxiv.org/abs/1205.4839","title":"Actor-Critic Algorithms"},{"location":"actor-critic/#actor-critic-algorithms","text":"Tldr Introduces Actor-Critic Algorithms . Published February 2016 - Very Influential - Link","title":"Actor-Critic Algorithms"},{"location":"actor-critic/#summary","text":"This paper introduces the the notion of actor-critic methods, which is the basis of a large part of the reinforcement learning methods today. This idea is to combine a value-function approximation, the critic, which helps reduce the variance of a policy-gradient approximation, the actor. This idea will further be extended with the notion of advantage . Note that this work only considers linear approximations, ie it doesn't use neural networks. In the actor-critic setting, learning the critic can be seen as a supervised learning problem. This means it can either be learned using TD-learning, or simply least square. It is important to understand that the goal of the critic is to approximate the returns that the actor will receive, and not the returns of the optimal policy. But the critic is regularly updated, which means that the critic is trying to learn a moving target! in practice, this is not a problem as the actor is updated more slowly than the critic Silver lecture on PG methods: https://www.youtube.com/watch?v=KHZVXao4qXs (actor-critic from https://youtu.be/KHZVXao4qXs?t=3178) Pretty much all the gradient-based approaches are now actor-critic methods, so this doesn't mean much anymore! https://github.com/openai/spinningup/issues/156#issuecomment-494596739 TODO cover Degris paper, the original resource for off-policy AC: https://arxiv.org/abs/1205.4839","title":"Summary"},{"location":"atari/","text":"Atari Learning Environment \u00b6 Tldr Short summary Published November XXXX - Influential (XXX citations) - arXiv Summary \u00b6 Key concepts \u00b6 - \u00b6 - Legacy \u00b6","title":"Atari Learning Environment"},{"location":"atari/#atari-learning-environment","text":"Tldr Short summary Published November XXXX - Influential (XXX citations) - arXiv","title":"Atari Learning Environment"},{"location":"atari/#summary","text":"","title":"Summary"},{"location":"atari/#key-concepts","text":"","title":"Key concepts"},{"location":"atari/#-","text":"-","title":"-"},{"location":"atari/#legacy","text":"","title":"Legacy"},{"location":"auxiliary-tasks/","text":"UNREAL paper Self-Supervised Learning for RL: Auxiliary tasks such as predicting the future conditioned on the past observation(s) and action(s) (Jaderberg et al., 2016; Shelhamer et al., 2016; van den Oord et al., 2018), and predicting the depth image for maze navigation (Mirowski et al., 2016) are a few representative examples of using auxiliary tasks to improve the sample-efficiency of model-free RL algorithms. The future prediction is either done in a pixel space (Jaderberg et al., 2016) or latent space (van den Oord et al., 2018). The sample-efficiency gains from reconstruction-based auxiliary losses have been benchmarked in Jaderberg et al. (2016); Higgins et al. (2017); Yarats et al. (2019). Contrastive learning across has been used to extract reward signals characterized as distance metrics in the latent space by -- CURL paper","title":"Auxiliary tasks"},{"location":"best-resources/","text":"Best RL resources \u00b6","title":"Best RL resources"},{"location":"best-resources/#best-rl-resources","text":"","title":"Best RL resources"},{"location":"cem/","text":"Cross-Entropy Methods \u00b6 http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf I. Szita and A. L\u00a8orincz. \u201cLearning Tetris using the noisy cross-entropy method\u201d. In: Neural computation 18.12 (2006), pp. 2936\u20132941.","title":"Cross-Entropy Methods"},{"location":"cem/#cross-entropy-methods","text":"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf I. Szita and A. L\u00a8orincz. \u201cLearning Tetris using the noisy cross-entropy method\u201d. In: Neural computation 18.12 (2006), pp. 2936\u20132941.","title":"Cross-Entropy Methods"},{"location":"contrastive-learning/","text":"Contrastive Learning: Contrastive Learning is a framework to learn representations that obey similarity constraints in a dataset typically organized by similar and dissimilar pairs. This is often best understood as performing a dictionary lookup task wherein the positive and negatives represent a set of keys with respect to a query (or an anchor). A simple instantiation of contrastive learning is Instance Discrimination (Wu et al., 2018) wherein a query and key are positive pairs if they are data-augmentations of the same instance (example, image) and negative otherwise. A key challenge in contrastive learning is the choice of negatives which can decide the quality of the underlying representations learned. The loss functions used to contrast could be among several choices such as InfoNCE (van den Oord et al., 2018), Triplet (Wang & Gupta, 2015), Siamese (Chopra et al., 2005) and so forth. -- CURL paper","title":"Contrastive learning"},{"location":"curl/","text":"CURL: Contrastive Unsupervised Representations for Reinforcement Learning \u00b6 Tldr Improves the sample efficiency when learning from pixels by using contrastive learning, a self-supervised method, as an auxiliary task. The contrastive objective works in a similar way as in SimCLR, using random cropping as an augmentation method. After 100k interactions, outperforms all other methods on DM Control Suite, and shows strong results on Atari. April 2020 - arXiv - Code Learning from pixels \u00b6 In reinforcement learning, solving a task from pixels is much harder than solving an equivalent task using \"physical\" features such as coordinates and angles. This makes sense: you can consider an image as a high-dimensional vector containing hundreds of features, which don't have any clear connection with the goal of the environment! As a result, you generally need a lot more interactions between an agent and its environment in order to learn a good policy from pixels. For example, the figure below shows the results of multiple recent RL methods on the DeepMind Control Suite, learning from physical features. Take a look at the x-axis: depending on the complexity of the task, the agents are trained from 1 to 10 million interactions: 1 On the other hand, let's have a look at results using Atari games as a benchmark. There is no notion of \"physical\" features with the Atari emulator: the only observations the agent can work with are RGB images. The agents now have to be trained for a staggering 200 million frames! 2 Some distributed approaches even consider numbers up to a billion. This low sample-efficiency is clearly a problem. Not only does it mean the experiment turn-around time is excessive, it also means that there can be little hope of bringing such methods to the real world. Can you imagine having to collect a billion real-world interactions? The paper we are considering takes a stab at this problem by bringing recent advances from vision and NLP to reinforcement learning. Contrastive learning takes advantage of data augmentation to learn more efficiently. CURL shows that it can be very useful in the context of RL to learn a good latent representation faster. What is contrastive learning? \u00b6 The core idea is to compare (contrast!) pairs of augmented samples. We consider two kinds of such pairs: Positive pairs consist of two different augmentations of the same sample Negative pairs contain augmentations of two different samples For each original sample, we create both positive and negative pairs. The contrastive representation is then learned by maximizing the agreement between positive pairs, and minimizing the agreement between negative pairs. Contrastive learning has seen dramatic progress in recent years for language and vision. See for example BERT, an application to masked language modeling 3 , or the SimCLR framework, used to learn visual representations 4 . The way contrastive learning is implemented in CURL is mostly influenced by the SimCLR framework 4 , Momentum Contrast (MoCo) 5 and Contrastive Predictive Coding (CPC) 6 . How CURL works \u00b6 With CURL, the same latent representation is used for both the RL algorithm and the contrastive learning, as illustrated below: 7 CURL uses random crops to augment the observations. Since most RL methods use frame-stacking, each observation is effectively a \"stack\" of sequential images. CURL preserves their temporal structure by applying the same augmentation to each frame in the stack. The illustration below gives an example of a positive pair: the same observation is augmented in two different ways. The representation will be changed in a way that maximizes their agreement. 7 Evaluation \u00b6 Using a contrastive objective as an auxiliary task appears to significantly improve the performance of the RL algorithm. CURL's performance is evaluated in two settings: with SAC on DeepMind Control Suite (continuous control) with data-efficient Rainbow DQN on Atari games (discrete control). In both cases, the performance is evaluated after 100k interactions, as the goal is to evaluate sample efficiency rather than asymptotic performance. Results are remarkable on DeepMind Control Suite : 7 (The last column, State SAC , uses physical states and is used as an \"oracle\" upper-bound.) Results are very good on Atari games. This is again after 100k interactions: 7 How can this difference in performance be explained? One idea explored in the appendix is that in some environments, there is simply not enough information to fully recover the state when looking only at the pixel data: the problem then becomes partially observable and therefore much harder. Key concepts \u00b6 \"Building blocks\" involved in this work: Focuses on sample efficiency Uses contrastive learning Uses auxiliary tasks Benchmarked on DeepMind Control Suite Benchmarked on Atari games Limitations \u00b6 No comparison with MuZero , which is SotA on multiple Atari games 8 Links \u00b6 Official code repository , a PyTorch implementation for SAC on DeepMind Control Suite Official project page , which provides a short summary of the paper Twitter summary from first author Authors \u00b6 Aravind Srinivas Twitter / Scholar / Academic Michael Laskin Twitter / Scholar / Academic Pieter Abbeel Twitter / Scholar / Academic Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor \u21a9 Rainbow: Combining Improvements in Deep Reinforcement Learning \u21a9 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding \u21a9 A Simple Framework for Contrastive Learning of Visual Representations (SimCLR) \u21a9 \u21a9 Momentum Contrast for Unsupervised Visual Representation Learning (MoCo) \u21a9 Data-Efficient Image Recognition with Contrastive Predictive Coding (CPC) \u21a9 excerpts from original paper \u21a9 \u21a9 \u21a9 \u21a9 https://twitter.com/gwern/status/1248087160391163906 \u21a9","title":"CURL: Contrastive Unsupervised Representations for Reinforcement Learning"},{"location":"curl/#curl-contrastive-unsupervised-representations-for-reinforcement-learning","text":"Tldr Improves the sample efficiency when learning from pixels by using contrastive learning, a self-supervised method, as an auxiliary task. The contrastive objective works in a similar way as in SimCLR, using random cropping as an augmentation method. After 100k interactions, outperforms all other methods on DM Control Suite, and shows strong results on Atari. April 2020 - arXiv - Code","title":"CURL: Contrastive Unsupervised Representations for Reinforcement Learning"},{"location":"curl/#learning-from-pixels","text":"In reinforcement learning, solving a task from pixels is much harder than solving an equivalent task using \"physical\" features such as coordinates and angles. This makes sense: you can consider an image as a high-dimensional vector containing hundreds of features, which don't have any clear connection with the goal of the environment! As a result, you generally need a lot more interactions between an agent and its environment in order to learn a good policy from pixels. For example, the figure below shows the results of multiple recent RL methods on the DeepMind Control Suite, learning from physical features. Take a look at the x-axis: depending on the complexity of the task, the agents are trained from 1 to 10 million interactions: 1 On the other hand, let's have a look at results using Atari games as a benchmark. There is no notion of \"physical\" features with the Atari emulator: the only observations the agent can work with are RGB images. The agents now have to be trained for a staggering 200 million frames! 2 Some distributed approaches even consider numbers up to a billion. This low sample-efficiency is clearly a problem. Not only does it mean the experiment turn-around time is excessive, it also means that there can be little hope of bringing such methods to the real world. Can you imagine having to collect a billion real-world interactions? The paper we are considering takes a stab at this problem by bringing recent advances from vision and NLP to reinforcement learning. Contrastive learning takes advantage of data augmentation to learn more efficiently. CURL shows that it can be very useful in the context of RL to learn a good latent representation faster.","title":"Learning from pixels"},{"location":"curl/#what-is-contrastive-learning","text":"The core idea is to compare (contrast!) pairs of augmented samples. We consider two kinds of such pairs: Positive pairs consist of two different augmentations of the same sample Negative pairs contain augmentations of two different samples For each original sample, we create both positive and negative pairs. The contrastive representation is then learned by maximizing the agreement between positive pairs, and minimizing the agreement between negative pairs. Contrastive learning has seen dramatic progress in recent years for language and vision. See for example BERT, an application to masked language modeling 3 , or the SimCLR framework, used to learn visual representations 4 . The way contrastive learning is implemented in CURL is mostly influenced by the SimCLR framework 4 , Momentum Contrast (MoCo) 5 and Contrastive Predictive Coding (CPC) 6 .","title":"What is contrastive learning?"},{"location":"curl/#how-curl-works","text":"With CURL, the same latent representation is used for both the RL algorithm and the contrastive learning, as illustrated below: 7 CURL uses random crops to augment the observations. Since most RL methods use frame-stacking, each observation is effectively a \"stack\" of sequential images. CURL preserves their temporal structure by applying the same augmentation to each frame in the stack. The illustration below gives an example of a positive pair: the same observation is augmented in two different ways. The representation will be changed in a way that maximizes their agreement. 7","title":"How CURL works"},{"location":"curl/#evaluation","text":"Using a contrastive objective as an auxiliary task appears to significantly improve the performance of the RL algorithm. CURL's performance is evaluated in two settings: with SAC on DeepMind Control Suite (continuous control) with data-efficient Rainbow DQN on Atari games (discrete control). In both cases, the performance is evaluated after 100k interactions, as the goal is to evaluate sample efficiency rather than asymptotic performance. Results are remarkable on DeepMind Control Suite : 7 (The last column, State SAC , uses physical states and is used as an \"oracle\" upper-bound.) Results are very good on Atari games. This is again after 100k interactions: 7 How can this difference in performance be explained? One idea explored in the appendix is that in some environments, there is simply not enough information to fully recover the state when looking only at the pixel data: the problem then becomes partially observable and therefore much harder.","title":"Evaluation"},{"location":"curl/#key-concepts","text":"\"Building blocks\" involved in this work: Focuses on sample efficiency Uses contrastive learning Uses auxiliary tasks Benchmarked on DeepMind Control Suite Benchmarked on Atari games","title":"Key concepts"},{"location":"curl/#limitations","text":"No comparison with MuZero , which is SotA on multiple Atari games 8","title":"Limitations"},{"location":"curl/#links","text":"Official code repository , a PyTorch implementation for SAC on DeepMind Control Suite Official project page , which provides a short summary of the paper Twitter summary from first author","title":"Links"},{"location":"curl/#authors","text":"Aravind Srinivas Twitter / Scholar / Academic Michael Laskin Twitter / Scholar / Academic Pieter Abbeel Twitter / Scholar / Academic Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor \u21a9 Rainbow: Combining Improvements in Deep Reinforcement Learning \u21a9 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding \u21a9 A Simple Framework for Contrastive Learning of Visual Representations (SimCLR) \u21a9 \u21a9 Momentum Contrast for Unsupervised Visual Representation Learning (MoCo) \u21a9 Data-Efficient Image Recognition with Contrastive Predictive Coding (CPC) \u21a9 excerpts from original paper \u21a9 \u21a9 \u21a9 \u21a9 https://twitter.com/gwern/status/1248087160391163906 \u21a9","title":"Authors"},{"location":"david-silver-ucl/","text":"David Silver at UCL \u00b6 https://www.davidsilver.uk/teaching/ Lecture 7: Policy Gradient Methods \u00b6 https://www.youtube.com/watch?v=KHZVXao4qXs https://www.davidsilver.uk/wp-content/uploads/2020/03/pg.pdf Why use policy-based methods? \"The TD error is an unbiaised estimate of the advantage function\"","title":"David Silver at UCL"},{"location":"david-silver-ucl/#david-silver-at-ucl","text":"https://www.davidsilver.uk/teaching/","title":"David Silver at UCL"},{"location":"david-silver-ucl/#lecture-7-policy-gradient-methods","text":"https://www.youtube.com/watch?v=KHZVXao4qXs https://www.davidsilver.uk/wp-content/uploads/2020/03/pg.pdf Why use policy-based methods? \"The TD error is an unbiaised estimate of the advantage function\"","title":"Lecture 7: Policy Gradient Methods"},{"location":"distributed-training/","text":"","title":"Distributed training"},{"location":"dm-control-suite/","text":"TODO - https://arxiv.org/abs/1801.00690 - https://github.com/deepmind/dm_control DeepMind Control: Recently, there have been a number of papers that have benchmarked for sample efficiency on challenging visual continuous control tasks belonging to the DMControl suite (Tassa et al., 2018) where the agent operates purely from pixels. The reason for operating in these environments is multi fold: (i) they present a reasonably challenging and diverse set of tasks; (ii) sample-efficiency of pure model-free RL algorithms operating from pixels on these benchmarks is poor; (iii) multiple recent efforts to improve the sample efficiency of both model-free and model-based methods on these benchmarks thereby giving us sufficient baselines to compare against; (iv) performance on the DM control suite is relevant to robot learning in real world benchmarks. -- CURL paper","title":"Dm control suite"},{"location":"dm-lab/","text":"","title":"Dm lab"},{"location":"experience-replay/","text":"Experience replay (Lin, 1992) has gained popularity in deep Q-learning (Mnih et al., 2015; Schaul et al., 2016; Wang et al., 2016; Narasimhan et al., 2015), where it is often motivated as a technique for reducing sample correlation. Replay is actually a valuable tool for improving sample efficiency -- ACER","title":"Experience replay"},{"location":"foundational-papers/","text":"Foundational Papers \u00b6","title":"Foundational Papers"},{"location":"foundational-papers/#foundational-papers","text":"","title":"Foundational Papers"},{"location":"gae/","text":"High-Dimensional Continuous Control Using Generalized Advantage Estimation \u00b6 June 2015 - https://arxiv.org/abs/1506.02438 Abstract Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.","title":"High-Dimensional Continuous Control Using Generalized Advantage Estimation"},{"location":"gae/#high-dimensional-continuous-control-using-generalized-advantage-estimation","text":"June 2015 - https://arxiv.org/abs/1506.02438 Abstract Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.","title":"High-Dimensional Continuous Control Using Generalized Advantage Estimation"},{"location":"hogwild/","text":"Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent \u00b6 https://papers.nips.cc/paper/4390-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent https://srome.github.io/Async-SGD-in-Python-Implementing-Hogwild!/","title":"Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent"},{"location":"hogwild/#hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent","text":"https://papers.nips.cc/paper/4390-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent https://srome.github.io/Async-SGD-in-Python-Implementing-Hogwild!/","title":"Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent"},{"location":"how-to-benchmark/","text":"How to benchmark RL \u00b6 https://arxiv.org/abs/1709.06009 https://arxiv.org/abs/1709.06560","title":"How to benchmark RL"},{"location":"how-to-benchmark/#how-to-benchmark-rl","text":"https://arxiv.org/abs/1709.06009 https://arxiv.org/abs/1709.06560","title":"How to benchmark RL"},{"location":"how-to-evaluate/","text":"A table of results and learning curves for all 49 games is provided in Appendix B. We consider the following two scoring metrics: (1) average reward per episode over entire training period (which favors fast learning), and (2) average reward per episode over last 100 episodes of training (which favors final performance). Table 2 shows the number of games \u201cwon\u201d by each algorithm, where we compute the victor by averaging the scoring metric across three trials. -- PPO paper","title":"How to evaluate"},{"location":"methods-comparison/","text":"Which RL Method Should I Use? \u00b6 Policy-based vs value-based \u00b6 \"Advantages of Policy-Based RL\" https://youtu.be/KHZVXao4qXs?t=625 - better convergence properties - effective in high-dimensional or continuous action spaces (no need to compute the max, a problem when continuous) - can learn stochastic properties - useful eg rock-paper-scicorss: less predictable - aliased states: may lead to situations where it is stuck! problems (at least naive methods): - convergence to local optimum - high variance ^silver-lecture-7","title":"Which RL Method Should I Use?"},{"location":"methods-comparison/#which-rl-method-should-i-use","text":"","title":"Which RL Method Should I Use?"},{"location":"methods-comparison/#policy-based-vs-value-based","text":"\"Advantages of Policy-Based RL\" https://youtu.be/KHZVXao4qXs?t=625 - better convergence properties - effective in high-dimensional or continuous action spaces (no need to compute the max, a problem when continuous) - can learn stochastic properties - useful eg rock-paper-scicorss: less predictable - aliased states: may lead to situations where it is stuck! problems (at least naive methods): - convergence to local optimum - high variance ^silver-lecture-7","title":"Policy-based vs value-based"},{"location":"model-free/","text":"AKA \"direct learning technique; it does not require a model to be given or learned\" (Baird, http://leemon.com/papers/1993b.pdf)","title":"Model free"},{"location":"mujoco/","text":"","title":"Mujoco"},{"location":"muzero/","text":"","title":"Muzero"},{"location":"notebook-pitfalls/","text":"Notebook Pitfalls \u00b6 Hint This concept is not directly related to reinforcement learning, however, it is still relevant to RL researchers! Notebooks are a great way to run quick experiments and exploratory data analysis. They're also perfect to show off your work. Services such as Google Colab and MyBinder even provide free compute in the form of notebooks. What's not to love? As we will see, notebooks do come with a number of pretty bad pitfalls . They are still an amazing tool, but they require some careful considerations before you integrate them into your workflow. Error-prone execution flow \u00b6 The first pitfall is the way code gets executed in notebooks: the execution of each cell alters the global state. There is no warranty that cells are ran in order, which can result in confusing situations if you don't keep track of cell history. In the example above 1 , the final results seems surprising. What happened is that a change was done to the global state between the first and second cell, in a cell that has since been deleted. This problem often happens with module imports or function definitions: it is easy to accidentally delete a cell that doesn't look useful anymore, only to realize later that its content was indeed needed. Since the side effect of that cell kept effect until the runtime was restarted, the problem can be overlooked for some time. A solution is to run \"Restart Runtime and Run All\" aggressively . This is your best weapon against hidden states. The example above 1 appears even nastier at first look, but is less common in practice. Here, what happened is that the content of some cells was modified after being executed, which surprisingly is not visible from the notebook UI. One use case where this can happen is if you start a long running task, then start editing other parts of the code. It can quickly become confusing to remember which parts of your code where changed before and after the start of that task, which can make debugging difficult if said task doesn't complete as expected. A solution is to commit your notebook often , ideally every time before you start a long-running task. This ensures that you have a snapshot of the state of your notebook when the task started. The page Running Long Tasks in Notebooks describes such workflows in more details. Poor code assist \u00b6 Modern IDEs, such as IntelliJ, have amazing auto-complete capabilities. They will not only suggest completions as you type, but will also perform type checking and other form of sanity checks that can save you precious time. In contrast, autocomplete in notebooks is still terribly lackluster: 1 Notebooks don't take advantage of type information: 1 At this point, IDEs and notebooks provide two different approaches to improve iteration time: IDEs: top-notch autocomplete helps writing code faster and more confidently Notebooks: instant execution of each piece of code helps you check that it work as expected Could they be combined? I think so. Modern Javascript engines are good enough to run the advanced heuristics Eclipse or IntelliJ uses under the hood. It's hopefully only a matter of time before such approaches are implemented. Beyond autocompletion and type checking, IDEs offer two other invaluable tools: Linting: IDEs will clean up entire files to conform to PEP8 at the push of a button Refactoring: IDEs can perform advanced refactors, looking through multiple files Both of these make it easier to keep your code clean and readable. \"I don't like notebooks\" \u00b6 These two points are inspired by a 2018 talk by Joel Grus , a veteran Python developer and author. I recommend you watch the full video: The slide deck is also available on Google Doc . Joel raises many other points, such as how notebooks hinder modularity and testability, and other usability problems. I don't like notebooks.- Joel Grus (Allen Institute for Artificial Intelligence) \u21a9 \u21a9 \u21a9 \u21a9","title":"Notebook Pitfalls"},{"location":"notebook-pitfalls/#notebook-pitfalls","text":"Hint This concept is not directly related to reinforcement learning, however, it is still relevant to RL researchers! Notebooks are a great way to run quick experiments and exploratory data analysis. They're also perfect to show off your work. Services such as Google Colab and MyBinder even provide free compute in the form of notebooks. What's not to love? As we will see, notebooks do come with a number of pretty bad pitfalls . They are still an amazing tool, but they require some careful considerations before you integrate them into your workflow.","title":"Notebook Pitfalls"},{"location":"notebook-pitfalls/#error-prone-execution-flow","text":"The first pitfall is the way code gets executed in notebooks: the execution of each cell alters the global state. There is no warranty that cells are ran in order, which can result in confusing situations if you don't keep track of cell history. In the example above 1 , the final results seems surprising. What happened is that a change was done to the global state between the first and second cell, in a cell that has since been deleted. This problem often happens with module imports or function definitions: it is easy to accidentally delete a cell that doesn't look useful anymore, only to realize later that its content was indeed needed. Since the side effect of that cell kept effect until the runtime was restarted, the problem can be overlooked for some time. A solution is to run \"Restart Runtime and Run All\" aggressively . This is your best weapon against hidden states. The example above 1 appears even nastier at first look, but is less common in practice. Here, what happened is that the content of some cells was modified after being executed, which surprisingly is not visible from the notebook UI. One use case where this can happen is if you start a long running task, then start editing other parts of the code. It can quickly become confusing to remember which parts of your code where changed before and after the start of that task, which can make debugging difficult if said task doesn't complete as expected. A solution is to commit your notebook often , ideally every time before you start a long-running task. This ensures that you have a snapshot of the state of your notebook when the task started. The page Running Long Tasks in Notebooks describes such workflows in more details.","title":"Error-prone execution flow"},{"location":"notebook-pitfalls/#poor-code-assist","text":"Modern IDEs, such as IntelliJ, have amazing auto-complete capabilities. They will not only suggest completions as you type, but will also perform type checking and other form of sanity checks that can save you precious time. In contrast, autocomplete in notebooks is still terribly lackluster: 1 Notebooks don't take advantage of type information: 1 At this point, IDEs and notebooks provide two different approaches to improve iteration time: IDEs: top-notch autocomplete helps writing code faster and more confidently Notebooks: instant execution of each piece of code helps you check that it work as expected Could they be combined? I think so. Modern Javascript engines are good enough to run the advanced heuristics Eclipse or IntelliJ uses under the hood. It's hopefully only a matter of time before such approaches are implemented. Beyond autocompletion and type checking, IDEs offer two other invaluable tools: Linting: IDEs will clean up entire files to conform to PEP8 at the push of a button Refactoring: IDEs can perform advanced refactors, looking through multiple files Both of these make it easier to keep your code clean and readable.","title":"Poor code assist"},{"location":"notebook-pitfalls/#i-dont-like-notebooks","text":"These two points are inspired by a 2018 talk by Joel Grus , a veteran Python developer and author. I recommend you watch the full video: The slide deck is also available on Google Doc . Joel raises many other points, such as how notebooks hinder modularity and testability, and other usability problems. I don't like notebooks.- Joel Grus (Allen Institute for Artificial Intelligence) \u21a9 \u21a9 \u21a9 \u21a9","title":"\"I don't like notebooks\""},{"location":"notebooks-long-tasks/","text":"Running Long Tasks in Notebooks \u00b6 Hint This concept is not directly related to reinforcement learning, however, it is still relevant to RL researchers! Notebooks make it comfortable to quickly implement new research ideas. You try things out with a toy dataset, and after a few iterations, you're reasonably sure that you got things right. Next step: trying it out on a more realistic dataset! This usually means having to wait for some time. In theory, nothing stops you from simply executing the notebook and letting it run for a few hours. In practice, this is generally a bad idea . Notebooks hinder your flow \u00b6 Let's assume you only need 10 minutes to run your notebook. Note that you should always run it from beginning to end, after having restarted the runtime, to avoid problems with stale states ! Now that your notebook is running, what do you do? The problem is that you can only run one cell at a time . As long as it is running, you should be careful what you. One option is to keep making changes to other cells, cleaning up your code or adding comments. The problem is that it now becomes unclear what was the state when you started the task. If it crashes mid-way through, it will be painful to debug, as you won't be sure which changes were made before or after it started. Even if the task doesn't crash, you can't run any other cell in the meantime, losing one of the main selling point of notebooks: instant feedback. Another option is to immediately duplicate the notebook . This is a common pattern among Kaggle participants. The advantage is that you keep a clear snapshot of the state in which the task started. The problem is that you end up with multiple different \"branches\" of your work, that quickly get hard to reconcile. Finally, you can also do something else entirely : work on another notebook, read Reddit or go grab coffee. But the problem is that you are forced to do these things, because of your workflow - this is bad! your workflow shouldn't force you to context-switch if you don't want to. Connection issues \u00b6 Another problem is that notebooks are not reliable enough to run for hours. Crucially, they are not able to recover if you lose connection to the kernel. You accidentally close your laptop? You lose connectivity for a few seconds? Too bad! You won't be able to recover the output of your task. If you are using Google Colab , another risk is that your runtime gets killed because of inactivity, or just because of lack of capacity. Sure, you can mitigate this problem by using some form of checkpointing, but this is a band-aid. Couldn't notebooks handle this better? I have been keeping an eye on two separate GitHub issues tracking this problem for a long time, and they are still open to this day. Git and jupytext to the rescue \u00b6 So, what's the solution? It depends how long your task takes. For notebooks that run for a few minutes , I would recommend adopting a rigorous git workflow : commit your code and parameters before each run. This allows you to keep making minor changes to your code while keeping a snapshot of the starting state. You still run into the problems mentioned above: you can't run other cells in the meantime, and you'll lose progress if you get disconnected. But for a few minutes, this is usually fine. For tasks that will run for hours , it is simply not reasonable to keep a browser tab opened for that long. The solution in that case is to turn your notebook into a Python file . One way to do this is to manually extract each cell, in order to turn your work into a proper Python module. This is usually a good idea when you have high confidence that things work as expected, and you move from an experimental mindset to a production mindset. But this is a lot of work! Couldn't that be automated? Yes it can! The solution is called Jupytext . Jupytext takes a Python notebook as input, and turns it into a plain Python file. This gives you the best of both worlds! You can now easily run your experiment in the background, or on another machine, in a \"fire and forget\" way. You can easily run multiple instances, for example to perform some basic hyperparameter search or check variance across multiple seeds. And you can start your task using robust Unix tools such as screen and tmux to ensure it will run to completion. Other notebook pitfalls \u00b6 Notebooks are an amazing tool, but they require some careful considerations before you integrate them into your workflow. You can read more on this topic in the article Notebook Pitfalls .","title":"Running Long Tasks in Notebooks"},{"location":"notebooks-long-tasks/#running-long-tasks-in-notebooks","text":"Hint This concept is not directly related to reinforcement learning, however, it is still relevant to RL researchers! Notebooks make it comfortable to quickly implement new research ideas. You try things out with a toy dataset, and after a few iterations, you're reasonably sure that you got things right. Next step: trying it out on a more realistic dataset! This usually means having to wait for some time. In theory, nothing stops you from simply executing the notebook and letting it run for a few hours. In practice, this is generally a bad idea .","title":"Running Long Tasks in Notebooks"},{"location":"notebooks-long-tasks/#notebooks-hinder-your-flow","text":"Let's assume you only need 10 minutes to run your notebook. Note that you should always run it from beginning to end, after having restarted the runtime, to avoid problems with stale states ! Now that your notebook is running, what do you do? The problem is that you can only run one cell at a time . As long as it is running, you should be careful what you. One option is to keep making changes to other cells, cleaning up your code or adding comments. The problem is that it now becomes unclear what was the state when you started the task. If it crashes mid-way through, it will be painful to debug, as you won't be sure which changes were made before or after it started. Even if the task doesn't crash, you can't run any other cell in the meantime, losing one of the main selling point of notebooks: instant feedback. Another option is to immediately duplicate the notebook . This is a common pattern among Kaggle participants. The advantage is that you keep a clear snapshot of the state in which the task started. The problem is that you end up with multiple different \"branches\" of your work, that quickly get hard to reconcile. Finally, you can also do something else entirely : work on another notebook, read Reddit or go grab coffee. But the problem is that you are forced to do these things, because of your workflow - this is bad! your workflow shouldn't force you to context-switch if you don't want to.","title":"Notebooks hinder your flow"},{"location":"notebooks-long-tasks/#connection-issues","text":"Another problem is that notebooks are not reliable enough to run for hours. Crucially, they are not able to recover if you lose connection to the kernel. You accidentally close your laptop? You lose connectivity for a few seconds? Too bad! You won't be able to recover the output of your task. If you are using Google Colab , another risk is that your runtime gets killed because of inactivity, or just because of lack of capacity. Sure, you can mitigate this problem by using some form of checkpointing, but this is a band-aid. Couldn't notebooks handle this better? I have been keeping an eye on two separate GitHub issues tracking this problem for a long time, and they are still open to this day.","title":"Connection issues"},{"location":"notebooks-long-tasks/#git-and-jupytext-to-the-rescue","text":"So, what's the solution? It depends how long your task takes. For notebooks that run for a few minutes , I would recommend adopting a rigorous git workflow : commit your code and parameters before each run. This allows you to keep making minor changes to your code while keeping a snapshot of the starting state. You still run into the problems mentioned above: you can't run other cells in the meantime, and you'll lose progress if you get disconnected. But for a few minutes, this is usually fine. For tasks that will run for hours , it is simply not reasonable to keep a browser tab opened for that long. The solution in that case is to turn your notebook into a Python file . One way to do this is to manually extract each cell, in order to turn your work into a proper Python module. This is usually a good idea when you have high confidence that things work as expected, and you move from an experimental mindset to a production mindset. But this is a lot of work! Couldn't that be automated? Yes it can! The solution is called Jupytext . Jupytext takes a Python notebook as input, and turns it into a plain Python file. This gives you the best of both worlds! You can now easily run your experiment in the background, or on another machine, in a \"fire and forget\" way. You can easily run multiple instances, for example to perform some basic hyperparameter search or check variance across multiple seeds. And you can start your task using robust Unix tools such as screen and tmux to ensure it will run to completion.","title":"Git and jupytext to the rescue"},{"location":"notebooks-long-tasks/#other-notebook-pitfalls","text":"Notebooks are an amazing tool, but they require some careful considerations before you integrate them into your workflow. You can read more on this topic in the article Notebook Pitfalls .","title":"Other notebook pitfalls"},{"location":"policy-based-methods/","text":"Policy-Based Methods \u00b6 gradient free methods: easy to scale, but don't work so well with too many parameters policy gradient methods Different ways to do policy optimization: https://youtu.be/KHZVXao4qXs?t=1532 - gradient free, eg evolution methods - gradient based, eg using gradient descent, see policy gradient methods","title":"Policy-Based Methods"},{"location":"policy-based-methods/#policy-based-methods","text":"gradient free methods: easy to scale, but don't work so well with too many parameters policy gradient methods Different ways to do policy optimization: https://youtu.be/KHZVXao4qXs?t=1532 - gradient free, eg evolution methods - gradient based, eg using gradient descent, see policy gradient methods","title":"Policy-Based Methods"},{"location":"policy-gradient-methods/","text":"Policy Gradient Methods \u00b6 Part of policy-based methods The existing variants applicable to both continuous and discrete domains, such as the on-policy asynchronous advantage actor critic (A3C) of Mnih et al. (2016), are sample inefficient. -- ACER Silver lecture on PG methods: https://www.youtube.com/watch?v=KHZVXao4qXs Links \u00b6 The best explanation of policy gradient is probably the lecture from David Silver at UCL This post highlights how policy gradient can be seen as a way to do supervised learning without a true label: https://amoudgl.github.io/blog/policy-gradient/","title":"Policy Gradient Methods"},{"location":"policy-gradient-methods/#policy-gradient-methods","text":"Part of policy-based methods The existing variants applicable to both continuous and discrete domains, such as the on-policy asynchronous advantage actor critic (A3C) of Mnih et al. (2016), are sample inefficient. -- ACER Silver lecture on PG methods: https://www.youtube.com/watch?v=KHZVXao4qXs","title":"Policy Gradient Methods"},{"location":"policy-gradient-methods/#links","text":"The best explanation of policy gradient is probably the lecture from David Silver at UCL This post highlights how policy gradient can be seen as a way to do supervised learning without a true label: https://amoudgl.github.io/blog/policy-gradient/","title":"Links"},{"location":"ppo/","text":"Proximal Policy Optimization Algorithms (PPO) \u00b6 Tldr Published July 2017 - highly influential (1673 citations) - arXiv Used all the time by OpenAI: DotA, robotic manipulation...","title":"Proximal Policy Optimization Algorithms (PPO)"},{"location":"ppo/#proximal-policy-optimization-algorithms-ppo","text":"Tldr Published July 2017 - highly influential (1673 citations) - arXiv Used all the time by OpenAI: DotA, robotic manipulation...","title":"Proximal Policy Optimization Algorithms (PPO)"},{"location":"rainbow/","text":"Rainbow: Combining Improvements in Deep Reinforcement Learning \u00b6 Published October 2017 - arXiv Main concepts \u00b6 Context \u00b6 At this point, Links \u00b6 Rainbow DQN (Hessel et al., 2017) is best summarized as multiple improvements on top of the original Nature DQN (Mnih et al., 2015) applied together. Specifically, Deep Q Network (DQN) (Mnih et al., 2015) combines the off-policy algorithm Q-Learning with a convolutional neural network as the function approximator to map raw pixels to action value functions. Since then, multiple improvements have been proposed such as Double Q Learning (Van Hasselt et al., 2016), Dueling Network Architectures (Wang et al., 2015), Prioritized Experience Replay (Schaul et al., 2015), and Noisy Networks (Fortunato et al., 2017). Additionally, distributional reinforcement learning (Bellemare et al., 2017) proposed the technique of predicting a distribution over possible value function bins through the C51 Algorithm. Rainbow DQN combines all of the above techniques into a single off-policy algorithm for state-of-the-art sample efficiency on Atari benchmarks. Additionally, Rainbow al -- CURL paper","title":"Rainbow: Combining Improvements in Deep Reinforcement Learning"},{"location":"rainbow/#rainbow-combining-improvements-in-deep-reinforcement-learning","text":"Published October 2017 - arXiv","title":"Rainbow: Combining Improvements in Deep Reinforcement Learning"},{"location":"rainbow/#main-concepts","text":"","title":"Main concepts"},{"location":"rainbow/#context","text":"At this point,","title":"Context"},{"location":"rainbow/#links","text":"Rainbow DQN (Hessel et al., 2017) is best summarized as multiple improvements on top of the original Nature DQN (Mnih et al., 2015) applied together. Specifically, Deep Q Network (DQN) (Mnih et al., 2015) combines the off-policy algorithm Q-Learning with a convolutional neural network as the function approximator to map raw pixels to action value functions. Since then, multiple improvements have been proposed such as Double Q Learning (Van Hasselt et al., 2016), Dueling Network Architectures (Wang et al., 2015), Prioritized Experience Replay (Schaul et al., 2015), and Noisy Networks (Fortunato et al., 2017). Additionally, distributional reinforcement learning (Bellemare et al., 2017) proposed the technique of predicting a distribution over possible value function bins through the C51 Algorithm. Rainbow DQN combines all of the above techniques into a single off-policy algorithm for state-of-the-art sample efficiency on Atari benchmarks. Additionally, Rainbow al -- CURL paper","title":"Links"},{"location":"rl-approaches/","text":"gradient free value-based policy-based model based vs model free","title":"Rl approaches"},{"location":"sac/","text":"SAC \u00b6 Tldr Short summary Published November XXXX - Influential (XXX citations) - arXiv Summary \u00b6 Key concepts \u00b6 - \u00b6 - Legacy \u00b6","title":"SAC"},{"location":"sac/#sac","text":"Tldr Short summary Published November XXXX - Influential (XXX citations) - arXiv","title":"SAC"},{"location":"sac/#summary","text":"","title":"Summary"},{"location":"sac/#key-concepts","text":"","title":"Key concepts"},{"location":"sac/#-","text":"-","title":"-"},{"location":"sac/#legacy","text":"","title":"Legacy"},{"location":"sample-efficiency/","text":"TODO - Improving Sample Efficiency in Model-Free Reinforcement Learning from Images https://arxiv.org/abs/1910.01741 This need for sample efficiency is even more compelling when agents are deployed in the real world -- ACER paper A number of approaches have been proposed in the literature to address the sample inefficiency of deep RL algorithms. Broadly, they can be classified into two streams of research, though not mutually exclusive: (i) Auxiliary tasks on the agent\u2019s sensory observations; (ii) World models that predict the future. While the former class of methods use auxiliary self-supervision tasks to accelerate the learning progress of model-free RL methods (Jaderberg et al., 2016; Mirowski et al., 2016), the latter class of methods build explicit predictive models of the world and use those models to plan through or collect fictitious rollouts for model-free methods to learn from (Sutton, 1990; Ha & Schmidhuber, 2018; Kaiser et al., 2019; Schrittwieser et al., 2019). -- CURL paper The DMControl suite has been used widely by Yarats et al. (2019), Hafner et al. (2018), Hafner et al. (2019) and Lee et al. (2019) for benchmarking sample-efficiency for image based continuous control methods. As for Atari, Kaiser et al. (2019) propose to use the 100k interaction steps benchmark for sample-efficiency which has been adopted in Kielak (2020); van Hasselt et al. (2019). The Rainbow DQN (Hessel et al., 2017) was originally proposed for maximum sample-efficiency on the Atari benchmark and in recent times has been adapted to a version known as DataEfficient Rainbow (van Hasselt et al., 2019) with competitive performance to SimPLe without learning world models. -- CURL paper We measure the data-efficiency and performance of our method and baselines at 100k interaction steps on both DMControl and Atari, which we will henceforth refer to as DMControl100k and Atari100k for clarity. Benchmarking at 100k steps makes for a fixed experimental setup that is easy to reproduce and has been common practice when investigating data-efficiency on Atari Kaiser et al. (2019); Kielak (2020). A broader motivation is that while RL al- CURL: Contrastive Unsupervised Representations for Reinforcement Learning gorithms can achieve super-human performance on many Atari games, they are still far from the data-efficiency of a human learner. Training for 100k steps is within the order of magnitude that we would expect for humans to a learn similar tasks. In our experiments, 100k steps corresponds to 300-400k frames (due to using a frame-skip of 3 or 4), which equates to roughly a 2-4 hours of human game play. -- CURL paper","title":"Sample efficiency"},{"location":"tips-and-tricks/","text":"RL Tips and Tricks \u00b6 Find Twitter thread! Study each topic in depth instead of covering as much as you can Do read old papers in details. You will be surprised how many old insights keep coming back, often presented as new.","title":"RL Tips and Tricks"},{"location":"tips-and-tricks/#rl-tips-and-tricks","text":"Find Twitter thread! Study each topic in depth instead of covering as much as you can Do read old papers in details. You will be surprised how many old insights keep coming back, often presented as new.","title":"RL Tips and Tricks"},{"location":"value-based-methods/","text":"Value-Based Methods \u00b6 However, we need to do better than deep Q-learning, because it has two important limitations. First, the deterministic nature of the optimal policy limits its use in adversarial domains. Second, finding the greedy action with respect to the Q function is costly for large action spaces. -- ACER https://arxiv.org/pdf/1611.01224.pdf Such methods are indirect in the sense that they do not try to optimize directly over a policy space. A method of this type may succeed in constructing a \"good\" approximation of the value function, yet lack reliable guarantees in terms of near-optimality of the resulting policy. -- Actor Critic https://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf","title":"Value-Based Methods"},{"location":"value-based-methods/#value-based-methods","text":"However, we need to do better than deep Q-learning, because it has two important limitations. First, the deterministic nature of the optimal policy limits its use in adversarial domains. Second, finding the greedy action with respect to the Q function is costly for large action spaces. -- ACER https://arxiv.org/pdf/1611.01224.pdf Such methods are indirect in the sense that they do not try to optimize directly over a policy space. A method of this type may succeed in constructing a \"good\" approximation of the value function, yet lack reliable guarantees in terms of near-optimality of the resulting policy. -- Actor Critic https://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf","title":"Value-Based Methods"},{"location":"what-is-rl/","text":"What is Reinforcement Learning? \u00b6 Reinforcement learning is a sub-field of machine learning. In reinforcement learning, our goal is to learn a behavior . Let's consider an example. How would you design a self-driving car? Assume you want to design a vehicle to drive you from Paris to Berlin. Many recent improvement in AI will come handy - image recognition could be used to locate other cars and pedestrians around you, text recognition will let you read the signs on the road, etc. However, the car ultimately needs to take decisions to bring you from point A to point B. This is where reinforcement learning takes the stage. In reinforcement learning, we train an agent to decide which action should be performed given the current situation. In the case of our self-driving car, this agent could take low-level decisions such as the angle of the steering weel or the pressure on the gas pedal. It could also take higher-level decisions, such as the general way to take to reach the destination as fast as possible. This kind of problems can't be handled by traditional supervised and unsupervised learning approaches. The diagram below shows how the sub-fields of machine learning interact. How to learn a behavior \u00b6 The core question of reinforcement learning is as follow: Given the current state, what is the optimal action in order to maximize the expected reward? There are many approaches to tackle this question. Evolution strategies \u00b6 The simplest, most intuitive approach may be the evolution strategy family of algorithms. The idea is Value-based methods \u00b6 More details about value-based methods... Reinforcement learning lingo \u00b6 In general, the agent can not access an exhaustive description of its environment. Instead, it needs to work with an observation of that state. A short history of reinforcement learning \u00b6 The history of reinforcement learning is actually nothing but short. The field as it exists today combines elements from different directions.","title":"What is Reinforcement Learning?"},{"location":"what-is-rl/#what-is-reinforcement-learning","text":"Reinforcement learning is a sub-field of machine learning. In reinforcement learning, our goal is to learn a behavior . Let's consider an example. How would you design a self-driving car? Assume you want to design a vehicle to drive you from Paris to Berlin. Many recent improvement in AI will come handy - image recognition could be used to locate other cars and pedestrians around you, text recognition will let you read the signs on the road, etc. However, the car ultimately needs to take decisions to bring you from point A to point B. This is where reinforcement learning takes the stage. In reinforcement learning, we train an agent to decide which action should be performed given the current situation. In the case of our self-driving car, this agent could take low-level decisions such as the angle of the steering weel or the pressure on the gas pedal. It could also take higher-level decisions, such as the general way to take to reach the destination as fast as possible. This kind of problems can't be handled by traditional supervised and unsupervised learning approaches. The diagram below shows how the sub-fields of machine learning interact.","title":"What is Reinforcement Learning?"},{"location":"what-is-rl/#how-to-learn-a-behavior","text":"The core question of reinforcement learning is as follow: Given the current state, what is the optimal action in order to maximize the expected reward? There are many approaches to tackle this question.","title":"How to learn a behavior"},{"location":"what-is-rl/#evolution-strategies","text":"The simplest, most intuitive approach may be the evolution strategy family of algorithms. The idea is","title":"Evolution strategies"},{"location":"what-is-rl/#value-based-methods","text":"More details about value-based methods...","title":"Value-based methods"},{"location":"what-is-rl/#reinforcement-learning-lingo","text":"In general, the agent can not access an exhaustive description of its environment. Instead, it needs to work with an observation of that state.","title":"Reinforcement learning lingo"},{"location":"what-is-rl/#a-short-history-of-reinforcement-learning","text":"The history of reinforcement learning is actually nothing but short. The field as it exists today combines elements from different directions.","title":"A short history of reinforcement learning"},{"location":"why-dnq-not-converging/","text":"Why is my DQN implementation not converging? \u00b6 \"Implementing the DQN\"","title":"Why is my DQN implementation not converging?"},{"location":"why-dnq-not-converging/#why-is-my-dqn-implementation-not-converging","text":"\"Implementing the DQN\"","title":"Why is my DQN implementation not converging?"},{"location":"notebooks/RL_Research_Workflow/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); RL Research Workflow \u00b6 # code comment print(1+1) 2 Setup \u00b6 Let's setup everything! ! pip install gym pandas matplotlib ! pip install gym [ box2d ] Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.1) Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.3) Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.1) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1) Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0) Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.2) Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0) Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1) Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9) Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0) Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7) Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0) Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.6/dist-packages (0.17.1) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.12.0) Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.5.0) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.4.1) Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.3.0) Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.18.2) Collecting box2d-py~=2.3.5; extra == \"box2d\" Downloading https://files.pythonhosted.org/packages/06/bd/6cdc3fd994b0649dcf5d9bad85bd9e26172308bbe9a421bfc6fdbf5081a6/box2d_py-2.3.8-cp36-cp36m-manylinux1_x86_64.whl (448kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 450kB 2.7MB/s Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]) (0.16.0) Installing collected packages: box2d-py Successfully installed box2d-py-2.3.8 # code comment print(1+1) for i in range(10): pass 2 # this code specifies language with triple colons print ( 1 + 1 ) for i in range ( 10 ): pass File \"<ipython-input-4-deb283a0c945>\" , line 1 :::python ^ SyntaxError : invalid syntax","title":"Test notebook 1"},{"location":"notebooks/RL_Research_Workflow/#rl-research-workflow","text":"# code comment print(1+1) 2","title":"RL Research Workflow"},{"location":"notebooks/RL_Research_Workflow/#setup","text":"Let's setup everything! ! pip install gym pandas matplotlib ! pip install gym [ box2d ] Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.1) Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.3) Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.1) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1) Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0) Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.2) Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0) Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1) Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9) Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0) Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7) Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0) Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.6/dist-packages (0.17.1) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.12.0) Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.5.0) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.4.1) Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.3.0) Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.18.2) Collecting box2d-py~=2.3.5; extra == \"box2d\" Downloading https://files.pythonhosted.org/packages/06/bd/6cdc3fd994b0649dcf5d9bad85bd9e26172308bbe9a421bfc6fdbf5081a6/box2d_py-2.3.8-cp36-cp36m-manylinux1_x86_64.whl (448kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 450kB 2.7MB/s Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]) (0.16.0) Installing collected packages: box2d-py Successfully installed box2d-py-2.3.8 # code comment print(1+1) for i in range(10): pass 2 # this code specifies language with triple colons print ( 1 + 1 ) for i in range ( 10 ): pass File \"<ipython-input-4-deb283a0c945>\" , line 1 :::python ^ SyntaxError : invalid syntax","title":"Setup"},{"location":"notebooks/Title_of_the_notebook/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Title 1 \u00b6 Title 2 \u00b6 Other Title 2 \u00b6 This is some plain text. code in markdown multiline code in markdown # code comment print(1+1) for i in range(10): pass 2 # this code specifies language with triple colons print ( 1 + 1 ) for i in range ( 10 ): pass","title":"Test notebook 2"},{"location":"notebooks/Title_of_the_notebook/#title-1","text":"","title":"Title 1"},{"location":"notebooks/Title_of_the_notebook/#title-2","text":"","title":"Title 2"},{"location":"notebooks/Title_of_the_notebook/#other-title-2","text":"This is some plain text. code in markdown multiline code in markdown # code comment print(1+1) for i in range(10): pass 2 # this code specifies language with triple colons print ( 1 + 1 ) for i in range ( 10 ): pass","title":"Other Title 2"},{"location":"notebooks/notebook/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Estimating COVID-19's R_t R_t in Real-Time \u00b6 Kevin Systrom - April 12 In any epidemic, R_t R_t is the measure known as the effective reproduction number. It's the number of people who become infected per infectious person at time t t . The most well-known version of this number is the basic reproduction number: R_0 R_0 when t=0 t=0 . However, R_0 R_0 is a single measure that does not adapt with changes in behavior and restrictions. As a pandemic evolves, increasing restrictions (or potential releasing of restrictions) change R_t R_t . Knowing the current R_t R_t is essential. When R>1 R>1 , the pandemic will spread through the entire population. If R_t<1 R_t<1 , the pandemic will grow to some fixed number less than the population. The lower R_t R_t , the more manageable the situation. The value of R_t R_t helps us (1) understand how effective our measures have been controlling an outbreak and (2) gives us vital information about whether we should increase or reduce restrictions based on our competing goals of economic prosperity and human safety. Well-respected epidemiologists argue that tracking R_t R_t is the only way to manage through this crisis. Yet, today, to my knowledge there is no real-time tracking of R_t R_t in United States. In fact, the only real-time measure I've seen has been for Hong Kong . More importantly, it is not useful to understand R_t R_t at a national level. Instead, to manage this crisis effectively, we need a local (state, county and/or city) level granularity of R_t R_t . What follows is a solution to this problem at the US State level. It's a modified version of a solution created by Bettencourt & Ribeiro 2008 to estimate real-time R_t R_t using a Bayesian approach. While I have stayed true to most of their process, my solution differs in an important way that I will call out clearly. If you have questions, comments, or improvments feel free to get in touch: hello@systrom.com . And if it's not entirely clear, I'm not an epidemiologist. At the same time, data is data, and statistics are statistics and this is based on work by well-known epidemiologists so calibrate accordingly. In the meantime, I hope you can learn something new as I did by reading through this example. Feel free to take this work and apply it elsewhere \u2013 internationally or to counties in the United States. import pandas as pd import numpy as np from matplotlib import pyplot as plt from matplotlib.dates import date2num , num2date from matplotlib import dates as mdates from matplotlib import ticker from matplotlib.colors import ListedColormap from matplotlib.patches import Patch from scipy import stats as sps from scipy.interpolate import interp1d from IPython.display import clear_output FILTERED_REGIONS = [ 'Virgin Islands' , 'American Samoa' , 'Northern Mariana Islands' , 'Guam' , 'Puerto Rico' ] % config InlineBackend . figure_format = 'retina' Bettencourt & Ribeiro's Approach \u00b6 Every day, we learn how many more people have COVID-19. This new case count gives us a clue about the current value of R_t R_t . We also, figure that the value of R_t R_t today is related to the value of R_{t-1} R_{t-1} (yesterday's value) and every previous value of R_{t-m} R_{t-m} for that matter. With these insights, the authors use Bayes' rule to update their beliefs about the true value of R_t R_t based on how many new cases have been reported each day. This is Bayes' Theorem as we'll use it: P(R_t|k)=\\frac{P(R_t)\\cdot\\mathcal{L}(R_t|k)}{P(k)} P(R_t|k)=\\frac{P(R_t)\\cdot\\mathcal{L}(R_t|k)}{P(k)} This says that, having seen k k new cases, we believe the distribution of R_t R_t is equal to: The prior beliefs of the value of P(R_t) P(R_t) without the data ... times the likelihood of R_t R_t given that we've seen k k new cases ... divided by the probability of seeing this many cases in general. Importantly, P(k) P(k) is a constant, so the numerator is proportional to the posterior. Since all probability distributions sum to 1.0, we can ignore P(k) P(k) and normalize our posterior to sum to 1.0: P(R_t|k) \\propto P(R_t) \\cdot \\mathcal{L}(R_t|k) P(R_t|k) \\propto P(R_t) \\cdot \\mathcal{L}(R_t|k) This is for a single day. To make it iterative: every day that passes, we use yesterday's conclusion (ie. posterior) P(R_{t-1}|k_{t-1}) P(R_{t-1}|k_{t-1}) to be today's prior P(R_t) P(R_t) so on day two: P(R_2|k) \\propto P(R_0)\\cdot\\mathcal{L}(R_2|k_2)\\cdot\\mathcal{L}(R_1|k_1) P(R_2|k) \\propto P(R_0)\\cdot\\mathcal{L}(R_2|k_2)\\cdot\\mathcal{L}(R_1|k_1) And more generally: P(R_t|k_t) \\propto P(R_0) \\cdot {\\displaystyle \\prod^{T}_{t=0}}\\mathcal{L}(R_t|k_t) P(R_t|k_t) \\propto P(R_0) \\cdot {\\displaystyle \\prod^{T}_{t=0}}\\mathcal{L}(R_t|k_t) With a uniform prior P(R_0) P(R_0) , this reduces to: $$ P(R_t|k_t) \\propto {\\displaystyle \\prod^{T}_{t=0}}\\mathcal{L}\\left(R_t|k_t\\right) $$ My Proposed Modification \u00b6 This works fine, but it suffers from an issue: the posterior on any given day is equally influenced by the distant past as much as the recent day. For epidemics that have R_t>1 R_t>1 for a long time and then become under control ( R_t<1 R_t<1 ), the posterior gets stuck. It cannot forget about the many days where R_t>1 R_t>1 so eventually P(R_t|k) P(R_t|k) asymptotically approaches 1 when we know it's well under 1. The authors note this in the paper as a footnote. Unfortunately this won't work for us. The most critical thing to know is when we've dipped below the 1.0 threshold! So, I propose to only incorporate the last m m days of the likelihood function. By doing this, the algorithm's prior is built based on the recent past which is a much more useful prior than the entire history of the epidemic. So this simple, but important change leads to the following: P(R_t|k_t) \\propto {\\displaystyle \\prod^{T}_{t=T-m}}\\mathcal{L}\\left(R_t|k_t\\right) P(R_t|k_t) \\propto {\\displaystyle \\prod^{T}_{t=T-m}}\\mathcal{L}\\left(R_t|k_t\\right) While this takes the last m m priors into account equally, you can decide to apply a windowing function (such as an exponential) to favor recent priors over more distant. Choosing a Likelihood Function \\mathcal{L}\\left(R_t|k_t\\right) \\mathcal{L}\\left(R_t|k_t\\right) \u00b6 A likelihood function function says how likely a value of R_t R_t is given an observed number of new cases k k . Any time you need to model 'arrivals' over some time period of time, statisticians like to use the Poisson Distribution . Given an average arrival rate of \\lambda \\lambda new cases per day, the probability of seeing k k new cases is distributed according to the Poisson distribution: P(k|\\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!} P(k|\\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!} <span><span class=\"MathJax_Preview\">P(k|\\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}</span><script type=\"math/tex\">P(k|\\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!} # Column vector of k k = np . arange ( 0 , 70 )[:, None ] # Different values of Lambda lambdas = [ 10 , 20 , 30 , 40 ] # Evaluated the Probability Mass Function (remember: poisson is discrete) y = sps . poisson . pmf ( k , lambdas ) # Show the resulting shape print ( y . shape ) (70, 4) Note : this was a terse expression which makes it tricky. All I did was to make k k a column. By giving it a column for k k and a 'row' for lambda it will evaluate the pmf over both and produce an array that has k k rows and lambda columns. This is an efficient way of producing many distributions all at once, and you will see it used again below ! fig , ax = plt . subplots () ax . set ( title = 'Poisson Distribution of Cases \\n $p(k|\\lambda)$' ) plt . plot ( k , y , marker = 'o' , markersize = 3 , lw = 0 ) plt . legend ( title = \"$\\lambda$\" , labels = lambdas ); The Poisson distribution says that if you think you're going to have \\lambda \\lambda cases per day, you'll probably get that many, plus or minus some variation based on chance. But in our case, we know there have been k k cases and we need to know what value of \\lambda \\lambda is most likely. In order to do this, we fix k k in place while varying \\lambda \\lambda . This is called the likelihood function. For example, imagine we observe k=20 k=20 new cases, and we want to know how likely each \\lambda \\lambda is: k = 20 lam = np . linspace ( 1 , 45 , 90 ) likelihood = pd . Series ( data = sps . poisson . pmf ( k , lam ), index = pd . Index ( lam , name = '$\\lambda$' ), name = 'lambda' ) likelihood . plot ( title = r 'Likelihood $L\\left(\\lambda|k_t\\right)$' ); This says that if we see 20 cases, the most likely value of \\lambda \\lambda is (not surprisingly) 20. But we're not certain: it's possible lambda was 21 or 17 and saw 20 new cases by chance alone. It also says that it's unlikely \\lambda \\lambda was 40 and we saw 20. Great. We have \\mathcal{L}\\left(\\lambda_t|k_t\\right) \\mathcal{L}\\left(\\lambda_t|k_t\\right) which is parameterized by \\lambda \\lambda but we were looking for \\mathcal{L}\\left(R_t|k_t\\right) \\mathcal{L}\\left(R_t|k_t\\right) which is parameterized by R_t R_t . We need to know the relationship between \\lambda \\lambda and R_t R_t Connecting \\lambda \\lambda and R_t R_t \u00b6 The key insight to making this work is to realize there's a connection between R_t R_t and \\lambda \\lambda . The derivation is beyond the scope of this notebook, but here it is: \\lambda = k_{t-1}e^{\\gamma(R_t-1)} \\lambda = k_{t-1}e^{\\gamma(R_t-1)} where \\gamma \\gamma is the reciprocal of the serial interval ( about 4 days for COVID19 ). Since we know every new case count on the previous day, we can now reformulate the likelihood function as a Poisson parameterized by fixing k k and varying R_t R_t . \\lambda = k_{t-1}e^{\\gamma(R_t-1)} \\lambda = k_{t-1}e^{\\gamma(R_t-1)} \\mathcal{L}\\left(R_t|k\\right) = \\frac{\\lambda^k e^{-\\lambda}}{k!} \\mathcal{L}\\left(R_t|k\\right) = \\frac{\\lambda^k e^{-\\lambda}}{k!} Evaluating the Likelihood Function \u00b6 To contiue our example, let's imagine a sample of new case counts k k . What is the likelihood of different values of R_t R_t on each of those days? k = np . array ([ 20 , 40 , 55 , 90 ]) # We create an array for every possible value of Rt R_T_MAX = 12 r_t_range = np . linspace ( 0 , R_T_MAX , R_T_MAX * 100 + 1 ) # Gamma is 1/serial interval # https://wwwnc.cdc.gov/eid/article/26/6/20-0357_article GAMMA = 1 / 4 # Map Rt into lambda so we can substitute it into the equation below # Note that we have N-1 lambdas because on the first day of an outbreak # you do not know what to expect. lam = k [: - 1 ] * np . exp ( GAMMA * ( r_t_range [:, None ] - 1 )) # Evaluate the likelihood on each day and normalize sum of each day to 1.0 likelihood_r_t = sps . poisson . pmf ( k [ 1 :], lam ) likelihood_r_t / np . sum ( likelihood_r_t , axis = 0 ) # Plot it ax = pd . DataFrame ( data = likelihood_r_t , index = r_t_range ) . plot ( title = 'Likelihood of $R_t$ given $k$' , xlim = ( 0 , 7 ) ) ax . legend ( labels = k [ 1 :], title = 'New Cases' ) ax . set_xlabel ( '$R_t$' ); You can see that each day we have a independent guesses for R_t R_t . The goal is to combine the information we have about previous days with the current day. To do this, we use Bayes' theorem. Performing the Bayesian Update \u00b6 To perform the Bayesian update, we need to multiply the likelihood by the prior (which is just the previous day's likelihood) to get the posteriors. Let's do that using the cumulative product of each successive day: posteriors = likelihood_r_t . cumprod ( axis = 1 ) posteriors = posteriors / np . sum ( posteriors , axis = 0 ) columns = pd . Index ( range ( 1 , posteriors . shape [ 1 ] + 1 ), name = 'Day' ) posteriors = pd . DataFrame ( data = posteriors , index = r_t_range , columns = columns ) ax = posteriors . plot ( title = 'Posterior $P(R_t|k)$' , xlim = ( 0 , 7 ) ) ax . legend ( title = 'Day' ) ax . set_xlabel ( '$R_t$' ); Notice how on Day 1, our posterior matches Day 1's likelihood from above? That's because we have no information other than that day. However, when we update the prior using Day 2's information, you can see the curve has moved left, but not nearly as left as the likelihood for Day 2 from above. This is because Bayesian updating uses information from both days and effectively averages the two. Since Day 3's likelihood is in between the other two, you see a small shift to the right, but more importantly: a narrower distribution. We're becoming more confident in our believes of the true value of R_t R_t . From these posteriors, we can answer important questions such as \"What is the most likely value of R_t R_t each day?\" most_likely_values = posteriors . idxmax ( axis = 0 ) We can also obtain the highest density intervals for R_t R_t : Note: I apologize in advance for the clunky brute force HDI algorithm. Please let me know if there are better ones out there. def highest_density_interval ( pmf , p =. 95 ): # If we pass a DataFrame, just call this recursively on the columns if ( isinstance ( pmf , pd . DataFrame )): return pd . DataFrame ([ highest_density_interval ( pmf [ col ]) for col in pmf ], index = pmf . columns ) cumsum = np . cumsum ( pmf . values ) best = None for i , value in enumerate ( cumsum ): for j , high_value in enumerate ( cumsum [ i + 1 :]): if ( high_value - value > p ) and ( not best or j < best [ 1 ] - best [ 0 ]): best = ( i , i + j + 1 ) break low = pmf . index [ best [ 0 ]] high = pmf . index [ best [ 1 ]] return pd . Series ([ low , high ], index = [ 'Low' , 'High' ]) hdi = highest_density_interval ( posteriors , p =. 95 ) Finally, we can plot both the most likely values for R_t R_t and the HDIs over time. This is the most useful representation as it shows how our beliefs change with every day. ax = most_likely_values . plot ( marker = 'o' , label = 'Most Likely' , title = f '$R_t$ by day' , c = 'k' , markersize = 4 ) ax . fill_between ( hdi . index , hdi [ 'Low' ], hdi [ 'High' ], color = 'k' , alpha =. 1 , lw = 0 , label = 'HDI' ) ax . legend (); We can see that the most likely value of R_t R_t changes with time and the highest-density interval narrows as we become more sure of the true value of R_t R_t over time. Note that since we only had four days of history, I did not apply my windowing modification to this sample. Next, however, we'll turn to a real-world application where this modification is necessary. Real-World Application to US Data \u00b6 Setup \u00b6 Load US state case data from the NYT archive url = 'https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv' states = pd . read_csv ( url , usecols = [ 0 , 1 , 3 ], index_col = [ 'state' , 'date' ], parse_dates = [ 'date' ], squeeze = True ) . sort_index () Taking a look at the state, we need to start the analysis when there are a consistent number of cases each day. Find the last zero new case day and start on the day after that. Also, case reporting is very erratic based on testing backlogs, etc. To get the best view of the 'true' data we can, I've applied a gaussian filter to the time series. This is obviously an arbitrary choice, but you'd imagine the real world process is not nearly as stochastic as the actual reporting. state_name = 'New York' def prepare_cases ( cases ): new_cases = cases . diff () smoothed = new_cases . rolling ( 7 , win_type = 'gaussian' , min_periods = 1 , center = True ) . mean ( std = 2 ) . round () zeros = smoothed . index [ smoothed . eq ( 0 )] if len ( zeros ) == 0 : idx_start = 0 else : last_zero = zeros . max () idx_start = smoothed . index . get_loc ( last_zero ) + 1 smoothed = smoothed . iloc [ idx_start :] original = new_cases . loc [ smoothed . index ] return original , smoothed cases = states . xs ( state_name ) . rename ( f \" { state_name } cases\" ) original , smoothed = prepare_cases ( cases ) original . plot ( title = f \" { state_name } New Cases per Day\" , c = 'k' , linestyle = ':' , alpha =. 5 , label = 'Actual' , legend = True , figsize = ( 600 / 72 , 400 / 72 )) ax = smoothed . plot ( label = 'Smoothed' , legend = True ) ax . get_figure () . set_facecolor ( 'w' ) Running the Algorithm \u00b6 Just like the example before, we create lambda based on the previous day's counts from all values of R_t R_t . Unlike the previous example, I now evaluate the log of the Poisson. Why? It makes windowing easier. Since \\log{ab}=\\log{a}+\\log{b} \\log{ab}=\\log{a}+\\log{b} , we can do a rolling sum over the last m m periods and then exponentiate to get the rolling product of the original values. This does not change any of the numbers \u2013 it's just a convenience. def get_posteriors ( sr , window = 7 , min_periods = 1 ): lam = sr [: - 1 ] . values * np . exp ( GAMMA * ( r_t_range [:, None ] - 1 )) # Note: if you want to have a Uniform prior you can use the following line instead. # I chose the gamma distribution because of our prior knowledge of the likely value # of R_t. # prior0 = np.full(len(r_t_range), np.log(1/len(r_t_range))) prior0 = np . log ( sps . gamma ( a = 3 ) . pdf ( r_t_range ) + 1e-14 ) likelihoods = pd . DataFrame ( # Short-hand way of concatenating the prior and likelihoods data = np . c_ [ prior0 , sps . poisson . logpmf ( sr [ 1 :] . values , lam )], index = r_t_range , columns = sr . index ) # Perform a rolling sum of log likelihoods. This is the equivalent # of multiplying the original distributions. Exponentiate to move # out of log. posteriors = likelihoods . rolling ( window , axis = 1 , min_periods = min_periods ) . sum () posteriors = np . exp ( posteriors ) # Normalize to 1.0 posteriors = posteriors . div ( posteriors . sum ( axis = 0 ), axis = 1 ) return posteriors posteriors = get_posteriors ( smoothed ) The Result \u00b6 Below you can see every day (row) of the posterior distribution plotted simultaneously. The posteriors start without much confidence (wide) and become progressively more confident (narrower) about the true value of R_t R_t ax = posteriors . plot ( title = f ' { state_name } - Daily Posterior for $R_t$' , legend = False , lw = 1 , c = 'k' , alpha =. 3 , xlim = ( 0.4 , 4 )) ax . set_xlabel ( '$R_t$' ); Plotting in the Time Domain with Credible Intervals \u00b6 Since our results include uncertainty, we'd like to be able to view the most likely value of R_t R_t along with its highest-density interval. # Note that this takes a while to execute - it's not the most efficient algorithm hdis = highest_density_interval ( posteriors ) most_likely = posteriors . idxmax () . rename ( 'ML' ) # Look into why you shift -1 result = pd . concat ([ most_likely , hdis ], axis = 1 ) result . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ML Low High date 2020-04-07 1.07 1.04 1.11 2020-04-08 1.05 1.01 1.08 2020-04-09 1.04 1.00 1.07 2020-04-10 1.04 1.00 1.07 2020-04-11 1.04 1.01 1.07 def plot_rt ( result , ax , state_name ): ax . set_title ( f \" { state_name } \" ) # Colors ABOVE = [ 1 , 0 , 0 ] MIDDLE = [ 1 , 1 , 1 ] BELOW = [ 0 , 0 , 0 ] cmap = ListedColormap ( np . r_ [ np . linspace ( BELOW , MIDDLE , 25 ), np . linspace ( MIDDLE , ABOVE , 25 ) ]) color_mapped = lambda y : np . clip ( y , . 5 , 1.5 ) -. 5 index = result [ 'ML' ] . index . get_level_values ( 'date' ) values = result [ 'ML' ] . values # Plot dots and line ax . plot ( index , values , c = 'k' , zorder = 1 , alpha =. 25 ) ax . scatter ( index , values , s = 40 , lw =. 5 , c = cmap ( color_mapped ( values )), edgecolors = 'k' , zorder = 2 ) # Aesthetically, extrapolate credible interval by 1 day either side lowfn = interp1d ( date2num ( index ), result [ 'Low' ] . values , bounds_error = False , fill_value = 'extrapolate' ) highfn = interp1d ( date2num ( index ), result [ 'High' ] . values , bounds_error = False , fill_value = 'extrapolate' ) extended = pd . date_range ( start = pd . Timestamp ( '2020-03-01' ), end = index [ - 1 ] + pd . Timedelta ( days = 1 )) ax . fill_between ( extended , lowfn ( date2num ( extended )), highfn ( date2num ( extended )), color = 'k' , alpha =. 1 , lw = 0 , zorder = 3 ) ax . axhline ( 1.0 , c = 'k' , lw = 1 , label = '$R_t=1.0$' , alpha =. 25 ); # Formatting ax . xaxis . set_major_locator ( mdates . MonthLocator ()) ax . xaxis . set_major_formatter ( mdates . DateFormatter ( '%b' )) ax . xaxis . set_minor_locator ( mdates . DayLocator ()) ax . yaxis . set_major_locator ( ticker . MultipleLocator ( 1 )) ax . yaxis . set_major_formatter ( ticker . StrMethodFormatter ( \" {x:.1f} \" )) ax . yaxis . tick_right () ax . spines [ 'left' ] . set_visible ( False ) ax . spines [ 'bottom' ] . set_visible ( False ) ax . spines [ 'right' ] . set_visible ( False ) ax . margins ( 0 ) ax . grid ( which = 'major' , axis = 'y' , c = 'k' , alpha =. 1 , zorder =- 2 ) ax . margins ( 0 ) ax . set_ylim ( 0.0 , 3.5 ) ax . set_xlim ( pd . Timestamp ( '2020-03-01' ), result . index . get_level_values ( 'date' )[ - 1 ] + pd . Timedelta ( days = 1 )) fig . set_facecolor ( 'w' ) fig , ax = plt . subplots ( figsize = ( 600 / 72 , 400 / 72 )) plot_rt ( result , ax , state_name ) ax . set_title ( f 'Real-time $R_t$ for { state_name } ' ) ax . set_ylim ( . 5 , 3.5 ) ax . xaxis . set_major_locator ( mdates . WeekdayLocator ()) ax . xaxis . set_major_formatter ( mdates . DateFormatter ( '%b %d ' )) Repeat the Process for Every State \u00b6 results = {} states_to_process = states . loc [ ~ states . index . get_level_values ( 'state' ) . isin ( FILTERED_REGIONS )] for state_name , cases in states_to_process . groupby ( level = 'state' ): clear_output ( wait = True ) print ( f 'Processing { state_name } ' ) new , smoothed = prepare_cases ( cases ) print ( ' \\t Getting Posteriors' ) try : posteriors = get_posteriors ( smoothed ) except : display ( cases ) print ( ' \\t Getting HDIs' ) hdis = highest_density_interval ( posteriors ) print ( ' \\t Getting most likely values' ) most_likely = posteriors . idxmax () . rename ( 'ML' ) result = pd . concat ([ most_likely , hdis ], axis = 1 ) results [ state_name ] = result . droplevel ( 0 ) clear_output ( wait = True ) print ( 'Done.' ) Done. Plot All US States \u00b6 ncols = 4 nrows = int ( np . ceil ( len ( results ) / ncols )) # fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, nrows*3)) fig , axes = plt . subplots ( nrows = nrows , ncols = ncols , figsize = ( 15 , nrows * 3 )) for i , ( state_name , result ) in enumerate ( results . items ()): plot_rt ( result , axes . flat [ i ], state_name ) fig . tight_layout () fig . set_facecolor ( 'w' ) Export Data to CSV \u00b6 overall = None for state_name , result in results . items (): r = result . copy () r . index = pd . MultiIndex . from_product ([[ state_name ], result . index ]) if overall is None : overall = r else : overall = pd . concat ([ overall , r ]) overall . sort_index ( inplace = True ) # Uncomment this line if you'd like to export # overall.to_csv('data/rt.csv') Standings \u00b6 # As of 4/12 no_lockdown = [ 'North Dakota' , 'South Dakota' , 'Nebraska' , 'Iowa' , 'Arkansas' ] partial_lockdown = [ 'Utah' , 'Wyoming' , 'Oklahoma' ] FULL_COLOR = [ . 7 , . 7 , . 7 ] NONE_COLOR = [ 179 / 255 , 35 / 255 , 14 / 255 ] PARTIAL_COLOR = [ . 5 , . 5 , . 5 ] ERROR_BAR_COLOR = [ . 3 , . 3 , . 3 ] filtered = overall . index . get_level_values ( 0 ) . isin ( FILTERED_REGIONS ) mr = overall . loc [ ~ filtered ] . groupby ( level = 0 )[[ 'ML' , 'High' , 'Low' ]] . last () def plot_standings ( mr , figsize = None , title = 'Most Recent $R_t$ by State' ): if not figsize : figsize = (( 15.9 / 50 ) * len ( mr ) +. 1 , 2.5 ) fig , ax = plt . subplots ( figsize = figsize ) ax . set_title ( title ) err = mr [[ 'Low' , 'High' ]] . sub ( mr [ 'ML' ], axis = 0 ) . abs () bars = ax . bar ( mr . index , mr [ 'ML' ], width =. 825 , color = FULL_COLOR , ecolor = ERROR_BAR_COLOR , capsize = 2 , error_kw = { 'alpha' : . 5 , 'lw' : 1 }, yerr = err . values . T ) for bar , state_name in zip ( bars , mr . index ): if state_name in no_lockdown : bar . set_color ( NONE_COLOR ) if state_name in partial_lockdown : bar . set_color ( PARTIAL_COLOR ) labels = mr . index . to_series () . replace ({ 'District of Columbia' : 'DC' }) ax . set_xticklabels ( labels , rotation = 90 , fontsize = 11 ) ax . margins ( 0 ) ax . set_ylim ( 0 , 2. ) ax . axhline ( 1.0 , linestyle = ':' , color = 'k' , lw = 1 ) leg = ax . legend ( handles = [ Patch ( label = 'Full' , color = FULL_COLOR ), Patch ( label = 'Partial' , color = PARTIAL_COLOR ), Patch ( label = 'None' , color = NONE_COLOR ) ], title = 'Lockdown' , ncol = 3 , loc = 'upper left' , columnspacing =. 75 , handletextpad =. 5 , handlelength = 1 ) leg . _legend_box . align = \"left\" fig . set_facecolor ( 'w' ) return fig , ax mr . sort_values ( 'ML' , inplace = True ) plot_standings ( mr ); mr . sort_values ( 'High' , inplace = True ) plot_standings ( mr ); show = mr [ mr . High . le ( 1.1 )] . sort_values ( 'ML' ) fig , ax = plot_standings ( show , title = 'Likely Under Control' ); show = mr [ mr . Low . ge ( 1.05 )] . sort_values ( 'Low' ) fig , ax = plot_standings ( show , title = 'Likely Not Under Control' ); ax . get_legend () . remove ()","title":"Notebook"},{"location":"notebooks/notebook/#estimating-covid-19s-r_tr_t-in-real-time","text":"Kevin Systrom - April 12 In any epidemic, R_t R_t is the measure known as the effective reproduction number. It's the number of people who become infected per infectious person at time t t . The most well-known version of this number is the basic reproduction number: R_0 R_0 when t=0 t=0 . However, R_0 R_0 is a single measure that does not adapt with changes in behavior and restrictions. As a pandemic evolves, increasing restrictions (or potential releasing of restrictions) change R_t R_t . Knowing the current R_t R_t is essential. When R>1 R>1 , the pandemic will spread through the entire population. If R_t<1 R_t<1 , the pandemic will grow to some fixed number less than the population. The lower R_t R_t , the more manageable the situation. The value of R_t R_t helps us (1) understand how effective our measures have been controlling an outbreak and (2) gives us vital information about whether we should increase or reduce restrictions based on our competing goals of economic prosperity and human safety. Well-respected epidemiologists argue that tracking R_t R_t is the only way to manage through this crisis. Yet, today, to my knowledge there is no real-time tracking of R_t R_t in United States. In fact, the only real-time measure I've seen has been for Hong Kong . More importantly, it is not useful to understand R_t R_t at a national level. Instead, to manage this crisis effectively, we need a local (state, county and/or city) level granularity of R_t R_t . What follows is a solution to this problem at the US State level. It's a modified version of a solution created by Bettencourt & Ribeiro 2008 to estimate real-time R_t R_t using a Bayesian approach. While I have stayed true to most of their process, my solution differs in an important way that I will call out clearly. If you have questions, comments, or improvments feel free to get in touch: hello@systrom.com . And if it's not entirely clear, I'm not an epidemiologist. At the same time, data is data, and statistics are statistics and this is based on work by well-known epidemiologists so calibrate accordingly. In the meantime, I hope you can learn something new as I did by reading through this example. Feel free to take this work and apply it elsewhere \u2013 internationally or to counties in the United States. import pandas as pd import numpy as np from matplotlib import pyplot as plt from matplotlib.dates import date2num , num2date from matplotlib import dates as mdates from matplotlib import ticker from matplotlib.colors import ListedColormap from matplotlib.patches import Patch from scipy import stats as sps from scipy.interpolate import interp1d from IPython.display import clear_output FILTERED_REGIONS = [ 'Virgin Islands' , 'American Samoa' , 'Northern Mariana Islands' , 'Guam' , 'Puerto Rico' ] % config InlineBackend . figure_format = 'retina'","title":"Estimating COVID-19's R_tR_t in Real-Time"},{"location":"notebooks/notebook/#bettencourt-ribeiros-approach","text":"Every day, we learn how many more people have COVID-19. This new case count gives us a clue about the current value of R_t R_t . We also, figure that the value of R_t R_t today is related to the value of R_{t-1} R_{t-1} (yesterday's value) and every previous value of R_{t-m} R_{t-m} for that matter. With these insights, the authors use Bayes' rule to update their beliefs about the true value of R_t R_t based on how many new cases have been reported each day. This is Bayes' Theorem as we'll use it: P(R_t|k)=\\frac{P(R_t)\\cdot\\mathcal{L}(R_t|k)}{P(k)} P(R_t|k)=\\frac{P(R_t)\\cdot\\mathcal{L}(R_t|k)}{P(k)} This says that, having seen k k new cases, we believe the distribution of R_t R_t is equal to: The prior beliefs of the value of P(R_t) P(R_t) without the data ... times the likelihood of R_t R_t given that we've seen k k new cases ... divided by the probability of seeing this many cases in general. Importantly, P(k) P(k) is a constant, so the numerator is proportional to the posterior. Since all probability distributions sum to 1.0, we can ignore P(k) P(k) and normalize our posterior to sum to 1.0: P(R_t|k) \\propto P(R_t) \\cdot \\mathcal{L}(R_t|k) P(R_t|k) \\propto P(R_t) \\cdot \\mathcal{L}(R_t|k) This is for a single day. To make it iterative: every day that passes, we use yesterday's conclusion (ie. posterior) P(R_{t-1}|k_{t-1}) P(R_{t-1}|k_{t-1}) to be today's prior P(R_t) P(R_t) so on day two: P(R_2|k) \\propto P(R_0)\\cdot\\mathcal{L}(R_2|k_2)\\cdot\\mathcal{L}(R_1|k_1) P(R_2|k) \\propto P(R_0)\\cdot\\mathcal{L}(R_2|k_2)\\cdot\\mathcal{L}(R_1|k_1) And more generally: P(R_t|k_t) \\propto P(R_0) \\cdot {\\displaystyle \\prod^{T}_{t=0}}\\mathcal{L}(R_t|k_t) P(R_t|k_t) \\propto P(R_0) \\cdot {\\displaystyle \\prod^{T}_{t=0}}\\mathcal{L}(R_t|k_t) With a uniform prior P(R_0) P(R_0) , this reduces to: $$ P(R_t|k_t) \\propto {\\displaystyle \\prod^{T}_{t=0}}\\mathcal{L}\\left(R_t|k_t\\right) $$","title":"Bettencourt &amp; Ribeiro's Approach"},{"location":"notebooks/notebook/#my-proposed-modification","text":"This works fine, but it suffers from an issue: the posterior on any given day is equally influenced by the distant past as much as the recent day. For epidemics that have R_t>1 R_t>1 for a long time and then become under control ( R_t<1 R_t<1 ), the posterior gets stuck. It cannot forget about the many days where R_t>1 R_t>1 so eventually P(R_t|k) P(R_t|k) asymptotically approaches 1 when we know it's well under 1. The authors note this in the paper as a footnote. Unfortunately this won't work for us. The most critical thing to know is when we've dipped below the 1.0 threshold! So, I propose to only incorporate the last m m days of the likelihood function. By doing this, the algorithm's prior is built based on the recent past which is a much more useful prior than the entire history of the epidemic. So this simple, but important change leads to the following: P(R_t|k_t) \\propto {\\displaystyle \\prod^{T}_{t=T-m}}\\mathcal{L}\\left(R_t|k_t\\right) P(R_t|k_t) \\propto {\\displaystyle \\prod^{T}_{t=T-m}}\\mathcal{L}\\left(R_t|k_t\\right) While this takes the last m m priors into account equally, you can decide to apply a windowing function (such as an exponential) to favor recent priors over more distant.","title":"My Proposed Modification"},{"location":"notebooks/notebook/#choosing-a-likelihood-function-mathcallleftr_tk_trightmathcallleftr_tk_tright","text":"A likelihood function function says how likely a value of R_t R_t is given an observed number of new cases k k . Any time you need to model 'arrivals' over some time period of time, statisticians like to use the Poisson Distribution . Given an average arrival rate of \\lambda \\lambda new cases per day, the probability of seeing k k new cases is distributed according to the Poisson distribution: P(k|\\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!} P(k|\\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!} <span><span class=\"MathJax_Preview\">P(k|\\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}</span><script type=\"math/tex\">P(k|\\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!} # Column vector of k k = np . arange ( 0 , 70 )[:, None ] # Different values of Lambda lambdas = [ 10 , 20 , 30 , 40 ] # Evaluated the Probability Mass Function (remember: poisson is discrete) y = sps . poisson . pmf ( k , lambdas ) # Show the resulting shape print ( y . shape ) (70, 4) Note : this was a terse expression which makes it tricky. All I did was to make k k a column. By giving it a column for k k and a 'row' for lambda it will evaluate the pmf over both and produce an array that has k k rows and lambda columns. This is an efficient way of producing many distributions all at once, and you will see it used again below ! fig , ax = plt . subplots () ax . set ( title = 'Poisson Distribution of Cases \\n $p(k|\\lambda)$' ) plt . plot ( k , y , marker = 'o' , markersize = 3 , lw = 0 ) plt . legend ( title = \"$\\lambda$\" , labels = lambdas ); The Poisson distribution says that if you think you're going to have \\lambda \\lambda cases per day, you'll probably get that many, plus or minus some variation based on chance. But in our case, we know there have been k k cases and we need to know what value of \\lambda \\lambda is most likely. In order to do this, we fix k k in place while varying \\lambda \\lambda . This is called the likelihood function. For example, imagine we observe k=20 k=20 new cases, and we want to know how likely each \\lambda \\lambda is: k = 20 lam = np . linspace ( 1 , 45 , 90 ) likelihood = pd . Series ( data = sps . poisson . pmf ( k , lam ), index = pd . Index ( lam , name = '$\\lambda$' ), name = 'lambda' ) likelihood . plot ( title = r 'Likelihood $L\\left(\\lambda|k_t\\right)$' ); This says that if we see 20 cases, the most likely value of \\lambda \\lambda is (not surprisingly) 20. But we're not certain: it's possible lambda was 21 or 17 and saw 20 new cases by chance alone. It also says that it's unlikely \\lambda \\lambda was 40 and we saw 20. Great. We have \\mathcal{L}\\left(\\lambda_t|k_t\\right) \\mathcal{L}\\left(\\lambda_t|k_t\\right) which is parameterized by \\lambda \\lambda but we were looking for \\mathcal{L}\\left(R_t|k_t\\right) \\mathcal{L}\\left(R_t|k_t\\right) which is parameterized by R_t R_t . We need to know the relationship between \\lambda \\lambda and R_t R_t","title":"Choosing a Likelihood Function \\mathcal{L}\\left(R_t|k_t\\right)\\mathcal{L}\\left(R_t|k_t\\right)"},{"location":"notebooks/notebook/#connecting-lambdalambda-and-r_tr_t","text":"The key insight to making this work is to realize there's a connection between R_t R_t and \\lambda \\lambda . The derivation is beyond the scope of this notebook, but here it is: \\lambda = k_{t-1}e^{\\gamma(R_t-1)} \\lambda = k_{t-1}e^{\\gamma(R_t-1)} where \\gamma \\gamma is the reciprocal of the serial interval ( about 4 days for COVID19 ). Since we know every new case count on the previous day, we can now reformulate the likelihood function as a Poisson parameterized by fixing k k and varying R_t R_t . \\lambda = k_{t-1}e^{\\gamma(R_t-1)} \\lambda = k_{t-1}e^{\\gamma(R_t-1)} \\mathcal{L}\\left(R_t|k\\right) = \\frac{\\lambda^k e^{-\\lambda}}{k!} \\mathcal{L}\\left(R_t|k\\right) = \\frac{\\lambda^k e^{-\\lambda}}{k!}","title":"Connecting \\lambda\\lambda and R_tR_t"},{"location":"notebooks/notebook/#evaluating-the-likelihood-function","text":"To contiue our example, let's imagine a sample of new case counts k k . What is the likelihood of different values of R_t R_t on each of those days? k = np . array ([ 20 , 40 , 55 , 90 ]) # We create an array for every possible value of Rt R_T_MAX = 12 r_t_range = np . linspace ( 0 , R_T_MAX , R_T_MAX * 100 + 1 ) # Gamma is 1/serial interval # https://wwwnc.cdc.gov/eid/article/26/6/20-0357_article GAMMA = 1 / 4 # Map Rt into lambda so we can substitute it into the equation below # Note that we have N-1 lambdas because on the first day of an outbreak # you do not know what to expect. lam = k [: - 1 ] * np . exp ( GAMMA * ( r_t_range [:, None ] - 1 )) # Evaluate the likelihood on each day and normalize sum of each day to 1.0 likelihood_r_t = sps . poisson . pmf ( k [ 1 :], lam ) likelihood_r_t / np . sum ( likelihood_r_t , axis = 0 ) # Plot it ax = pd . DataFrame ( data = likelihood_r_t , index = r_t_range ) . plot ( title = 'Likelihood of $R_t$ given $k$' , xlim = ( 0 , 7 ) ) ax . legend ( labels = k [ 1 :], title = 'New Cases' ) ax . set_xlabel ( '$R_t$' ); You can see that each day we have a independent guesses for R_t R_t . The goal is to combine the information we have about previous days with the current day. To do this, we use Bayes' theorem.","title":"Evaluating the Likelihood Function"},{"location":"notebooks/notebook/#performing-the-bayesian-update","text":"To perform the Bayesian update, we need to multiply the likelihood by the prior (which is just the previous day's likelihood) to get the posteriors. Let's do that using the cumulative product of each successive day: posteriors = likelihood_r_t . cumprod ( axis = 1 ) posteriors = posteriors / np . sum ( posteriors , axis = 0 ) columns = pd . Index ( range ( 1 , posteriors . shape [ 1 ] + 1 ), name = 'Day' ) posteriors = pd . DataFrame ( data = posteriors , index = r_t_range , columns = columns ) ax = posteriors . plot ( title = 'Posterior $P(R_t|k)$' , xlim = ( 0 , 7 ) ) ax . legend ( title = 'Day' ) ax . set_xlabel ( '$R_t$' ); Notice how on Day 1, our posterior matches Day 1's likelihood from above? That's because we have no information other than that day. However, when we update the prior using Day 2's information, you can see the curve has moved left, but not nearly as left as the likelihood for Day 2 from above. This is because Bayesian updating uses information from both days and effectively averages the two. Since Day 3's likelihood is in between the other two, you see a small shift to the right, but more importantly: a narrower distribution. We're becoming more confident in our believes of the true value of R_t R_t . From these posteriors, we can answer important questions such as \"What is the most likely value of R_t R_t each day?\" most_likely_values = posteriors . idxmax ( axis = 0 ) We can also obtain the highest density intervals for R_t R_t : Note: I apologize in advance for the clunky brute force HDI algorithm. Please let me know if there are better ones out there. def highest_density_interval ( pmf , p =. 95 ): # If we pass a DataFrame, just call this recursively on the columns if ( isinstance ( pmf , pd . DataFrame )): return pd . DataFrame ([ highest_density_interval ( pmf [ col ]) for col in pmf ], index = pmf . columns ) cumsum = np . cumsum ( pmf . values ) best = None for i , value in enumerate ( cumsum ): for j , high_value in enumerate ( cumsum [ i + 1 :]): if ( high_value - value > p ) and ( not best or j < best [ 1 ] - best [ 0 ]): best = ( i , i + j + 1 ) break low = pmf . index [ best [ 0 ]] high = pmf . index [ best [ 1 ]] return pd . Series ([ low , high ], index = [ 'Low' , 'High' ]) hdi = highest_density_interval ( posteriors , p =. 95 ) Finally, we can plot both the most likely values for R_t R_t and the HDIs over time. This is the most useful representation as it shows how our beliefs change with every day. ax = most_likely_values . plot ( marker = 'o' , label = 'Most Likely' , title = f '$R_t$ by day' , c = 'k' , markersize = 4 ) ax . fill_between ( hdi . index , hdi [ 'Low' ], hdi [ 'High' ], color = 'k' , alpha =. 1 , lw = 0 , label = 'HDI' ) ax . legend (); We can see that the most likely value of R_t R_t changes with time and the highest-density interval narrows as we become more sure of the true value of R_t R_t over time. Note that since we only had four days of history, I did not apply my windowing modification to this sample. Next, however, we'll turn to a real-world application where this modification is necessary.","title":"Performing the Bayesian Update"},{"location":"notebooks/notebook/#real-world-application-to-us-data","text":"","title":"Real-World Application to US Data"},{"location":"notebooks/notebook/#setup","text":"Load US state case data from the NYT archive url = 'https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv' states = pd . read_csv ( url , usecols = [ 0 , 1 , 3 ], index_col = [ 'state' , 'date' ], parse_dates = [ 'date' ], squeeze = True ) . sort_index () Taking a look at the state, we need to start the analysis when there are a consistent number of cases each day. Find the last zero new case day and start on the day after that. Also, case reporting is very erratic based on testing backlogs, etc. To get the best view of the 'true' data we can, I've applied a gaussian filter to the time series. This is obviously an arbitrary choice, but you'd imagine the real world process is not nearly as stochastic as the actual reporting. state_name = 'New York' def prepare_cases ( cases ): new_cases = cases . diff () smoothed = new_cases . rolling ( 7 , win_type = 'gaussian' , min_periods = 1 , center = True ) . mean ( std = 2 ) . round () zeros = smoothed . index [ smoothed . eq ( 0 )] if len ( zeros ) == 0 : idx_start = 0 else : last_zero = zeros . max () idx_start = smoothed . index . get_loc ( last_zero ) + 1 smoothed = smoothed . iloc [ idx_start :] original = new_cases . loc [ smoothed . index ] return original , smoothed cases = states . xs ( state_name ) . rename ( f \" { state_name } cases\" ) original , smoothed = prepare_cases ( cases ) original . plot ( title = f \" { state_name } New Cases per Day\" , c = 'k' , linestyle = ':' , alpha =. 5 , label = 'Actual' , legend = True , figsize = ( 600 / 72 , 400 / 72 )) ax = smoothed . plot ( label = 'Smoothed' , legend = True ) ax . get_figure () . set_facecolor ( 'w' )","title":"Setup"},{"location":"notebooks/notebook/#running-the-algorithm","text":"Just like the example before, we create lambda based on the previous day's counts from all values of R_t R_t . Unlike the previous example, I now evaluate the log of the Poisson. Why? It makes windowing easier. Since \\log{ab}=\\log{a}+\\log{b} \\log{ab}=\\log{a}+\\log{b} , we can do a rolling sum over the last m m periods and then exponentiate to get the rolling product of the original values. This does not change any of the numbers \u2013 it's just a convenience. def get_posteriors ( sr , window = 7 , min_periods = 1 ): lam = sr [: - 1 ] . values * np . exp ( GAMMA * ( r_t_range [:, None ] - 1 )) # Note: if you want to have a Uniform prior you can use the following line instead. # I chose the gamma distribution because of our prior knowledge of the likely value # of R_t. # prior0 = np.full(len(r_t_range), np.log(1/len(r_t_range))) prior0 = np . log ( sps . gamma ( a = 3 ) . pdf ( r_t_range ) + 1e-14 ) likelihoods = pd . DataFrame ( # Short-hand way of concatenating the prior and likelihoods data = np . c_ [ prior0 , sps . poisson . logpmf ( sr [ 1 :] . values , lam )], index = r_t_range , columns = sr . index ) # Perform a rolling sum of log likelihoods. This is the equivalent # of multiplying the original distributions. Exponentiate to move # out of log. posteriors = likelihoods . rolling ( window , axis = 1 , min_periods = min_periods ) . sum () posteriors = np . exp ( posteriors ) # Normalize to 1.0 posteriors = posteriors . div ( posteriors . sum ( axis = 0 ), axis = 1 ) return posteriors posteriors = get_posteriors ( smoothed )","title":"Running the Algorithm"},{"location":"notebooks/notebook/#the-result","text":"Below you can see every day (row) of the posterior distribution plotted simultaneously. The posteriors start without much confidence (wide) and become progressively more confident (narrower) about the true value of R_t R_t ax = posteriors . plot ( title = f ' { state_name } - Daily Posterior for $R_t$' , legend = False , lw = 1 , c = 'k' , alpha =. 3 , xlim = ( 0.4 , 4 )) ax . set_xlabel ( '$R_t$' );","title":"The Result"},{"location":"notebooks/notebook/#plotting-in-the-time-domain-with-credible-intervals","text":"Since our results include uncertainty, we'd like to be able to view the most likely value of R_t R_t along with its highest-density interval. # Note that this takes a while to execute - it's not the most efficient algorithm hdis = highest_density_interval ( posteriors ) most_likely = posteriors . idxmax () . rename ( 'ML' ) # Look into why you shift -1 result = pd . concat ([ most_likely , hdis ], axis = 1 ) result . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ML Low High date 2020-04-07 1.07 1.04 1.11 2020-04-08 1.05 1.01 1.08 2020-04-09 1.04 1.00 1.07 2020-04-10 1.04 1.00 1.07 2020-04-11 1.04 1.01 1.07 def plot_rt ( result , ax , state_name ): ax . set_title ( f \" { state_name } \" ) # Colors ABOVE = [ 1 , 0 , 0 ] MIDDLE = [ 1 , 1 , 1 ] BELOW = [ 0 , 0 , 0 ] cmap = ListedColormap ( np . r_ [ np . linspace ( BELOW , MIDDLE , 25 ), np . linspace ( MIDDLE , ABOVE , 25 ) ]) color_mapped = lambda y : np . clip ( y , . 5 , 1.5 ) -. 5 index = result [ 'ML' ] . index . get_level_values ( 'date' ) values = result [ 'ML' ] . values # Plot dots and line ax . plot ( index , values , c = 'k' , zorder = 1 , alpha =. 25 ) ax . scatter ( index , values , s = 40 , lw =. 5 , c = cmap ( color_mapped ( values )), edgecolors = 'k' , zorder = 2 ) # Aesthetically, extrapolate credible interval by 1 day either side lowfn = interp1d ( date2num ( index ), result [ 'Low' ] . values , bounds_error = False , fill_value = 'extrapolate' ) highfn = interp1d ( date2num ( index ), result [ 'High' ] . values , bounds_error = False , fill_value = 'extrapolate' ) extended = pd . date_range ( start = pd . Timestamp ( '2020-03-01' ), end = index [ - 1 ] + pd . Timedelta ( days = 1 )) ax . fill_between ( extended , lowfn ( date2num ( extended )), highfn ( date2num ( extended )), color = 'k' , alpha =. 1 , lw = 0 , zorder = 3 ) ax . axhline ( 1.0 , c = 'k' , lw = 1 , label = '$R_t=1.0$' , alpha =. 25 ); # Formatting ax . xaxis . set_major_locator ( mdates . MonthLocator ()) ax . xaxis . set_major_formatter ( mdates . DateFormatter ( '%b' )) ax . xaxis . set_minor_locator ( mdates . DayLocator ()) ax . yaxis . set_major_locator ( ticker . MultipleLocator ( 1 )) ax . yaxis . set_major_formatter ( ticker . StrMethodFormatter ( \" {x:.1f} \" )) ax . yaxis . tick_right () ax . spines [ 'left' ] . set_visible ( False ) ax . spines [ 'bottom' ] . set_visible ( False ) ax . spines [ 'right' ] . set_visible ( False ) ax . margins ( 0 ) ax . grid ( which = 'major' , axis = 'y' , c = 'k' , alpha =. 1 , zorder =- 2 ) ax . margins ( 0 ) ax . set_ylim ( 0.0 , 3.5 ) ax . set_xlim ( pd . Timestamp ( '2020-03-01' ), result . index . get_level_values ( 'date' )[ - 1 ] + pd . Timedelta ( days = 1 )) fig . set_facecolor ( 'w' ) fig , ax = plt . subplots ( figsize = ( 600 / 72 , 400 / 72 )) plot_rt ( result , ax , state_name ) ax . set_title ( f 'Real-time $R_t$ for { state_name } ' ) ax . set_ylim ( . 5 , 3.5 ) ax . xaxis . set_major_locator ( mdates . WeekdayLocator ()) ax . xaxis . set_major_formatter ( mdates . DateFormatter ( '%b %d ' ))","title":"Plotting in the Time Domain with Credible Intervals"},{"location":"notebooks/notebook/#repeat-the-process-for-every-state","text":"results = {} states_to_process = states . loc [ ~ states . index . get_level_values ( 'state' ) . isin ( FILTERED_REGIONS )] for state_name , cases in states_to_process . groupby ( level = 'state' ): clear_output ( wait = True ) print ( f 'Processing { state_name } ' ) new , smoothed = prepare_cases ( cases ) print ( ' \\t Getting Posteriors' ) try : posteriors = get_posteriors ( smoothed ) except : display ( cases ) print ( ' \\t Getting HDIs' ) hdis = highest_density_interval ( posteriors ) print ( ' \\t Getting most likely values' ) most_likely = posteriors . idxmax () . rename ( 'ML' ) result = pd . concat ([ most_likely , hdis ], axis = 1 ) results [ state_name ] = result . droplevel ( 0 ) clear_output ( wait = True ) print ( 'Done.' ) Done.","title":"Repeat the Process for Every State"},{"location":"notebooks/notebook/#plot-all-us-states","text":"ncols = 4 nrows = int ( np . ceil ( len ( results ) / ncols )) # fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, nrows*3)) fig , axes = plt . subplots ( nrows = nrows , ncols = ncols , figsize = ( 15 , nrows * 3 )) for i , ( state_name , result ) in enumerate ( results . items ()): plot_rt ( result , axes . flat [ i ], state_name ) fig . tight_layout () fig . set_facecolor ( 'w' )","title":"Plot All US States"},{"location":"notebooks/notebook/#export-data-to-csv","text":"overall = None for state_name , result in results . items (): r = result . copy () r . index = pd . MultiIndex . from_product ([[ state_name ], result . index ]) if overall is None : overall = r else : overall = pd . concat ([ overall , r ]) overall . sort_index ( inplace = True ) # Uncomment this line if you'd like to export # overall.to_csv('data/rt.csv')","title":"Export Data to CSV"},{"location":"notebooks/notebook/#standings","text":"# As of 4/12 no_lockdown = [ 'North Dakota' , 'South Dakota' , 'Nebraska' , 'Iowa' , 'Arkansas' ] partial_lockdown = [ 'Utah' , 'Wyoming' , 'Oklahoma' ] FULL_COLOR = [ . 7 , . 7 , . 7 ] NONE_COLOR = [ 179 / 255 , 35 / 255 , 14 / 255 ] PARTIAL_COLOR = [ . 5 , . 5 , . 5 ] ERROR_BAR_COLOR = [ . 3 , . 3 , . 3 ] filtered = overall . index . get_level_values ( 0 ) . isin ( FILTERED_REGIONS ) mr = overall . loc [ ~ filtered ] . groupby ( level = 0 )[[ 'ML' , 'High' , 'Low' ]] . last () def plot_standings ( mr , figsize = None , title = 'Most Recent $R_t$ by State' ): if not figsize : figsize = (( 15.9 / 50 ) * len ( mr ) +. 1 , 2.5 ) fig , ax = plt . subplots ( figsize = figsize ) ax . set_title ( title ) err = mr [[ 'Low' , 'High' ]] . sub ( mr [ 'ML' ], axis = 0 ) . abs () bars = ax . bar ( mr . index , mr [ 'ML' ], width =. 825 , color = FULL_COLOR , ecolor = ERROR_BAR_COLOR , capsize = 2 , error_kw = { 'alpha' : . 5 , 'lw' : 1 }, yerr = err . values . T ) for bar , state_name in zip ( bars , mr . index ): if state_name in no_lockdown : bar . set_color ( NONE_COLOR ) if state_name in partial_lockdown : bar . set_color ( PARTIAL_COLOR ) labels = mr . index . to_series () . replace ({ 'District of Columbia' : 'DC' }) ax . set_xticklabels ( labels , rotation = 90 , fontsize = 11 ) ax . margins ( 0 ) ax . set_ylim ( 0 , 2. ) ax . axhline ( 1.0 , linestyle = ':' , color = 'k' , lw = 1 ) leg = ax . legend ( handles = [ Patch ( label = 'Full' , color = FULL_COLOR ), Patch ( label = 'Partial' , color = PARTIAL_COLOR ), Patch ( label = 'None' , color = NONE_COLOR ) ], title = 'Lockdown' , ncol = 3 , loc = 'upper left' , columnspacing =. 75 , handletextpad =. 5 , handlelength = 1 ) leg . _legend_box . align = \"left\" fig . set_facecolor ( 'w' ) return fig , ax mr . sort_values ( 'ML' , inplace = True ) plot_standings ( mr ); mr . sort_values ( 'High' , inplace = True ) plot_standings ( mr ); show = mr [ mr . High . le ( 1.1 )] . sort_values ( 'ML' ) fig , ax = plot_standings ( show , title = 'Likely Under Control' ); show = mr [ mr . Low . ge ( 1.05 )] . sort_values ( 'Low' ) fig , ax = plot_standings ( show , title = 'Likely Not Under Control' ); ax . get_legend () . remove ()","title":"Standings"}]}