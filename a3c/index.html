<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=description content="A reinforcement learning knowledge base"><meta name=author content="Florian LAURENT"><meta name=lang:clipboard.copy content="Copy to clipboard"><meta name=lang:clipboard.copied content="Copied to clipboard"><meta name=lang:search.language content=en><meta name=lang:search.pipeline.stopwords content=True><meta name=lang:search.pipeline.trimmer content=True><meta name=lang:search.result.none content="No matching documents"><meta name=lang:search.result.one content="1 matching document"><meta name=lang:search.result.other content="# matching documents"><meta name=lang:search.tokenizer content=[\s\-]+><link rel="shortcut icon" href=../assets/images/favicon.png><meta name=generator content="mkdocs-1.1, mkdocs-material-4.6.3"><title>Asynchronous Methods for Deep Reinforcement Learning - RL Insights</title><link rel=stylesheet href=../assets/stylesheets/application.adb8469c.css><link rel=stylesheet href=../assets/stylesheets/application-palette.a8b3c06d.css><meta name=theme-color content=#3f51b5><script src=../assets/javascripts/modernizr.86422ebf.js></script><link href=https://fonts.gstatic.com rel=preconnect crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback"><style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style><link rel=stylesheet href=../assets/fonts/material-icons.css><link rel=stylesheet href=../stylesheets/extra.css><link rel=stylesheet href=../css/ansi-colours.css><link rel=stylesheet href=../css/jupyter-cells.css><link rel=stylesheet href=../css/pandas-dataframe.css><script>
  window.ga = window.ga || function() {
    (ga.q = ga.q || []).push(arguments)
  }
  ga.l = +new Date
  /* Setup integration and send page view */
  ga("create", "UA-36471625-6", "auto")
  ga("set", "anonymizeIp", true)
  ga("send", "pageview")
  /* Register handler to log search on blur */
  document.addEventListener("DOMContentLoaded", () => {
    if (document.forms.search) {
      var query = document.forms.search.query
      query.addEventListener("blur", function() {
        if (this.value) {
          var path = document.location.pathname;
          ga("send", "pageview", path + "?q=" + this.value)
        }
      })
    }
  })
</script><script async src=https://www.google-analytics.com/analytics.js></script></head> <body dir=ltr data-md-color-primary=indigo data-md-color-accent=orange> <svg class=md-svg> <defs> <svg xmlns=http://www.w3.org/2000/svg width=416 height=448 viewbox="0 0 416 448" id=__github><path fill=currentColor d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg> </defs> </svg> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay data-md-component=overlay for=__drawer></label> <a href=#asynchronous-methods-for-deep-reinforcement-learning tabindex=0 class=md-skip> Skip to content </a> <header class=md-header data-md-component=header> <nav class="md-header-nav md-grid"> <div class=md-flex> <div class="md-flex__cell md-flex__cell--shrink"> <a href=.. title="RL Insights" aria-label="RL Insights" class="md-header-nav__button md-logo"> <i class=md-icon></i> </a> </div> <div class="md-flex__cell md-flex__cell--shrink"> <label class="md-icon md-icon--menu md-header-nav__button" for=__drawer></label> </div> <div class="md-flex__cell md-flex__cell--stretch"> <div class="md-flex__ellipsis md-header-nav__title" data-md-component=title> <span class=md-header-nav__topic> RL Insights </span> <span class=md-header-nav__topic> Asynchronous Methods for Deep Reinforcement Learning </span> </div> </div> <div class="md-flex__cell md-flex__cell--shrink"> <label class="md-icon md-icon--search md-header-nav__button" for=__search></label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input aria-label=search name=query placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=query data-md-state=active> <label class="md-icon md-search__icon" for=__search></label> <button type=reset class="md-icon md-search__icon" data-md-component=reset tabindex=-1> &#xE5CD; </button> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=result> <div class=md-search-result__meta> Type to start searching </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> </div> <div class="md-flex__cell md-flex__cell--shrink"> <div class=md-header-nav__source> <a href=https://github.com/MasterScrat/rl-insights title="Go to repository" class=md-source data-md-source=github> <div class=md-source__icon> <svg viewbox="0 0 24 24" width=24 height=24> <use xlink:href=#__github width=24 height=24></use> </svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> </div> </div> </nav> </header> <div class=md-container> <main class=md-main role=main> <div class="md-main__inner md-grid" data-md-component=container> <div class="md-sidebar md-sidebar--primary" data-md-component=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" data-md-level=0> <label class="md-nav__title md-nav__title--site" for=__drawer> <a href=.. title="RL Insights" class="md-nav__button md-logo"> <i class=md-icon></i> </a> RL Insights </label> <div class=md-nav__source> <a href=https://github.com/MasterScrat/rl-insights title="Go to repository" class=md-source data-md-source=github> <div class=md-source__icon> <svg viewbox="0 0 24 24" width=24 height=24> <use xlink:href=#__github width=24 height=24></use> </svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=.. title=Home class=md-nav__link> Home </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-2 type=checkbox id=nav-2 checked> <label class=md-nav__link for=nav-2> Papers </label> <nav class=md-nav data-md-component=collapsible data-md-level=1> <label class=md-nav__title for=nav-2> Papers </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../curl/ title="CURL: Contrastive Unsupervised Representations for Reinforcement Learning" class=md-nav__link> CURL: Contrastive Unsupervised Representations for Reinforcement Learning </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-toggle md-nav__toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> Asynchronous Methods for Deep Reinforcement Learning </label> <a href=./ title="Asynchronous Methods for Deep Reinforcement Learning" class="md-nav__link md-nav__link--active"> Asynchronous Methods for Deep Reinforcement Learning </a> <nav class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc>Table of contents</label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#scaling-up-rl class=md-nav__link> Scaling up RL </a> </li> <li class=md-nav__item> <a href=#decorrelating-experiences class=md-nav__link> Decorrelating experiences </a> </li> <li class=md-nav__item> <a href=#implementation-details class=md-nav__link> Implementation details </a> </li> <li class=md-nav__item> <a href=#evaluation class=md-nav__link> Evaluation </a> </li> <li class=md-nav__item> <a href=#extra-improvements class=md-nav__link> Extra improvements </a> </li> <li class=md-nav__item> <a href=#from-a3c-to-a2c class=md-nav__link> From A3C to A2C </a> </li> <li class=md-nav__item> <a href=#in-hindsight class=md-nav__link> In hindsight </a> </li> <li class=md-nav__item> <a href=#key-concepts class=md-nav__link> Key concepts </a> </li> <li class=md-nav__item> <a href=#links class=md-nav__link> Links </a> </li> <li class=md-nav__item> <a href=#authors class=md-nav__link> Authors </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-3 type=checkbox id=nav-3> <label class=md-nav__link for=nav-3> Notebooks </label> <nav class=md-nav data-md-component=collapsible data-md-level=1> <label class=md-nav__title for=nav-3> Notebooks </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../notebooks/RL_Research_Workflow/ title="Test notebook 1" class=md-nav__link> Test notebook 1 </a> </li> <li class=md-nav__item> <a href=../notebooks/Title_of_the_notebook/ title="Test notebook 2" class=md-nav__link> Test notebook 2 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-4 type=checkbox id=nav-4> <label class=md-nav__link for=nav-4> Blog </label> <nav class=md-nav data-md-component=collapsible data-md-level=1> <label class=md-nav__title for=nav-4> Blog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../notebook-pitfalls/ title="Notebook Pitfalls" class=md-nav__link> Notebook Pitfalls </a> </li> <li class=md-nav__item> <a href=../notebooks-long-tasks/ title="Running Long Tasks in Notebooks" class=md-nav__link> Running Long Tasks in Notebooks </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc>Table of contents</label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#scaling-up-rl class=md-nav__link> Scaling up RL </a> </li> <li class=md-nav__item> <a href=#decorrelating-experiences class=md-nav__link> Decorrelating experiences </a> </li> <li class=md-nav__item> <a href=#implementation-details class=md-nav__link> Implementation details </a> </li> <li class=md-nav__item> <a href=#evaluation class=md-nav__link> Evaluation </a> </li> <li class=md-nav__item> <a href=#extra-improvements class=md-nav__link> Extra improvements </a> </li> <li class=md-nav__item> <a href=#from-a3c-to-a2c class=md-nav__link> From A3C to A2C </a> </li> <li class=md-nav__item> <a href=#in-hindsight class=md-nav__link> In hindsight </a> </li> <li class=md-nav__item> <a href=#key-concepts class=md-nav__link> Key concepts </a> </li> <li class=md-nav__item> <a href=#links class=md-nav__link> Links </a> </li> <li class=md-nav__item> <a href=#authors class=md-nav__link> Authors </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content> <article class="md-content__inner md-typeset"> <h1 id=asynchronous-methods-for-deep-reinforcement-learning>Asynchronous Methods for Deep Reinforcement Learning<a class=headerlink href=#asynchronous-methods-for-deep-reinforcement-learning title="Permanent link">&para;</a></h1> <div class="admonition tldr"> <p class=admonition-title>Tldr</p> <ul> <li>Introduces an RL framework that uses multiple CPU cores to speed up training on a single machine.</li> <li>The main result is A3C, a parallel actor-critic method that uses shared layers between actor and critic, n-step returns and entropy regularization.</li> <li>A synchronous version called A2C, optimized for GPUs, is generally preferred nowadays.<sup id=fnref2:a2c><a class=footnote-ref href=#fn:a2c>1</a></sup></li> </ul> <p><strong>February 2016 - <a href=https://arxiv.org/abs/1602.01783>arXiv</a></strong></p> </div> <h2 id=scaling-up-rl>Scaling up RL<a class=headerlink href=#scaling-up-rl title="Permanent link">&para;</a></h2> <p>This paper introduces a multi-threaded training framework. Its main result is the <strong>A3C algorithm</strong>. When this paper came out, it beat the state of the art on Atari games while training for only half the time! The core idea is to scale up training by using the multiple CPU cores. </p> <p>Scaling up training brings two advantages:</p> <ul> <li>Shorter training times: we can throw more compute at the problem</li> <li>More stable training: it decorrelates the experiences observed by the agent</li> </ul> <h2 id=decorrelating-experiences>Decorrelating experiences<a class=headerlink href=#decorrelating-experiences title="Permanent link">&para;</a></h2> <p>A common problem in reinforcement learning is the correlation between successive experiences. When using off-policy methods such as <a href=dqn.md>DQN</a>, it is possible to use a <a href=../experience-replay/ >replay buffer</a> to store previous experiences that can then be sampled randomly. This is more complicated when using on-policy algorithms such as <a href=../policy-gradient-methods/ >policy gradient methods</a>, as they can't reuse past experiences.</p> <p>The proposed framework solves this problem by executing multiple agents in parallel:</p> <blockquote> <p>Instead of experience replay, we asynchronously execute multiple agents in parallel, on multiple instances of the environment. This parallelism decorrelates the agents’ data into a more stationary process, since at any given timestep the parallel agents will be experiencing a variety of different states.</p> </blockquote> <p>For off-policy methods, this is a memory vs compute trade-off:</p> <ul> <li>Replay buffers are memory-intensive, but use little computing power</li> <li>Parallel agents are CPU-intensive, but are not as memory-hungry as replay buffers</li> </ul> <p>Note that it would be possible to combine a replay buffer - either central or separate per worker - with parallel agents. Going in this direction leads us to architectures such as Gorila or Ape-X.</p> <p>For on-policy methods, there is no downside to using parallel agents, as these methods can't learn from previous experiences! (Following this publication other algorithms came up to address this limitation such as <a href=../acer/ >ACER</a>, <a href=impala.md>IMPALA</a> and others.)</p> <h2 id=implementation-details>Implementation details<a class=headerlink href=#implementation-details title="Permanent link">&para;</a></h2> <p>One of the goals of this framework is to take advantage of multi-cores CPUs without relying on GPUs:</p> <blockquote> <p>Our parallel reinforcement learning paradigm also offers practical benefits. Whereas previous approaches to deep reinforcement learning rely heavily on specialized hardware such as GPUs or massively distributed architectures, our experiments run on a single machine with a standard multi-core CPU.</p> </blockquote> <p>Since the gradients are calculated on the CPU, there's no need to batch large amount of data to optimize performance, as would be the case with a GPU. As a result, the updates are performed asynchronously: each agent performs a number of training steps, then updates the global parameters without having to care about the status of the other agents.</p> <p>Here's the algorithm as pseudocode: <sup id=fnref:original><a class=footnote-ref href=#fn:original>2</a></sup></p> <p><img alt="a3c algo" src=../img/a3c_algo.png></p> <h2 id=evaluation>Evaluation<a class=headerlink href=#evaluation title="Permanent link">&para;</a></h2> <p>The training framework proposed in this paper could be used with any RL methods. In order to find which method works best, they try it out with SARSA, deep Q-learning, n-step deep Q-learning, and advantage actor-critic.</p> <p>This evaluation is performed on four different platforms:</p> <ul> <li>The traditional <a href=../atari/ >Atari</a> learning environment, using 5 games</li> <li>The <a href=http://torcs.sourceforge.net/ >Torcs</a> driving simulator, using 4 game modes</li> <li>The <a href=../mujoco/ >Mujoco</a> domain, using 14 tasks</li> <li>A custom "labyrinth" environment (released afterward as <a href=../dm-lab/ >DeepMind Lab</a>)</li> </ul> <p>Using the parallel framework almost always increases training speed. The results are especially impressive when combining it with the advantage actor-critic method. See for example the last line of these benchmarks on 3 Atari games: <sup id=fnref2:original><a class=footnote-ref href=#fn:original>2</a></sup></p> <p><img alt="atari evaluation" src=../img/a3c_evaluation.png></p> <p>This method is called <strong>A3C</strong>, for "Asynchronous Advantage Actor Critic" - this paper's claim to fame!</p> <p>The paper then provide an evaluation of A3C on 57 Atari games compared to the other top RL methods of the time. Looking at mean performances, A3C beats the state of the art while training twice faster than its competition: <sup id=fnref3:original><a class=footnote-ref href=#fn:original>2</a></sup></p> <p><img alt="atari evaluation" src=../img/a3c_final_results.png></p> <p>It should be noted that the question of <a href=../how-to-benchmark/ >evaluation of RL agents</a>, and more specifically evaluation when using Atari games, has been hotly debated since this paper came out. The use of mean performance for example is no longer regarded as reliable as some games can easily skew the final ranking.</p> <h2 id=extra-improvements>Extra improvements<a class=headerlink href=#extra-improvements title="Permanent link">&para;</a></h2> <p>Beyond its parallel nature, A3C includes a number of other improvements compared to vanilla <a href=../actor-critic/ >advantage actor critic</a>:</p> <ul> <li>The layers are shared between the actor and the critic</li> <li>Both actor and critic use n-step learning</li> <li>The policy uses entropy regularization</li> <li>Adding an LSTM layer helps in some environments</li> <li>The RMSProp optimizer uses shared statistics between agents </li> </ul> <h2 id=from-a3c-to-a2c>From A3C to A2C<a class=headerlink href=#from-a3c-to-a2c title="Permanent link">&para;</a></h2> <p>A3C was explicitly designed to take advantage of CPUs with many cores, and to not require a GPU. With this use case in mind, asynchronous updates made a lot of sense: they don't require any synchronisation between the agents, and parameter updates can be done <a href=../hogwild/ >Hogwild syle</a> without locking. </p> <p>But GPUs are much more efficient then CPUs when optimizing large policies. When the policy becomes large enough, it starts making more sense to sacrifice asynchronous updates in order to feed large batches to a GPU. In that case, the agents don't compute the gradients themselves anymore. Instead, they perform a rollout and send the experiences to the master thread, which performs gradient updates in a synchronous way. </p> <p>This non-asynchronous version is simply called <strong>A2C</strong> (Advantage Actor Critic) <sup id=fnref:a2c><a class=footnote-ref href=#fn:a2c>1</a></sup>. It generally performs comparably to A3C in terms of final performance, but it trains even faster as it can take advantage of a GPU. The paper <a href=../accelerated-methods/ >Accelerated Methods for Deep Reinforcement Learning</a> provides advanced analysis between the two as well as optimization insights. </p> <h2 id=in-hindsight>In hindsight<a class=headerlink href=#in-hindsight title="Permanent link">&para;</a></h2> <ul> <li><img alt=✅ class=twemoji src=https://twemoji.maxcdn.com/v/latest/svg/2705.svg title=:white_check_mark:> Executing multiple agents in parallel is a good way to decorrelate observations</li> <li><img alt=❌ class=twemoji src=https://twemoji.maxcdn.com/v/latest/svg/274c.svg title=:x:> Asynchronous updates are not a good match for GPU training: see <a href=#from-a3c-to-a2c>A2C</a></li> <li><img alt=❌ class=twemoji src=https://twemoji.maxcdn.com/v/latest/svg/274c.svg title=:x:> Using exclusively on-policy data is bad for sample efficiency: see <a href=../acer/ >ACER</a>, <a href=impala.md>IMPALA</a></li> </ul> <h2 id=key-concepts>Key concepts<a class=headerlink href=#key-concepts title="Permanent link">&para;</a></h2> <p>"Building blocks" involved in this work:</p> <ul> <li>Builds on <a href=../actor-critic/ >Actor-Critic</a> methods</li> <li>Introduces a multi-threaded <a href=parallel-training.md>parallel training</a> method</li> <li>Uses <a href=entropy-regularization.md>entropy regularization</a></li> <li>Uses an LSTM <a href=rnn.md>Recurrent Neural Network</a> layer</li> <li>Uses <a href=n-step-return.md>multi-step return</a></li> <li>Uses <a href=shared-layers.md>shared layers</a> between policy and value functions</li> <li>Benchmarked on <a href=../atari/ >Atari games</a></li> </ul> <h2 id=links>Links<a class=headerlink href=#links title="Permanent link">&para;</a></h2> <ul> <li><a href=https://danieltakeshi.github.io/2018/06/28/a2c-a3c/ >Actor-Critic Methods: A3C and A2C</a> from Daniel Seita, a detailed walkthrough</li> </ul> <h2 id=authors>Authors<a class=headerlink href=#authors title="Permanent link">&para;</a></h2> <ul> <li>Volodymyr Mnih <a href=https://twitter.com/vladmnih>Twitter</a>/<a href="https://scholar.google.com/citations?user=rLdfJ1gAAAAJ">Scholar</a>/<a href=https://www.cs.toronto.edu/~vmnih/ >Academic</a></li> <li>Adrià Puigdomènech Badia <a href="https://scholar.google.com/citations?user=DcWRJW4AAAAJ">Scholar</a></li> <li>Mehdi Mirza <a href=https://twitter.com/memimo>Twitter</a>/<a href="https://scholar.google.com/citations?user=c646VbAAAAAJ">Scholar</a>/<a href=https://memimo.net/ >Personal</a></li> <li>Alex Graves <a href="https://scholar.google.com/citations?user=DaFHynwAAAAJ">Scholar</a>/<a href=https://www.cs.toronto.edu/~graves/ >Academic</a></li> <li>Tim Harley <a href=https://twitter.com/_timharley>Twitter</a>/<a href="https://scholar.google.com/citations?user=yxwU5CMAAAAJ">Scholar</a></li> <li>Timothy P. Lillicrap <a href=https://twitter.com/countzerozzz>Twitter</a>/<a href="https://scholar.google.com/citations?user=htPVdRMAAAAJ">Scholar</a>/<a href=http://contrastiveconvergence.net/~timothylillicrap>Personal</a></li> <li>David Silver <a href="https://scholar.google.com/citations?user=-8DNE4UAAAAJ">Scholar</a>/<a href=https://www.davidsilver.uk/ >Personal</a></li> <li>Koray Kavukcuoglu <a href=https://twitter.com/koraykv>Twitter</a>/<a href="https://scholar.google.com/citations?user=sGFyDIUAAAAJ">Scholar</a>/<a href=https://koray.kavukcuoglu.org/ >Personal</a></li> </ul> <div class=footnote> <hr> <ol> <li id=fn:a2c> <p><a href=https://openai.com/blog/baselines-acktr-a2c/ >OpenAI Baselines: ACKTR &amp; A2C</a>&#160;<a class=footnote-backref href=#fnref:a2c title="Jump back to footnote 1 in the text">&#8617;</a><a class=footnote-backref href=#fnref2:a2c title="Jump back to footnote 1 in the text">&#8617;</a></p> </li> <li id=fn:original> <p>excerpts from original paper&#160;<a class=footnote-backref href=#fnref:original title="Jump back to footnote 2 in the text">&#8617;</a><a class=footnote-backref href=#fnref2:original title="Jump back to footnote 2 in the text">&#8617;</a><a class=footnote-backref href=#fnref3:original title="Jump back to footnote 2 in the text">&#8617;</a></p> </li> </ol> </div> <hr> <div class=md-source-date> <small> Last update: April 14, 2020 </small> </div> </article> </div> </div> </main> <footer class=md-footer> <div class=md-footer-nav> <nav class="md-footer-nav__inner md-grid"> <a href=../curl/ title="CURL: Contrastive Unsupervised Representations for Reinforcement Learning" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel=prev> <div class="md-flex__cell md-flex__cell--shrink"> <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i> </div> <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"> <span class=md-flex__ellipsis> <span class=md-footer-nav__direction> Previous </span> CURL: Contrastive Unsupervised Representations for Reinforcement Learning </span> </div> </a> <a href=../notebooks/RL_Research_Workflow/ title="Test notebook 1" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel=next> <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"> <span class=md-flex__ellipsis> <span class=md-footer-nav__direction> Next </span> Test notebook 1 </span> </div> <div class="md-flex__cell md-flex__cell--shrink"> <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i> </div> </a> </nav> </div> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> <div class=md-footer-copyright__highlight> Copyright &copy; 2020 Florian Laurent </div> powered by <a href=https://www.mkdocs.org target=_blank rel=noopener>MkDocs</a> and <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs</a> </div> </div> </div> </footer> </div> <script src=../assets/javascripts/application.c33a9706.js></script> <script>app.initialize({version:"1.1",url:{base:".."}})</script> <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script> <script src=../js/extra.js></script> </body> </html>