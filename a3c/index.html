<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=description content="Reinforcement learning resources"><meta name=author content="Florian LAURENT"><meta name=lang:clipboard.copy content="Copy to clipboard"><meta name=lang:clipboard.copied content="Copied to clipboard"><meta name=lang:search.language content=en><meta name=lang:search.pipeline.stopwords content=True><meta name=lang:search.pipeline.trimmer content=True><meta name=lang:search.result.none content="No matching documents"><meta name=lang:search.result.one content="1 matching document"><meta name=lang:search.result.other content="# matching documents"><meta name=lang:search.tokenizer content=[\s\-]+><link rel="shortcut icon" href=../assets/images/favicon.png><meta name=generator content="mkdocs-1.1, mkdocs-material-4.6.3"><title>Asynchronous Methods for Deep Reinforcement Learning - RL Insights</title><link rel=stylesheet href=../assets/stylesheets/application.adb8469c.css><link rel=stylesheet href=../assets/stylesheets/application-palette.a8b3c06d.css><meta name=theme-color content=#3f51b5><script src=../assets/javascripts/modernizr.86422ebf.js></script><link href=https://fonts.gstatic.com rel=preconnect crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback"><style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style><link rel=stylesheet href=../assets/fonts/material-icons.css><link rel=stylesheet href=../stylesheets/extra.css><script>
  window.ga = window.ga || function() {
    (ga.q = ga.q || []).push(arguments)
  }
  ga.l = +new Date
  /* Setup integration and send page view */
  ga("create", "UA-36471625-6", "auto")
  ga("set", "anonymizeIp", true)
  ga("send", "pageview")
  /* Register handler to log search on blur */
  document.addEventListener("DOMContentLoaded", () => {
    if (document.forms.search) {
      var query = document.forms.search.query
      query.addEventListener("blur", function() {
        if (this.value) {
          var path = document.location.pathname;
          ga("send", "pageview", path + "?q=" + this.value)
        }
      })
    }
  })
</script><script async src=https://www.google-analytics.com/analytics.js></script></head> <body dir=ltr data-md-color-primary=indigo data-md-color-accent=orange> <svg class=md-svg> <defs> <svg xmlns=http://www.w3.org/2000/svg width=416 height=448 viewbox="0 0 416 448" id=__github><path fill=currentColor d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg> </defs> </svg> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay data-md-component=overlay for=__drawer></label> <a href=#asynchronous-methods-for-deep-reinforcement-learning tabindex=0 class=md-skip> Skip to content </a> <header class=md-header data-md-component=header> <nav class="md-header-nav md-grid"> <div class=md-flex> <div class="md-flex__cell md-flex__cell--shrink"> <a href=.. title="RL Insights" aria-label="RL Insights" class="md-header-nav__button md-logo"> <i class=md-icon></i> </a> </div> <div class="md-flex__cell md-flex__cell--shrink"> <label class="md-icon md-icon--menu md-header-nav__button" for=__drawer></label> </div> <div class="md-flex__cell md-flex__cell--stretch"> <div class="md-flex__ellipsis md-header-nav__title" data-md-component=title> <span class=md-header-nav__topic> RL Insights </span> <span class=md-header-nav__topic> Asynchronous Methods for Deep Reinforcement Learning </span> </div> </div> <div class="md-flex__cell md-flex__cell--shrink"> <label class="md-icon md-icon--search md-header-nav__button" for=__search></label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input aria-label=search name=query placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=query data-md-state=active> <label class="md-icon md-search__icon" for=__search></label> <button type=reset class="md-icon md-search__icon" data-md-component=reset tabindex=-1> &#xE5CD; </button> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=result> <div class=md-search-result__meta> Type to start searching </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> </div> <div class="md-flex__cell md-flex__cell--shrink"> <div class=md-header-nav__source> <a href=https://github.com/MasterScrat/rl-insights title="Go to repository" class=md-source data-md-source=github> <div class=md-source__icon> <svg viewbox="0 0 24 24" width=24 height=24> <use xlink:href=#__github width=24 height=24></use> </svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> </div> </div> </nav> </header> <div class=md-container> <main class=md-main role=main> <div class="md-main__inner md-grid" data-md-component=container> <div class="md-sidebar md-sidebar--primary" data-md-component=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" data-md-level=0> <label class="md-nav__title md-nav__title--site" for=__drawer> <a href=.. title="RL Insights" class="md-nav__button md-logo"> <i class=md-icon></i> </a> RL Insights </label> <div class=md-nav__source> <a href=https://github.com/MasterScrat/rl-insights title="Go to repository" class=md-source data-md-source=github> <div class=md-source__icon> <svg viewbox="0 0 24 24" width=24 height=24> <use xlink:href=#__github width=24 height=24></use> </svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=.. title=Home class=md-nav__link> Home </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-2 type=checkbox id=nav-2 checked> <label class=md-nav__link for=nav-2> Papers </label> <nav class=md-nav data-md-component=collapsible data-md-level=1> <label class=md-nav__title for=nav-2> Papers </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../curl/ title="CURL: Contrastive Unsupervised Representations for Reinforcement Learning" class=md-nav__link> CURL: Contrastive Unsupervised Representations for Reinforcement Learning </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-toggle md-nav__toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> Asynchronous Methods for Deep Reinforcement Learning </label> <a href=./ title="Asynchronous Methods for Deep Reinforcement Learning" class="md-nav__link md-nav__link--active"> Asynchronous Methods for Deep Reinforcement Learning </a> <nav class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc>Table of contents</label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#learning-faster class=md-nav__link> Learning faster </a> </li> <li class=md-nav__item> <a href=#evaluation class=md-nav__link> Evaluation </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#summary class=md-nav__link> Summary </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#in-hindsight class=md-nav__link> In hindsight </a> </li> <li class=md-nav__item> <a href=#key-concepts class=md-nav__link> Key concepts </a> </li> <li class=md-nav__item> <a href=#links class=md-nav__link> Links </a> </li> <li class=md-nav__item> <a href=#authors class=md-nav__link> Authors </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc>Table of contents</label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#learning-faster class=md-nav__link> Learning faster </a> </li> <li class=md-nav__item> <a href=#evaluation class=md-nav__link> Evaluation </a> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a href=#summary class=md-nav__link> Summary </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#in-hindsight class=md-nav__link> In hindsight </a> </li> <li class=md-nav__item> <a href=#key-concepts class=md-nav__link> Key concepts </a> </li> <li class=md-nav__item> <a href=#links class=md-nav__link> Links </a> </li> <li class=md-nav__item> <a href=#authors class=md-nav__link> Authors </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content> <article class="md-content__inner md-typeset"> <h1 id=asynchronous-methods-for-deep-reinforcement-learning>Asynchronous Methods for Deep Reinforcement Learning<a class=headerlink href=#asynchronous-methods-for-deep-reinforcement-learning title="Permanent link">&para;</a></h1> <div class="admonition warning"> <p class=admonition-title>Warning</p> <p>Under construction! This page is still incomplete.</p> </div> <div class="admonition tldr"> <p class=admonition-title>Tldr</p> <ul> <li>Introduces a parallel training framework that uses multiple CPU cores to speed up training on a single machine.</li> <li>The main result is A3C, a parallel actor-critic method that uses an LSTM layer, n-step returns and entropy regularization.</li> <li>Parallel training with DQN is also investigated, but results are less convincing.</li> <li>A non-asynchronous version called A2C, geared towards GPUs, is generally preferred nowadays.<sup id=fnref:a2c><a class=footnote-ref href=#fn:a2c>1</a></sup></li> </ul> <p><strong>February 2016 - <a href=https://arxiv.org/abs/1602.01783>arXiv</a></strong></p> </div> <h2 id=learning-faster>Learning faster<a class=headerlink href=#learning-faster title="Permanent link">&para;</a></h2> <p>Let's start with some context. When this paper comes out in February 2016, DQN had already shown the promise of "deep" reinforcement learning, with extensions such as <a href=double-dqn.md>Double DQN</a> and <a href=per.md>Prioritized Experience Replay</a> already published - but the Rainbow agent that would combine them all was still a year away. On the policy gradient front, <a href=trpo.md>TPRO</a> was out, but not yet <a href=../ppo/ >PPO</a>.</p> <p>~ The concerns of learning in a stable and efficient way were even more at the forefront than right now.</p> <p>The replay buffer is an efficient solution against observation non-stationarity. But: - uses memory (yes) - can't be used for off-po methods! (YES) </p> <p>To address these concerns, instead of using a replay buffer, this method executes multiple agents in parallel: - no need </p> <p>There are multiple ways to consider efficiency of RL training: - sample efficiency - time efficiency</p> <p>This method, as presented, has very low sample efficiency (each observation is used only once, or a few times due to n-step learning) but can be scaled up easily. Note that for off-policy method, it could very well be combined with a replay buffer!</p> <p>A stated goal of this approach is to rely only on CPU. This allows Howgwild-style updates. However, a different method called A2C, which does leverage GPUs, is preferred today.</p> <p>The asynchronous RL framework </p> <h2 id=evaluation>Evaluation<a class=headerlink href=#evaluation title="Permanent link">&para;</a></h2> <p>Evaluation is performed in multiple environments, using four different platforms:</p> <ul> <li><a href=../atari/ >Atari</a></li> <li>Torcs (http://www.cse.chalmers.se/~chrdimi/papers/torcs.pdf, http://torcs.sourceforge.net/)</li> <li><a href=../mujoco/ >Mujoco</a> (14 envs, figure S7)</li> <li>Custom "labyrinth" environment, which will subsequently be released as <a href=../dm-lab/ >DeepMind Lab</a></li> </ul> <p>This RL framework is evaluated against DQN, Double DQN, Dueling DQN, Prioritized DQN and the distributed training system Gorila, which itself relies on DQN.</p> <h3 id=summary>Summary<a class=headerlink href=#summary title="Permanent link">&para;</a></h3> <p>This paper investigates a new parallel training method which uses multiple CPU cores on the same machine, rather than relying on GPUs. Both value-based and policy-based methods are considered. </p> <p>The major result is the performance of this training scheme using an <a href=../actor-critic/ >actor-critic method</a>.</p> <p>This training scheme also shows promising results for value-based methods. However, these methods being off-policy, they can be significantly improved with the use of a replay buffer. </p> <p>The core idea is to use multiple asynchronous "actor-learners" running in different threads. Each actor-learner performs a number of learning steps, then perform an asynchronous update of the global parameters using accumulated gradients. </p> <p>Since this method is designed to train networks using a CPU, asynchronous updates made a lot of sense as it enabled <a href=hogwild.md>Hogwild syle</a> updates. However, it appeared later on that this was not optimal when using a GPU. For this reason, this method is commonly used today in a synchronous way in which the updates from all the actor-learners are batched to make better use of the GPU. This non-asynchronous version is simply called <strong>A2C</strong> (Advantage Actor Critic) [^openai-acktr-a2c].</p> <h2 id=in-hindsight>In hindsight<a class=headerlink href=#in-hindsight title="Permanent link">&para;</a></h2> <ul> <li>✅ Using multiple environments in parallel is a good way to decorrelate the agent's data</li> <li>❌ Asynchronous updates are not a good match for GPU training</li> <li>❌ Using exclusively on-policy data is very bad for sample efficiency</li> </ul> <h2 id=key-concepts>Key concepts<a class=headerlink href=#key-concepts title="Permanent link">&para;</a></h2> <p>"Building blocks" involved in this work:</p> <ul> <li>Builds on <a href=gorila.md>Gorila</a> distributed training system</li> <li>Builds on <a href=dqn.md>DQN</a> and <a href=../actor-critic/ >Actor-Critic</a> methods</li> <li>Introduces a single-machine <a href=parallel-training.md>parallel training</a> method</li> <li>Uses <a href=entropy-regularization.md>entropy regularization</a></li> <li>Uses an LSTM <a href=rnn.md>Recurrent Neural Network</a> layer</li> <li>Uses <a href=n-step-return.md>multi-step return</a> with the forward view</li> <li>Uses <a href=shared-layers.md>shared layers</a> between policy and value functions</li> <li>Benchmarked on <a href=../atari/ >Atari games</a></li> </ul> <h2 id=links>Links<a class=headerlink href=#links title="Permanent link">&para;</a></h2> <ul> <li><a href=https://danieltakeshi.github.io/2018/06/28/a2c-a3c/ >Actor-Critic Methods: A3C and A2C</a> from Daniel Seita</li> </ul> <h2 id=authors>Authors<a class=headerlink href=#authors title="Permanent link">&para;</a></h2> <ul> <li>Volodymyr Mnih <a href=https://twitter.com/vladmnih>Twitter</a>/<a href="https://scholar.google.com/citations?user=rLdfJ1gAAAAJ">Scholar</a>/<a href=https://www.cs.toronto.edu/~vmnih/ >Academic</a></li> </ul> <p>TODO!</p> <ul> <li>Adrià Puigdomènech Badia <a href=https://twitter.com/vladmnih>Twitter</a>/<a href="https://scholar.google.com/citations?user=rLdfJ1gAAAAJ">Scholar</a>/<a href=https://www.cs.toronto.edu/~vmnih/ >Academic</a></li> <li>Mehdi Mirza <a href=https://twitter.com/vladmnih>Twitter</a>/<a href="https://scholar.google.com/citations?user=rLdfJ1gAAAAJ">Scholar</a>/<a href=https://www.cs.toronto.edu/~vmnih/ >Academic</a></li> <li>Alex Graves <a href=https://twitter.com/vladmnih>Twitter</a>/<a href="https://scholar.google.com/citations?user=rLdfJ1gAAAAJ">Scholar</a>/<a href=https://www.cs.toronto.edu/~vmnih/ >Academic</a></li> <li>Tim Harley <a href=https://twitter.com/vladmnih>Twitter</a>/<a href="https://scholar.google.com/citations?user=rLdfJ1gAAAAJ">Scholar</a>/<a href=https://www.cs.toronto.edu/~vmnih/ >Academic</a></li> <li>Timothy P. Lillicrap <a href=https://twitter.com/vladmnih>Twitter</a>/<a href="https://scholar.google.com/citations?user=rLdfJ1gAAAAJ">Scholar</a>/<a href=https://www.cs.toronto.edu/~vmnih/ >Academic</a></li> <li>David Silver <a href=https://twitter.com/vladmnih>Twitter</a>/<a href="https://scholar.google.com/citations?user=rLdfJ1gAAAAJ">Scholar</a>/<a href=https://www.cs.toronto.edu/~vmnih/ >Academic</a></li> <li>Koray Kavukcuoglu <a href=https://twitter.com/vladmnih>Twitter</a>/<a href="https://scholar.google.com/citations?user=rLdfJ1gAAAAJ">Scholar</a>/<a href=https://www.cs.toronto.edu/~vmnih/ >Academic</a></li> </ul> <div class=footnote> <hr> <ol> <li id=fn:a2c> <p><a href=https://openai.com/blog/baselines-acktr-a2c/ >OpenAI Baselines: ACKTR &amp; A2C</a>&#160;<a class=footnote-backref href=#fnref:a2c title="Jump back to footnote 1 in the text">&#8617;</a></p> </li> </ol> </div> <hr> <div class=md-source-date> <small> Last update: April 11, 2020 </small> </div> </article> </div> </div> </main> <footer class=md-footer> <div class=md-footer-nav> <nav class="md-footer-nav__inner md-grid"> <a href=../curl/ title="CURL: Contrastive Unsupervised Representations for Reinforcement Learning" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel=prev> <div class="md-flex__cell md-flex__cell--shrink"> <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i> </div> <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"> <span class=md-flex__ellipsis> <span class=md-footer-nav__direction> Previous </span> CURL: Contrastive Unsupervised Representations for Reinforcement Learning </span> </div> </a> </nav> </div> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> <div class=md-footer-copyright__highlight> Copyright &copy; 2020 Florian Laurent </div> powered by <a href=https://www.mkdocs.org target=_blank rel=noopener>MkDocs</a> and <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs</a> </div> </div> </div> </footer> </div> <script src=../assets/javascripts/application.c33a9706.js></script> <script>app.initialize({version:"1.1",url:{base:".."}})</script> </body> </html>