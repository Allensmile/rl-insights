<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=description content="A reinforcement learning knowledge base"><meta name=author content="Florian LAURENT"><meta name=lang:clipboard.copy content="Copy to clipboard"><meta name=lang:clipboard.copied content="Copied to clipboard"><meta name=lang:search.language content=en><meta name=lang:search.pipeline.stopwords content=True><meta name=lang:search.pipeline.trimmer content=True><meta name=lang:search.result.none content="No matching documents"><meta name=lang:search.result.one content="1 matching document"><meta name=lang:search.result.other content="# matching documents"><meta name=lang:search.tokenizer content=[\s\-]+><link rel="shortcut icon" href=../assets/images/favicon.png><meta name=generator content="mkdocs-1.1, mkdocs-material-4.6.3"><title>CURL: Contrastive Unsupervised Representations for Reinforcement Learning - RL Insights</title><link rel=stylesheet href=../assets/stylesheets/application.adb8469c.css><link rel=stylesheet href=../assets/stylesheets/application-palette.a8b3c06d.css><meta name=theme-color content=#3f51b5><script src=../assets/javascripts/modernizr.86422ebf.js></script><link href=https://fonts.gstatic.com rel=preconnect crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback"><style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style><link rel=stylesheet href=../assets/fonts/material-icons.css><link rel=stylesheet href=../stylesheets/extra.css><link rel=stylesheet href=../css/ansi-colours.css><link rel=stylesheet href=../css/jupyter-cells.css><link rel=stylesheet href=../css/pandas-dataframe.css><script>
  window.ga = window.ga || function() {
    (ga.q = ga.q || []).push(arguments)
  }
  ga.l = +new Date
  /* Setup integration and send page view */
  ga("create", "UA-36471625-6", "auto")
  ga("set", "anonymizeIp", true)
  ga("send", "pageview")
  /* Register handler to log search on blur */
  document.addEventListener("DOMContentLoaded", () => {
    if (document.forms.search) {
      var query = document.forms.search.query
      query.addEventListener("blur", function() {
        if (this.value) {
          var path = document.location.pathname;
          ga("send", "pageview", path + "?q=" + this.value)
        }
      })
    }
  })
</script><script async src=https://www.google-analytics.com/analytics.js></script></head> <body dir=ltr data-md-color-primary=indigo data-md-color-accent=orange> <svg class=md-svg> <defs> <svg xmlns=http://www.w3.org/2000/svg width=416 height=448 viewbox="0 0 416 448" id=__github><path fill=currentColor d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg> </defs> </svg> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay data-md-component=overlay for=__drawer></label> <a href=#curl-contrastive-unsupervised-representations-for-reinforcement-learning tabindex=0 class=md-skip> Skip to content </a> <header class=md-header data-md-component=header> <nav class="md-header-nav md-grid"> <div class=md-flex> <div class="md-flex__cell md-flex__cell--shrink"> <a href=.. title="RL Insights" aria-label="RL Insights" class="md-header-nav__button md-logo"> <i class=md-icon></i> </a> </div> <div class="md-flex__cell md-flex__cell--shrink"> <label class="md-icon md-icon--menu md-header-nav__button" for=__drawer></label> </div> <div class="md-flex__cell md-flex__cell--stretch"> <div class="md-flex__ellipsis md-header-nav__title" data-md-component=title> <span class=md-header-nav__topic> RL Insights </span> <span class=md-header-nav__topic> CURL: Contrastive Unsupervised Representations for Reinforcement Learning </span> </div> </div> <div class="md-flex__cell md-flex__cell--shrink"> <label class="md-icon md-icon--search md-header-nav__button" for=__search></label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input aria-label=search name=query placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=query data-md-state=active> <label class="md-icon md-search__icon" for=__search></label> <button type=reset class="md-icon md-search__icon" data-md-component=reset tabindex=-1> &#xE5CD; </button> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=result> <div class=md-search-result__meta> Type to start searching </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> </div> <div class="md-flex__cell md-flex__cell--shrink"> <div class=md-header-nav__source> <a href=https://github.com/MasterScrat/rl-insights title="Go to repository" class=md-source data-md-source=github> <div class=md-source__icon> <svg viewbox="0 0 24 24" width=24 height=24> <use xlink:href=#__github width=24 height=24></use> </svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> </div> </div> </nav> </header> <div class=md-container> <main class=md-main role=main> <div class="md-main__inner md-grid" data-md-component=container> <div class="md-sidebar md-sidebar--primary" data-md-component=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" data-md-level=0> <label class="md-nav__title md-nav__title--site" for=__drawer> <a href=.. title="RL Insights" class="md-nav__button md-logo"> <i class=md-icon></i> </a> RL Insights </label> <div class=md-nav__source> <a href=https://github.com/MasterScrat/rl-insights title="Go to repository" class=md-source data-md-source=github> <div class=md-source__icon> <svg viewbox="0 0 24 24" width=24 height=24> <use xlink:href=#__github width=24 height=24></use> </svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=.. title=Home class=md-nav__link> Home </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-2 type=checkbox id=nav-2 checked> <label class=md-nav__link for=nav-2> Papers </label> <nav class=md-nav data-md-component=collapsible data-md-level=1> <label class=md-nav__title for=nav-2> Papers </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-toggle md-nav__toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> CURL: Contrastive Unsupervised Representations for Reinforcement Learning </label> <a href=./ title="CURL: Contrastive Unsupervised Representations for Reinforcement Learning" class="md-nav__link md-nav__link--active"> CURL: Contrastive Unsupervised Representations for Reinforcement Learning </a> <nav class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc>Table of contents</label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#learning-from-pixels class=md-nav__link> Learning from pixels </a> </li> <li class=md-nav__item> <a href=#what-is-contrastive-learning class=md-nav__link> What is contrastive learning? </a> </li> <li class=md-nav__item> <a href=#how-curl-works class=md-nav__link> How CURL works </a> </li> <li class=md-nav__item> <a href=#evaluation class=md-nav__link> Evaluation </a> </li> <li class=md-nav__item> <a href=#key-concepts class=md-nav__link> Key concepts </a> </li> <li class=md-nav__item> <a href=#limitations class=md-nav__link> Limitations </a> </li> <li class=md-nav__item> <a href=#links class=md-nav__link> Links </a> </li> <li class=md-nav__item> <a href=#authors class=md-nav__link> Authors </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../a3c/ title="Asynchronous Methods for Deep Reinforcement Learning" class=md-nav__link> Asynchronous Methods for Deep Reinforcement Learning </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-toggle md-nav__toggle" data-md-toggle=nav-3 type=checkbox id=nav-3> <label class=md-nav__link for=nav-3> Notebooks </label> <nav class=md-nav data-md-component=collapsible data-md-level=1> <label class=md-nav__title for=nav-3> Notebooks </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../notebooks/RL_Research_Workflow/ title="Test notebook 1" class=md-nav__link> Test notebook 1 </a> </li> <li class=md-nav__item> <a href=../notebooks/Title_of_the_notebook/ title="Test notebook 2" class=md-nav__link> Test notebook 2 </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc>Table of contents</label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#learning-from-pixels class=md-nav__link> Learning from pixels </a> </li> <li class=md-nav__item> <a href=#what-is-contrastive-learning class=md-nav__link> What is contrastive learning? </a> </li> <li class=md-nav__item> <a href=#how-curl-works class=md-nav__link> How CURL works </a> </li> <li class=md-nav__item> <a href=#evaluation class=md-nav__link> Evaluation </a> </li> <li class=md-nav__item> <a href=#key-concepts class=md-nav__link> Key concepts </a> </li> <li class=md-nav__item> <a href=#limitations class=md-nav__link> Limitations </a> </li> <li class=md-nav__item> <a href=#links class=md-nav__link> Links </a> </li> <li class=md-nav__item> <a href=#authors class=md-nav__link> Authors </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content> <article class="md-content__inner md-typeset"> <h1 id=curl-contrastive-unsupervised-representations-for-reinforcement-learning>CURL: Contrastive Unsupervised Representations for Reinforcement Learning<a class=headerlink href=#curl-contrastive-unsupervised-representations-for-reinforcement-learning title="Permanent link">&para;</a></h1> <div class="admonition tldr"> <p class=admonition-title>Tldr</p> <ul> <li>Improves the sample efficiency when learning from pixels by using contrastive learning, a self-supervised method, as an auxiliary task. </li> <li>The contrastive objective works in a similar way as in SimCLR, using random cropping as an augmentation method.</li> <li>After 100k interactions, outperforms all other methods on DM Control Suite, and shows strong results on Atari. </li> </ul> <p><strong>April 2020 - <a href=https://arxiv.org/abs/2004.04136>arXiv</a> - <a href=https://github.com/MishaLaskin/curl>Code</a></strong></p> </div> <h2 id=learning-from-pixels>Learning from pixels<a class=headerlink href=#learning-from-pixels title="Permanent link">&para;</a></h2> <p>In reinforcement learning, solving a task <em>from pixels</em> is much harder than solving an equivalent task using "physical" features such as coordinates and angles. This makes sense: you can consider an image as a high-dimensional vector containing hundreds of features, which don't have any clear connection with the goal of the environment!</p> <p>As a result, you generally need a lot more interactions between an agent and its environment in order to learn a good policy from pixels. For example, the figure below shows the results of multiple recent RL methods on the DeepMind Control Suite, learning from physical features. Take a look at the x-axis: depending on the complexity of the task, the agents are trained from 1 to 10 million interactions: <sup id=fnref:sac><a class=footnote-ref href=#fn:sac>1</a></sup> </p> <p><img alt="SAC benchmarks" src=../img/curl_sac.png></p> <p>On the other hand, let's have a look at results using Atari games as a benchmark. There is no notion of "physical" features with the Atari emulator: the only observations the agent can work with are RGB images. The agents now have to be trained for a staggering 200 million frames!<sup id=fnref:rainbow><a class=footnote-ref href=#fn:rainbow>2</a></sup> Some distributed approaches even consider numbers up to a billion. </p> <p><img alt="Rainbow benchmarks" src=../img/curl_rainbow.png></p> <p>This low sample-efficiency is clearly a problem. Not only does it mean the experiment turn-around time is excessive, it also means that there can be little hope of bringing such methods to the real world. Can you imagine having to collect a billion real-world interactions?</p> <p>The paper we are considering takes a stab at this problem by bringing recent advances from vision and NLP to reinforcement learning. Contrastive learning takes advantage of data augmentation to learn more efficiently. CURL shows that it can be very useful in the context of RL to learn a good latent representation faster.</p> <h2 id=what-is-contrastive-learning>What is contrastive learning?<a class=headerlink href=#what-is-contrastive-learning title="Permanent link">&para;</a></h2> <p>The core idea is to compare (contrast!) pairs of augmented samples. We consider two kinds of such pairs:</p> <ul> <li><strong>Positive pairs</strong> consist of two different augmentations of the <em>same sample</em> </li> <li><strong>Negative pairs</strong> contain augmentations of two <em>different samples</em></li> </ul> <p>For each original sample, we create both positive and negative pairs. The contrastive representation is then learned by maximizing the agreement between positive pairs, and minimizing the agreement between negative pairs.</p> <p>Contrastive learning has seen dramatic progress in recent years for language and vision. See for example BERT, an application to masked language modeling<sup id=fnref:bert><a class=footnote-ref href=#fn:bert>3</a></sup>, or the SimCLR framework, used to learn visual representations<sup id=fnref:simclr><a class=footnote-ref href=#fn:simclr>4</a></sup>. </p> <p>The way contrastive learning is implemented in CURL is mostly influenced by the SimCLR framework<sup id=fnref2:simclr><a class=footnote-ref href=#fn:simclr>4</a></sup>, Momentum Contrast (MoCo)<sup id=fnref:moco><a class=footnote-ref href=#fn:moco>5</a></sup> and Contrastive Predictive Coding (CPC)<sup id=fnref:cpc><a class=footnote-ref href=#fn:cpc>6</a></sup>.</p> <h2 id=how-curl-works>How CURL works<a class=headerlink href=#how-curl-works title="Permanent link">&para;</a></h2> <p>With CURL, the same latent representation is used for both the RL algorithm and the contrastive learning, as illustrated below: <sup id=fnref:original><a class=footnote-ref href=#fn:original>7</a></sup></p> <p><img alt="CURL diagram" src=../img/curl_diagram.png></p> <p>CURL uses random crops to augment the observations. Since most RL methods use frame-stacking, each observation is effectively a "stack" of sequential images. CURL preserves their temporal structure by applying the same augmentation to each frame in the stack. </p> <p>The illustration below gives an example of a positive pair: the same observation is augmented in two different ways. The representation will be changed in a way that maximizes their agreement. <sup id=fnref2:original><a class=footnote-ref href=#fn:original>7</a></sup></p> <p><img alt="cropping augmentation" src=../img/curl_augment.png></p> <h2 id=evaluation>Evaluation<a class=headerlink href=#evaluation title="Permanent link">&para;</a></h2> <p>Using a contrastive objective as an auxiliary task appears to significantly improve the performance of the RL algorithm. CURL's performance is evaluated in two settings:</p> <ul> <li>with <a href=../sac/ >SAC</a> on DeepMind Control Suite (continuous control)</li> <li>with data-efficient <a href=../rainbow/ >Rainbow DQN</a> on Atari games (discrete control). </li> </ul> <p>In both cases, the performance is evaluated after 100k interactions, as the goal is to evaluate sample efficiency rather than asymptotic performance. </p> <p>Results are remarkable on DeepMind Control Suite : <sup id=fnref3:original><a class=footnote-ref href=#fn:original>7</a></sup> <img alt=dmcs src=../img/curl_dmc.png> (The last column, <em>State SAC</em>, uses physical states and is used as an "oracle" upper-bound.)</p> <p>Results are very good on Atari games. This is again after 100k interactions: <sup id=fnref4:original><a class=footnote-ref href=#fn:original>7</a></sup> <img alt=atari src=../img/curl_atari.png></p> <p>How can this difference in performance be explained? One idea explored in the appendix is that in some environments, there is simply not enough information to fully recover the state when looking only at the pixel data: the problem then becomes <strong>partially observable</strong> and therefore much harder. </p> <h2 id=key-concepts>Key concepts<a class=headerlink href=#key-concepts title="Permanent link">&para;</a></h2> <p>"Building blocks" involved in this work:</p> <ul> <li>Focuses on <a href=../sample-efficiency/ >sample efficiency</a></li> <li>Uses <a href=../contrastive-learning/ >contrastive learning</a></li> <li>Uses <a href=../auxiliary-tasks/ >auxiliary tasks</a></li> <li>Benchmarked on <a href=../dm-control-suite/ >DeepMind Control Suite</a></li> <li>Benchmarked on <a href=../atari/ >Atari games</a></li> </ul> <h2 id=limitations>Limitations<a class=headerlink href=#limitations title="Permanent link">&para;</a></h2> <ul> <li>❌ No comparison with <a href=../muzero/ >MuZero</a>, which is SotA on multiple Atari games<sup id=fnref:muzero-tweet><a class=footnote-ref href=#fn:muzero-tweet>8</a></sup></li> </ul> <h2 id=links>Links<a class=headerlink href=#links title="Permanent link">&para;</a></h2> <ul> <li><a href=https://github.com/MishaLaskin/curl>Official code repository</a>, a PyTorch implementation for SAC on DeepMind Control Suite</li> <li><a href=https://mishalaskin.github.io/curl/ >Official project page</a>, which provides a short summary of the paper</li> <li><a href=https://twitter.com/Aravind7694/status/1248049713149906945>Twitter summary</a> from first author</li> </ul> <h2 id=authors>Authors<a class=headerlink href=#authors title="Permanent link">&para;</a></h2> <ul> <li>Aravind Srinivas <a href=https://twitter.com/Aravind7694>Twitter</a>/<a href="https://scholar.google.com/citations?user=GhrKC1gAAAAJ">Scholar</a>/<a href=https://people.eecs.berkeley.edu/~aravind/ >Academic</a></li> <li>Michael Laskin <a href=https://twitter.com/MishaLaskin>Twitter</a>/<a href="https://scholar.google.com/citations?user=DOGDnwsAAAAJ">Scholar</a>/<a href=https://mishalaskin.github.io/ >Academic</a></li> <li>Pieter Abbeel <a href=https://twitter.com/pabbeel>Twitter</a>/<a href="https://scholar.google.com/citations?user=vtwH6GkAAAAJ">Scholar</a>/<a href=https://people.eecs.berkeley.edu/~pabbeel/ >Academic</a></li> </ul> <div class=footnote> <hr> <ol> <li id=fn:sac> <p><a href=https://arxiv.org/abs/1801.01290>Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</a>&#160;<a class=footnote-backref href=#fnref:sac title="Jump back to footnote 1 in the text">&#8617;</a></p> </li> <li id=fn:rainbow> <p><a href=https://arxiv.org/abs/1710.02298>Rainbow: Combining Improvements in Deep Reinforcement Learning</a>&#160;<a class=footnote-backref href=#fnref:rainbow title="Jump back to footnote 2 in the text">&#8617;</a></p> </li> <li id=fn:bert> <p><a href=https://arxiv.org/abs/1810.04805>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>&#160;<a class=footnote-backref href=#fnref:bert title="Jump back to footnote 3 in the text">&#8617;</a></p> </li> <li id=fn:simclr> <p><a href=https://arxiv.org/abs/2002.05709>A Simple Framework for Contrastive Learning of Visual Representations</a> (SimCLR)&#160;<a class=footnote-backref href=#fnref:simclr title="Jump back to footnote 4 in the text">&#8617;</a><a class=footnote-backref href=#fnref2:simclr title="Jump back to footnote 4 in the text">&#8617;</a></p> </li> <li id=fn:moco> <p><a href=https://arxiv.org/abs/1911.05722>Momentum Contrast for Unsupervised Visual Representation Learning</a> (MoCo)&#160;<a class=footnote-backref href=#fnref:moco title="Jump back to footnote 5 in the text">&#8617;</a></p> </li> <li id=fn:cpc> <p><a href=https://arxiv.org/abs/1905.09272>Data-Efficient Image Recognition with Contrastive Predictive Coding</a> (CPC)&#160;<a class=footnote-backref href=#fnref:cpc title="Jump back to footnote 6 in the text">&#8617;</a></p> </li> <li id=fn:original> <p>excerpts from original paper&#160;<a class=footnote-backref href=#fnref:original title="Jump back to footnote 7 in the text">&#8617;</a><a class=footnote-backref href=#fnref2:original title="Jump back to footnote 7 in the text">&#8617;</a><a class=footnote-backref href=#fnref3:original title="Jump back to footnote 7 in the text">&#8617;</a><a class=footnote-backref href=#fnref4:original title="Jump back to footnote 7 in the text">&#8617;</a></p> </li> <li id=fn:muzero-tweet> <p><a href=https://twitter.com/gwern/status/1248087160391163906>https://twitter.com/gwern/status/1248087160391163906</a>&#160;<a class=footnote-backref href=#fnref:muzero-tweet title="Jump back to footnote 8 in the text">&#8617;</a></p> </li> </ol> </div> <hr> <div class=md-source-date> <small> Last update: April 12, 2020 </small> </div> </article> </div> </div> </main> <footer class=md-footer> <div class=md-footer-nav> <nav class="md-footer-nav__inner md-grid"> <a href=.. title=Home class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel=prev> <div class="md-flex__cell md-flex__cell--shrink"> <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i> </div> <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"> <span class=md-flex__ellipsis> <span class=md-footer-nav__direction> Previous </span> Home </span> </div> </a> <a href=../a3c/ title="Asynchronous Methods for Deep Reinforcement Learning" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel=next> <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"> <span class=md-flex__ellipsis> <span class=md-footer-nav__direction> Next </span> Asynchronous Methods for Deep Reinforcement Learning </span> </div> <div class="md-flex__cell md-flex__cell--shrink"> <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i> </div> </a> </nav> </div> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> <div class=md-footer-copyright__highlight> Copyright &copy; 2020 Florian Laurent </div> powered by <a href=https://www.mkdocs.org target=_blank rel=noopener>MkDocs</a> and <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs</a> </div> </div> </div> </footer> </div> <script src=../assets/javascripts/application.c33a9706.js></script> <script>app.initialize({version:"1.1",url:{base:".."}})</script> </body> </html>